# Instalaci√≥n y Configuraci√≥n de KVM y M√°quinas Virtuales

## Tabla de contenido

- [Desarrollo](#desarrollo)
  - [Fase 1. Configuraci√≥n del sistema anfitri√≥n](#fase1-configuraci√≥n-del-sistema-anfitri√≥n)
  - [Fase 2. Verificaci√≥n de requerimientos m√≠nimos](#fase2-verificaci√≥n-de-requerimientos-m√≠nimos)
  - [Fase 3. Instalaci√≥n de paquetes de virtualizaci√≥n](#fase3-instalaci√≥n-de-paquetes-de-virtualizaci√≥n)
  - [Fase 4. Creaci√≥n de una m√°quina virtual](#fase4-creaci√≥n-de-una-m√°quina-virtual)
- [Pruebas y Validaci√≥n](#pruebasvalidaci√≥n)
- [Bibliograf√≠a](#bibliograf√≠a)

## Desarrollo

### Fase 1. Configuraci√≥n del sistema anfitri√≥n

#### Tarea 1. Personalizar la clave del usuario root

Se personaliza la clave del usuario root del PC asignado ejecutando la orden `passwd`.

#### Tarea 2. Completar la encuesta "Datos PC"

Para completar la encuesta "Datos PC" se procede de la siguiente forma:

1. Identificador del PC asignado: **LQ-D25**
2. Direcci√≥n IP del PC: **10.140.92.125**

Se ejecuta la orden:

```bash
root@lq-d25:/# ip -4 addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: enp6s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    inet 10.140.92.125/24 brd 10.140.92.255 scope global dynamic noprefixroute enp6s0
       valid_lft 9880sec preferred_lft 9880sec
3: virbr0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
       valid_lft forever preferred_lft forever
```

**Explicaci√≥n del comando**:
- `ip`: Herramienta de l√≠nea de comandos para configurar y mostrar informaci√≥n sobre interfaces de red, rutas, pol√≠ticas y t√∫neles
- `-4`: Limita la salida a direcciones IPv4 √∫nicamente
- `addr`: Subcomando que solicita informaci√≥n sobre direcciones de red
- `show`: Muestra la informaci√≥n detallada de todas las interfaces disponibles

La salida muestra tres interfaces:
1. `lo`: La interfaz de loopback (127.0.0.1) utilizada para comunicaciones internas dentro del mismo sistema
2. `enp6s0`: La interfaz f√≠sica conectada a la red local con direcci√≥n IP 10.140.92.125/24
3. `virbr0`: Una interfaz de red virtual creada por libvirt (192.168.122.1/24) que sirve como puente para las m√°quinas virtuales

#### Tarea 3. Comprobar el modo de funcionamiento de SELinux

Se ejecuta la siguiente orden para comprobar que el modo de funcionamiento de SELinux se est√° ejecutando en modo "enforcing":

```bash
root@lq-d25:/# sestatus
SELinux status:                 enabled
SELinuxfs mount:                /sys/fs/selinux
SELinux root directory:         /etc/selinux
Loaded policy name:             targeted
Current mode:                   enforcing
Mode from config file:          enforcing
Policy MLS status:              enabled
Policy deny_unknown status:     allowed
Memory protection checking:     actual (secure)
Max kernel policy version:      33
```

**Explicaci√≥n del comando**:
- `sestatus`: Utilidad especializada para mostrar el estado completo de SELinux en el sistema
  - No requiere opciones adicionales ya que su funci√≥n principal es mostrar el estado actual de SELinux

La salida muestra informaci√≥n detallada sobre la configuraci√≥n de SELinux:
- `SELinux status: enabled`: Indica que SELinux est√° activo
- `Current mode: enforcing`: Confirma que SELinux est√° en modo enforcing, lo que significa que est√° aplicando activamente todas las pol√≠ticas de seguridad configuradas
- `Loaded policy name: targeted`: Indica que se est√° utilizando la pol√≠tica "targeted", que aplica restricciones principalmente a servicios de red y daemons expuestos
- `Policy MLS status: enabled`: Muestra que el control de acceso multinivel est√° habilitado

> **Nota**: El modo "enforcing" en SELinux es cr√≠tico para entornos de virtualizaci√≥n, ya que proporciona aislamiento adicional entre las m√°quinas virtuales y el sistema anfitri√≥n.

#### Tarea 4. Instalar un entorno gr√°fico GNOME m√≠nimo

Para instalar un entorno gr√°fico GNOME m√≠nimo, primero se debe instalar el grupo de paquetes @base-x:

```bash
dnf groupinstall "base-x" -y
```

**Explicaci√≥n del comando**:
- `dnf`: Gestor de paquetes moderno en distribuciones basadas en RPM, como Fedora
- `groupinstall`: Subcomando para instalar un grupo predefinido de paquetes relacionados
- `"base-x"`: Nombre del grupo de paquetes que contiene los componentes b√°sicos del sistema X Window
- `-y`: Opci√≥n que acepta autom√°ticamente todas las preguntas de confirmaci√≥n durante la instalaci√≥n

A continuaci√≥n, se instalan los paquetes esenciales de GNOME:

```bash
sudo dnf install -y gnome-shell gnome-terminal nautilus
```

**Explicaci√≥n del comando**:
- `sudo`: Ejecuta el comando con privilegios elevados (necesario para instalar software)
- `dnf install`: Subcomando para instalar paquetes espec√≠ficos
- `-y`: Acepta autom√°ticamente la confirmaci√≥n
- Paquetes instalados:
  - `gnome-shell`: El entorno de escritorio GNOME que proporciona la interfaz gr√°fica principal
  - `gnome-terminal`: El emulador de terminal para el entorno GNOME
  - `nautilus`: El gestor de archivos oficial de GNOME

Tambi√©n se debe instalar virt-manager, que permitir√° gestionar m√°quinas virtuales:

```bash
sudo dnf install -y virt-manager
```

**Explicaci√≥n del comando**:
- `virt-manager`: Interfaz gr√°fica para la gesti√≥n de m√°quinas virtuales a trav√©s de libvirt
  - Proporciona una forma intuitiva de crear, modificar y monitorizar m√°quinas virtuales
  - Se integra con KVM, QEMU y Xen

Para iniciar el sistema en modo gr√°fico por defecto:

```bash
sudo systemctl set-default graphical.target
```

**Explicaci√≥n del comando**:
- `systemctl`: Herramienta principal para controlar systemd (sistema de inicio y gestor de servicios)
- `set-default`: Establece el objetivo (target) de inicio predeterminado
- `graphical.target`: Unidad systemd que define el nivel de ejecuci√≥n para un sistema con interfaz gr√°fica completa
  - Equivalente al antiguo runlevel 5 en sistemas SysV

Este comando configura el sistema para arrancar directamente en modo gr√°fico en futuros reinicios, sin necesidad de iniciar manualmente el entorno gr√°fico.

Ahora para poder iniciar la interfaz gr√°fica habr√≠a que reiniciar el PC o ejecutar:

```bash
sudo systemctl start gdm
```

**Explicaci√≥n del comando**:
- `systemctl start`: Inicia inmediatamente un servicio o unidad
- `gdm`: GNOME Display Manager, el gestor de pantalla que maneja el inicio de sesi√≥n gr√°fico y lanza el entorno GNOME
  - Se encarga de la autenticaci√≥n gr√°fica de usuarios
  - Permite la selecci√≥n de diferentes sesiones de escritorio

Esta orden inicia GDM sin necesidad de reiniciar, proporcionando acceso inmediato al entorno gr√°fico.

Finalmente, se instalan las herramientas adicionales Firefox y gedit:
 
```bash
sudo dnf install -y firefox gedit
```

**Explicaci√≥n del comando**:
- Paquetes instalados:
  - `firefox`: Navegador web de c√≥digo abierto desarrollado por Mozilla
  - `gedit`: Editor de texto gr√°fico optimizado para el entorno GNOME
    - Incluye resaltado de sintaxis y es adecuado para editar archivos de configuraci√≥n

#### Tarea 5. Configurar el nombre del sistema

En esta secci√≥n se describe como configurar el nombre del sistema utilizando la herramienta hostnamectl. El objetivo es establecer un identificador FQDN (Full Qualified Domain Name) que siga el patr√≥n IdentificadorPC.vpd.com.

Primero se verifica el nombre actual del sistema con el comando:

```bash
root@LQ-D25:~# hostnamectl status
   Static hostname: (unset)                         
Transient hostname: LQ-D25
         Icon name: computer-desktop
           Chassis: desktop üñ•Ô∏è
        Machine ID: ee3433707aa84f56860cd67efede9ce8
           Boot ID: a8c57ab742fc427ab9883280a554ee97
  Operating System: Fedora Linux 39 (Server Edition)
       CPE OS Name: cpe:/o:fedoraproject:fedora:39
    OS Support End: Tue 2024-11-12
OS Support Expired: 2month 3w 4d                    
            Kernel: Linux 6.9.9-100.fc39.x86_64
      Architecture: x86-64
   Hardware Vendor: ASUS
    Hardware Model: PRIME A520M-A II
  Firmware Version: 3002
     Firmware Date: Thu 2023-02-23
      Firmware Age: 1y 11month 2w
```

Este comando muestra informaci√≥n sobre el nombre del host, incluyendo el nombre est√°tico (el que se usa por defecto al arrancar), el nombre transitorio (asignador por la red) y otra informaci√≥n relevante del sistema. 

A continuaci√≥n, se configura el nuevo hostname:

```bash
sudo hostnamectl set-hostname lq-d25.vpd.com
```

Donde:
- `hostnamectl`: herramienta para controlar el nombre del host
- `set-hostname`: establece el nombre del host
- `lq-d25.vpd.com`: nuevo nombre de dominio completamente cualificado (FQDN)

Despu√©s de ejecutar este comando, el sistema tendr√° el nuevo FQDN configurado.

Este cambio de nombre es importante para la correcta identificaci√≥n del sistema en la red y para las pr√°cticas posteriores de la asignatura, donde se utilizar√° este FQDN para acceder a las m√°quinas virtuales.

### Fase 2. Verificaci√≥n de requerimientos m√≠nimos

#### Tarea 6. Verificar la virtualizaci√≥n del procesador

Para verificar que la extensi√≥n de virtualizaci√≥n est√° habilitada en el procesador, se utiliza el siguiente comando:

```bash
root@lq-d25:~# lscpu | grep Virtualization
Virtualization:                       AMD-V
```

**Explicaci√≥n del comando**:
- `lscpu`: Herramienta que muestra informaci√≥n detallada sobre la arquitectura del procesador
  - Analiza el contenido de `/proc/cpuinfo` y lo presenta en un formato estructurado
- `|`: Operador de tuber√≠a (pipe) que redirige la salida del comando anterior a la entrada del siguiente
- `grep Virtualization`: Filtra la salida para mostrar √∫nicamente las l√≠neas que contienen el texto "Virtualization"
  - Permite localizar r√°pidamente la informaci√≥n espec√≠fica sobre capacidades de virtualizaci√≥n

En el resultado se indica que el procesador del sistema soporta la tecnolog√≠a de virtualizaci√≥n AMD-V, lo que significa que se puede utilizar para crear y ejecutar m√°quinas virtuales. 

Tambi√©n se puede verificar que la extensi√≥n de virtualizaci√≥n est√° habilitada en el procesador con el siguiente comando:

```bash
root@lq-d25:~# grep -E 'svm|vmx' /proc/cpuinfo
flags‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm debug_swap
```

**Explicaci√≥n del comando**:
- `grep`: Herramienta para buscar patrones en archivos de texto
- `-E`: Habilita el uso de expresiones regulares extendidas
- `'svm|vmx'`: Expresi√≥n regular que busca:
  - `svm`: Indicador de Secure Virtual Machine (tecnolog√≠a de virtualizaci√≥n de AMD)
  - `vmx`: Indicador de Virtual Machine Extensions (tecnolog√≠a de virtualizaci√≥n de Intel)
- `/proc/cpuinfo`: Archivo virtual que contiene informaci√≥n detallada sobre todos los procesadores del sistema

Este comando busca espec√≠ficamente las banderas de virtualizaci√≥n en los datos del procesador:
- Para procesadores AMD se busca la bandera "svm"
- Para procesadores Intel se busca la bandera "vmx"

La presencia de la bandera "svm" en la salida (resaltada en la lista de flags) confirma que el hardware no solo soporta virtualizaci√≥n, sino que adem√°s est√° habilitada a nivel de BIOS/UEFI.

#### Tarea 7. Recopilar informaci√≥n del sistema

Para responder a las preguntas sobre el sistema, se utilizan los siguientes comandos:

¬øCu√°ntos de chips de procesador posee el sistema?

```bash
root@lq-d25:~# lscpu | grep "Socket(s):"
```

**Explicaci√≥n del comando**:
- Este comando combina `lscpu` con `grep` para filtrar espec√≠ficamente la informaci√≥n sobre sockets f√≠sicos
- El resultado muestra el n√∫mero de procesadores f√≠sicos (chips) instalados en el sistema
- En sistemas de servidor, pueden existir m√∫ltiples sockets, mientras que en estaciones de trabajo y PCs t√≠picos suele haber un √∫nico socket

¬øCu√°ntos n√∫cleos posee cada chip?

```bash
root@lq-d25:~# lscpu | grep "Core(s) per socket:"
Core(s) per socket:                   6
```

**Explicaci√≥n del comando**:
- `lscpu | grep "Core(s) per socket:"`: Filtra la salida para mostrar solo la l√≠nea que contiene informaci√≥n sobre n√∫cleos por socket
- El resultado "6" indica que cada procesador f√≠sico (socket) contiene 6 n√∫cleos de procesamiento independientes
- Los n√∫cleos son unidades de procesamiento que pueden ejecutar instrucciones de forma paralela

Resultado: 6 n√∫cleos por socket. 

¬øCu√°ntos hilos de procesamiento se podr√°n ejecutar en el sistema?

```bash
root@lq-d25:~# lscpu | grep "Thread(s) per core:"
Thread(s) per core:                   2
```

**Explicaci√≥n del comando**:
- `lscpu | grep "Thread(s) per core:"`: Filtra la informaci√≥n sobre hilos de ejecuci√≥n por n√∫cleo
- El resultado "2" indica que cada n√∫cleo puede manejar 2 hilos de ejecuci√≥n simult√°neos
- Esta es una tecnolog√≠a conocida como SMT (Simultaneous Multi-Threading):
  - En procesadores Intel se llama Hyper-Threading
  - En procesadores AMD se conoce como SMT

Resultado: 2 hilos por n√∫cleo. 

C√°lculo: 1 socket * 6 n√∫cleos/socket * 2 hilos/n√∫cleos = 12 hilos de procesamiento.

¬øQu√© cantidad de memoria RAM dispone el sistema?

Para estudiar la informaci√≥n de la memoria del PC Podemos ejecutar el siguiente comando:

```bash
root@lq-d25:~# free -h
               total        used        free      shared  buff/cache   available
Mem:            62Gi       4,4Gi        56Gi       123Mi       1,4Gi        57Gi
Swap:           39Gi          0B        39Gi
```

**Explicaci√≥n del comando**:
- `free`: Muestra informaci√≥n sobre la memoria RAM y swap del sistema
- `-h`: Opci√≥n "human-readable" que muestra los valores en unidades legibles (GB, MB) en lugar de bytes
- La salida presenta la siguiente informaci√≥n:
  - `total`: Cantidad total de memoria f√≠sica instalada
  - `used`: Memoria actualmente en uso por aplicaciones y el sistema
  - `free`: Memoria completamente libre
  - `shared`: Memoria compartida entre m√∫ltiples procesos
  - `buff/cache`: Memoria utilizada por buffers y cach√© del sistema para mejorar el rendimiento de E/S
  - `available`: Memoria que puede ser asignada a nuevas aplicaciones sin recurrir a swap

Luego, para averiguar la cantidad de memoria RAM de la que dispone el sistema:

```bash
root@lq-d25:~# free -h | grep Mem | awk '{print $2}'
62Gi
```

**Explicaci√≥n del comando**:
- `free -h`: Muestra informaci√≥n de memoria en formato legible
- `grep Mem`: Filtra solo la l√≠nea que contiene informaci√≥n de la memoria principal
- `awk '{print $2}'`: Extrae el segundo campo de la l√≠nea filtrada, que corresponde al total de memoria

Este tipo de comando combinado (pipeline) es muy √∫til para extraer valores espec√≠ficos de comandos con salida m√°s compleja, facilitando su uso en scripts.

Respuesta: 62 GiB

¬øQu√© cantidad de RAM est√° disponible?

```bash
root@lq-d25:~# free -h | grep Mem | awk '{print $7}'
57Gi
```

**Explicaci√≥n del comando**: 
- Similar al comando anterior, pero extrae el s√©ptimo campo ($7) que corresponde a la memoria disponible
- La memoria disponible representa la RAM que puede ser asignada inmediatamente a aplicaciones
- Es importante destacar que este valor suele ser mayor que la memoria "free" porque incluye memoria de cach√© que puede ser liberada si es necesario

Respuesta: 57 GiB

¬øCu√°l es el tama√±o del √°rea de swap?

```bash
root@lq-d25:~# free -h | grep Swap | awk '{print $2}'
39Gi
```

**Explicaci√≥n del comando**: 
- `grep Swap`: Filtra la l√≠nea que contiene informaci√≥n sobre el √°rea de swap
- `awk '{print $2}'`: Extrae el valor total del √°rea de swap
- El √°rea de swap es espacio en disco que el sistema utiliza como extensi√≥n de la memoria RAM cuando √©sta se agota
- Un tama√±o adecuado de swap depende del uso del sistema y la cantidad de RAM, pero suele recomendarse entre 1.5x y 2x la RAM para sistemas con hibernaci√≥n

Resultado: 39 GiB

¬øCu√°l es el estado de ocupaci√≥n y espacio libre del almacenamiento persistente?

```bash
root@lq-d25:~# df -h
S.ficheros     Tama√±o Usados  Disp Uso% Montado en
/dev/sda7        144G   4,7G  132G   4% /
devtmpfs         4,0M      0  4,0M   0% /dev
tmpfs             32G      0   32G   0% /dev/shm
tmpfs             13G   9,8M   13G   1% /run
tmpfs             32G    20K   32G   1% /tmp
tmpfs            6,3G   3,7M  6,3G   1% /run/user/0
```

**Explicaci√≥n del comando**:
- `df`: (Disk Free) Muestra informaci√≥n sobre el espacio utilizado y disponible en sistemas de archivos
- `-h`: Presenta los tama√±os en formato legible para humanos (GB, MB)
- La salida proporciona la siguiente informaci√≥n para cada sistema de archivos:
  - `S.ficheros`: El dispositivo o punto de montaje
  - `Tama√±o`: Capacidad total
  - `Usados`: Espacio utilizado
  - `Disp`: Espacio disponible
  - `Uso%`: Porcentaje de uso
  - `Montado en`: Punto de montaje en el sistema de archivos

La salida detallada muestra:

- `/dev/sda7`: La partici√≥n principal del disco duro, donde est√° instalado el sistema operativo. Tiene 144 GB de capacidad con solo 4.7 GB utilizados (4% de uso), lo que indica amplio espacio disponible.

- Sistemas de archivos temporales en memoria (tmpfs):
  - `/dev/shm`: Memoria compartida entre procesos (32 GB)
  - `/run`: Almacena archivos temporales de estado del sistema (13 GB)
  - `/tmp`: Para archivos temporales generales (32 GB)
  - `/run/user/0`: Archivos temporales espec√≠ficos del usuario root (6.3 GB)

En general, el sistema tiene abundante espacio libre tanto en almacenamiento persistente como en memoria.

### Fase 3. Instalaci√≥n de paquetes de virtualizaci√≥n

#### Tarea 8. Instalar el entorno de virtualizaci√≥n KVM

Para instalar el entorno de virtualizaci√≥n KVM, se deben ejecutar los siguientes comandos:

```bash
root@lq-d25:~# dnf groupinstall "Virtualization " --with-optional -y
√öltima comprobaci√≥n de caducidad de metadatos hecha hace 0:51:35, el vie 07 feb 2025 19:33:38.
Dependencias resueltas.
============================================================================================================================
Paquete                      Arquitectura                Versi√≥n                        Repositorio                   Tam.
============================================================================================================================
Instalando grupos:
Headless Virtualization                                                                                                   
 
Resumen de la transacci√≥n
============================================================================================================================
 
¬°Listo!
```

**Explicaci√≥n del comando**:
- `dnf groupinstall`: Instala un grupo predefinido de paquetes relacionados
- `"Virtualization"`: Nombre del grupo que contiene los paquetes b√°sicos para virtualizaci√≥n con interfaz gr√°fica
- `--with-optional`: Incluye tambi√©n los paquetes opcionales del grupo, proporcionando caracter√≠sticas adicionales
- `-y`: Responde autom√°ticamente "s√≠" a todas las preguntas de confirmaci√≥n
  
Este comando instala los paquetes principales para virtualizaci√≥n con KVM, incluyendo:
- `qemu-kvm`: La implementaci√≥n del hipervisor KVM
- `libvirt`: La API de virtualizaci√≥n
- `virt-manager`: La interfaz gr√°fica para gesti√≥n de m√°quinas virtuales
- Herramientas auxiliares como `virt-install` y `virt-viewer`

Luego:

```bash
root@lq-d25:~# dnf groupinstall "Virtualization-headless" --with-optional -y
√öltima comprobaci√≥n de caducidad de metadatos hecha hace 0:56:23, el vie 07 feb 2025 19:33:38.
Dependencias resueltas.
============================================================================================================================
Paquete                      Arquitectura                Versi√≥n                        Repositorio                   Tam.
============================================================================================================================
Instalando grupos:
Headless Virtualization                                                                                                   
 
Resumen de la transacci√≥n
============================================================================================================================
 
¬°Listo!
```

**Explicaci√≥n del comando**:
- `"Virtualization-headless"`: Este grupo contiene paquetes para ejecutar y administrar m√°quinas virtuales sin necesidad de interfaz gr√°fica
- Incluye herramientas de l√≠nea de comandos como `virsh` para administraci√≥n remota
- Es especialmente √∫til para servidores sin interfaz gr√°fica o para automatizaci√≥n

La instalaci√≥n de ambos grupos proporciona un entorno de virtualizaci√≥n completo con:
- Capacidades gr√°ficas para administraci√≥n local
- Herramientas de l√≠nea de comandos para administraci√≥n remota o automatizada
- Bibliotecas y dependencias necesarias para el funcionamiento √≥ptimo

Una vez instalado el entorno de virtualizaci√≥n, se puede verificar la instalaci√≥n ejecutando el comando:

```bash
root@lq-d25:~# virsh cpu-models x86_64
486
pentium
pentium2
pentium3
pentiumpro
coreduo
n270
core2duo
qemu32
kvm32
cpu64-rhel5
cpu64-rhel6
qemu64
kvm64
 ‚Ä¶
```

**Explicaci√≥n del comando**:
- `virsh`: La interfaz de l√≠nea de comandos para gestionar la virtualizaci√≥n con libvirt
- `cpu-models`: Subcomando que consulta los modelos de CPU que el hipervisor puede emular
- `x86_64`: Especifica la arquitectura sobre la que se consultan los modelos disponibles

Este comando es √∫til para:
- Verificar que el sistema de virtualizaci√≥n est√° correctamente instalado y funcionando
- Conocer los modelos de CPU que pueden emularse para las m√°quinas virtuales
- Ayudar a seleccionar el modelo de CPU m√°s adecuado al crear nuevas VMs

La salida muestra diversos modelos de CPU soportados, desde procesadores antiguos (486, Pentium) hasta modelos m√°s modernos, permitiendo compatibilidad con diferentes sistemas operativos invitados.

#### Tarea 9. Configurar y verificar el servicio virtnetworkd

Antes de configurar el servicio virtnetworkd se debe iniciar el servicio libvirtd, que es el demonio de virtualizaci√≥n principal, para ello se utiliza el siguiente comando:

```bash
root@lq-d25:~# sudo systemctl enable --now libvirtd
Created symlink /etc/systemd/system/multi-user.target.wants/libvirtd.service ‚Üí /usr/lib/systemd/system/libvirtd.service.
Created symlink /etc/systemd/system/sockets.target.wants/libvirtd.socket ‚Üí /usr/lib/systemd/system/libvirtd.socket.
Created symlink /etc/systemd/system/sockets.target.wants/libvirtd-ro.socket ‚Üí /usr/lib/systemd/system/libvirtd-ro.socket.
```

**Explicaci√≥n del comando**:
- `systemctl`: Herramienta principal para gestionar servicios en sistemas que utilizan systemd
- `enable`: Configura el servicio para que se inicie autom√°ticamente durante el arranque del sistema
- `--now`: Bandera que combina las acciones de `enable` y `start`, habilitando el servicio para arranques futuros e inici√°ndolo inmediatamente
- `libvirtd`: El demonio principal de libvirt, que proporciona una interfaz unificada para gestionar diferentes tecnolog√≠as de virtualizaci√≥n

La salida muestra la creaci√≥n de tres enlaces simb√≥licos:
1. `libvirtd.service`: El servicio principal que gestiona las m√°quinas virtuales
2. `libvirtd.socket`: Socket para comunicaci√≥n est√°ndar con el demonio
3. `libvirtd-ro.socket`: Socket para comunicaciones de solo lectura (ro = read-only)

Estos enlaces se crean en los directorios correspondientes a los targets de systemd, garantizando que el servicio se inicie autom√°ticamente en el nivel adecuado.

Para verificar que el servicio libvirtd se est√° ejecutando correctamente se utiliza el siguiente comando:

```bash
root@lq-d25:~# systemctl status libvirtd
‚óè libvirtd.service - Virtualization daemon
     Loaded: loaded (/usr/lib/systemd/system/libvirtd.service; enabled; preset: disabled)
    Drop-In: /usr/lib/systemd/system/service.d
             ‚îî‚îÄ10-timeout-abort.conf
     Active: active (running) since Fri 2025-02-07 20:28:27 WET; 11s ago
TriggeredBy: ‚óè libvirtd-ro.socket
             ‚óè libvirtd.socket
             ‚óã libvirtd-tls.socket
             ‚óè libvirtd-admin.socket
             ‚óã libvirtd-tcp.socket
       Docs: man:libvirtd(8)
https://libvirt.org
   Main PID: 10869 (libvirtd)
      Tasks: 22 (limit: 32768)
     Memory: 18.0M
        CPU: 374ms
     CGroup: /system.slice/libvirtd.service
             ‚îú‚îÄ10869 /usr/sbin/libvirtd --timeout 120
             ‚îú‚îÄ10970 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/>
             ‚îî‚îÄ10971 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/>
 
feb 07 20:28:27 lq-d25.vpc.com systemd[1]: Started libvirtd.service - Virtualization daemon.
feb 07 20:28:27 lq-d25.vpc.com dnsmasq[10970]: started, version 2.90 cachesize 150
feb 07 20:28:27 lq-d25.vpc.com dnsmasq[10970]: compile time options: IPv6 GNU-getopt DBus no-UBus i18n IDN2 DHCP DHCPv6 no->
feb 07 20:28:27 lq-d25.vpc.com dnsmasq-dhcp[10970]: DHCP, IP range 192.168.122.2 -- 192.168.122.254, lease time 1h
feb 07 20:28:27 lq-d25.vpc.com dnsmasq-dhcp[10970]: DHCP, sockets bound exclusively to interface virbr0
feb 07 20:28:27 lq-d25.vpc.com dnsmasq[10970]: reading /etc/resolv.conf
feb 07 20:28:27 lq-d25.vpc.com dnsmasq[10970]: using nameserver 127.0.0.53#53
feb 07 20:28:27 lq-d25.vpc.com dnsmasq[10970]: read /etc/hosts - 8 names
feb 07 20:28:27 lq-d25.vpc.com dnsmasq[10970]: read /var/lib/libvirt/dnsmasq/default.addnhosts - 0 names
feb 07 20:28:27 lq-d25.vpc.com dnsmasq-dhcp[10970]: read /var/lib/libvirt/dnsmasq/default.hostsfile
```

**Explicaci√≥n del comando**:
- `systemctl status libvirtd`: Muestra el estado detallado del servicio libvirtd
- La salida proporciona informaci√≥n completa sobre:
  - Estado actual: `active (running)` indica que el servicio est√° ejecut√°ndose correctamente
  - Ubicaci√≥n del archivo de servicio y si est√° habilitado para inicio autom√°tico
  - Sockets relacionados que pueden activar el servicio (TrigeredBy)
  - PID del proceso principal y estad√≠sticas de uso de recursos
  - Procesos hijos, incluyendo dnsmasq para servicios DHCP y DNS
  - √öltimos eventos relevantes del servicio

Esta informaci√≥n es especialmente √∫til para diagnosticar problemas, ya que muestra tanto la configuraci√≥n como el estado actual de ejecuci√≥n y los mensajes recientes del servicio.

A continuaci√≥n, para verificar que el servicio libvirtd est√° funcionando correctamente y que no hay m√°quinas virtuales en ejecuci√≥n, se utiliza el siguiente comando:

```bash
root@lq-d25:~# sudo virsh list --all
Id   Nombre   Estado
-----------------------
```

**Explicaci√≥n del comando**:
- `virsh`: Interfaz de l√≠nea de comandos para gestionar la virtualizaci√≥n mediante libvirt
- `list`: Subcomando que muestra las m√°quinas virtuales definidas
- `--all`: Opci√≥n que incluye todas las m√°quinas virtuales, tanto las activas como las inactivas

Este comando proporciona una vista r√°pida de todas las m√°quinas virtuales configuradas en el sistema. La salida vac√≠a indica que a√∫n no se ha definido ninguna VM, lo que es normal en un sistema reci√©n configurado.

Para configurar el sistema anfitri√≥n para que el servicio virtnetworkd se inicie durante el arranque, se utiliza la siguiente orden:

```bash
root@lq-d25:~# systemctl enable --now virtnetworkd
Created symlink /etc/systemd/system/multi-user.target.wants/virtnetworkd.service ‚Üí /usr/lib/systemd/system/virtnetworkd.service.
```

**Explicaci√≥n del comando**:
- `systemctl enable --now virtnetworkd`: Habilita e inicia el servicio virtnetworkd
- `virtnetworkd`: Componente de libvirt especializado en la gesti√≥n de redes virtuales
  - Controla las interfaces virtuales
  - Gestiona los puentes de red
  - Proporciona servicios DHCP y DNS para las redes virtuales
  - Administra las reglas de NAT y firewall necesarias para la conectividad

A diferencia del servicio monol√≠tico anterior, libvirt ahora utiliza un enfoque modular donde diferentes componentes se ejecutan como servicios separados: virtnetworkd para redes, virtqemud para QEMU, virtstoraged para almacenamiento, etc. Este dise√±o mejora la seguridad y el rendimiento.

Una vez ejecutado el comando, el servicio virtnetworkd estar√° habilitado e iniciado. 

Para verificar que el servicio se est√° ejecutando correctamente, se puede utilizar el siguiente comando:

```bash
root@lq-d25:~# systemctl status virtnetworkd
‚óè virtnetworkd.service - Virtualization network daemon
     Loaded: loaded (/usr/lib/systemd/system/virtnetworkd.service; enabled; preset: disabled)
    Drop-In: /usr/lib/systemd/system/service.d
             ‚îî‚îÄ10-timeout-abort.conf
     Active: active (running) since Fri 2025-02-07 20:36:27 WET; 10s ago
TriggeredBy: ‚óè virtnetworkd-admin.socket
             ‚óè virtnetworkd.socket
             ‚óè virtnetworkd-ro.socket
       Docs: man:virtnetworkd(8)
https://libvirt.org
   Main PID: 11524 (virtnetworkd)
      Tasks: 19 (limit: 76221)
     Memory: 3.9M
        CPU: 157ms
     CGroup: /system.slice/virtnetworkd.service
             ‚îî‚îÄ11524 /usr/sbin/virtnetworkd --timeout 120
 
feb 07 20:36:27 lq-d25.vpc.com systemd[1]: Starting virtnetworkd.service - Virtualization network daemon...
feb 07 20:36:27 lq-d25.vpc.com systemd[1]: Started virtnetworkd.service - Virtualization network daemon.
```

**Explicaci√≥n del comando**:
- Similar al comando anterior para libvirtd, pero espec√≠fico para el servicio de redes virtuales
- La salida confirma que el servicio:
  - Est√° cargado y habilitado para inicio autom√°tico
  - Se encuentra activo y en ejecuci√≥n
  - Usa aproximadamente 3.9 MB de memoria
  - Est√° vinculado a varios sockets que pueden activarlo

La informaci√≥n de estado `active (running)` confirma que el servicio est√° funcionando correctamente.

Ahora, para verificar que los m√≥dulos del kernel kvm est√°n cargados, utilizamos el siguiente comando:

```bash
root@lq-d25:~# lsmod | grep kvm
kvm_amd               217088  0
kvm                  1441792  1 kvm_amd
ccp                   180224  1 kvm_amd
```

**Explicaci√≥n del comando**:
- `lsmod`: Muestra los m√≥dulos del kernel actualmente cargados
- `| grep kvm`: Filtra la salida para mostrar solo los m√≥dulos relacionados con KVM

La salida muestra tres m√≥dulos relacionados con la virtualizaci√≥n:
- `kvm_amd`: M√≥dulo espec√≠fico para procesadores AMD (existir√≠a `kvm_intel` en sistemas con procesadores Intel)
- `kvm`: M√≥dulo principal que implementa la infraestructura de virtualizaci√≥n
- `ccp`: Crypto Co-Processor, un m√≥dulo relacionado con funciones criptogr√°ficas de AMD

El n√∫mero despu√©s del tama√±o del m√≥dulo (`2` para `kvm`) indica cu√°ntos otros m√≥dulos dependen de √©l. En este caso, `kvm_amd` depende de `kvm`, lo que muestra la relaci√≥n jer√°rquica entre los m√≥dulos.

Finalmente, para comprobar que la configuraci√≥n de SELinux permite que el entorno de virtualizaci√≥n utilice el servicio NFS, se ejecuta el siguiente comando:

```bash
root@lq-d25:~# getsebool virt_use_nfs
virt_use_nfs --> on
```

**Explicaci√≥n del comando**:
- `getsebool`: Herramienta que consulta la configuraci√≥n actual de las pol√≠ticas booleanas de SELinux
- `virt_use_nfs`: Pol√≠tica espec√≠fica que controla si las m√°quinas virtuales pueden acceder a recursos compartidos mediante NFS

La salida `on` indica que la pol√≠tica est√° habilitada, lo que permite que los procesos de virtualizaci√≥n accedan a sistemas de archivos NFS. Esto es importante para:
- Almacenar im√°genes de m√°quinas virtuales en servidores NFS
- Permitir que las m√°quinas virtuales accedan a recursos compartidos en red
- Utilizar almacenamiento compartido entre diferentes hosts de virtualizaci√≥n

Si esta pol√≠tica estuviera desactivada (`off`), SELinux bloquear√≠a estos accesos incluso si el resto de la configuraci√≥n del sistema lo permitiera.

### Fase 4. Creaci√≥n de una m√°quina virtual

#### Tarea 10. Crear una m√°quina virtual

Para crear una m√°quina virtual, primero se debe montar el directorio que contiene la imagen ISO de Fedora Server 41 a trav√©s de NFS:

```bash
root@lq-d25:~# sudo mount -t nfs 10.22.146.216:/imagenes/fedora/41/isos/x86_64 /mnt
Created symlink /run/systemd/system/remote-fs.target.wants/rpc-statd.service ‚Üí /usr/lib/systemd/system/rpc-statd.service.
```

**Explicaci√≥n del comando**:
- `mount`: Comando para montar sistemas de archivos
- `-t nfs`: Especifica el tipo de sistema de archivos como NFS (Network File System)
- `10.22.146.216:/imagenes/fedora/41/isos/x86_64`: La direcci√≥n del servidor NFS y la ruta compartida
  - `10.22.146.216`: Direcci√≥n IP del servidor NFS
  - `/imagenes/fedora/41/isos/x86_64`: Ruta al directorio compartido que contiene las im√°genes ISO
- `/mnt`: Punto de montaje local donde ser√° accesible el contenido del directorio remoto

La salida muestra la creaci√≥n de un enlace simb√≥lico para el servicio `rpc-statd`, que es necesario para el funcionamiento de NFS. Este servicio ayuda a recuperar conexiones interrumpidas en caso de que el servidor NFS se reinicie.

Luego, se copia la imagen ISO a un directorio local, por ejemplo:

```bash
root@lq-d25:/mnt# sudo mkdir -p /ISO
root@lq-d25:/mnt# sudo cp /mnt/Fedora-Server-netinst-x86_64-41-1.4.iso /ISO/
```

**Explicaci√≥n de los comandos**:
- `mkdir -p /ISO`: Crea el directorio /ISO (la opci√≥n -p crea directorios padres si es necesario)
- `cp /mnt/Fedora-Server-netinst-x86_64-41-1.4.iso /ISO/`: Copia la imagen ISO al directorio local
  - `Fedora-Server-netinst-x86_64-41-1.4.iso`: Imagen de instalaci√≥n por red (netinst) de Fedora Server 41
  - Este tipo de imagen contiene solo los componentes m√≠nimos necesarios para iniciar la instalaci√≥n, descargando el resto de paquetes desde Internet durante el proceso

Copiar la imagen ISO localmente proporciona varias ventajas:
- Mayor velocidad de instalaci√≥n (acceso local vs. red)
- Independencia de la disponibilidad del servidor NFS
- Posibilidad de reutilizar la imagen para m√∫ltiples instalaciones

Despu√©s de copiar la imagen, se desmonta el directorio /mnt:

```bash
root@lq-d25:/# sudo umount /mnt
```

**Explicaci√≥n del comando**:
- `umount`: Comando para desmontar sistemas de archivos
- `/mnt`: El punto de montaje que se va a desmontar

Es una buena pr√°ctica desmontar sistemas de archivos en red cuando ya no son necesarios para:
- Liberar recursos de red
- Evitar posibles problemas de consistencia de datos
- Eliminar dependencias de servicios externos

A continuaci√≥n, se utiliza la herramienta gr√°fica virt-manager para crear la m√°quina virtual. Se deben proporcionar los siguientes par√°metros:
1. Nombre de la m√°quina virtual: mvp1
2. Medio de instalaci√≥n: Imagen ISO local 
 
Ilustraci√≥n 1. Herramienta virt-manager. Configuraci√≥n del medio de instalaci√≥n del sistema operativo
3. Cantidad de memoria: 2GB
4. N√∫mero de procesadores: 1
5. Disco de la m√°quina virtual: 10GB
6. Interfaz de red: 1 interfaz en modo NAT

Este proceso a trav√©s de la interfaz gr√°fica es equivalente a ejecutar un comando de l√≠nea como:

```bash
virt-install --name mvp1 --memory 2048 --vcpus 1 --disk size=10 \
  --cdrom /ISO/Fedora-Server-netinst-x86_64-41-1.4.iso \
  --os-variant fedora41 --network network=default
```

Donde:
- `--name mvp1`: Asigna el nombre "mvp1" a la m√°quina virtual
- `--memory 2048`: Asigna 2GB de RAM (2048MB)
- `--vcpus 1`: Configura un procesador virtual
- `--disk size=10`: Crea un disco virtual de 10GB
- `--cdrom`: Especifica la ubicaci√≥n de la imagen ISO de instalaci√≥n
- `--os-variant`: Optimiza la configuraci√≥n seg√∫n el sistema operativo a instalar
- `--network network=default`: Configura una interfaz de red conectada a la red "default" (NAT)

Una vez creada la m√°quina virtual, se inicia la instalaci√≥n del sistema operativo Fedora Server 41.  Se debe realizar una instalaci√≥n m√≠nima y habilitar la cuenta de administraci√≥n root con acceso SSH. 
 
Ilustraci√≥n 2. Herramienta virt-manager. Configuraci√≥n del usuario root

Una vez terminada la instalaci√≥n, se comprueba la conexi√≥n SSH a nuestra m√°quina virtual:

```bash
root@lq-d25:/# ssh root@192.168.122.123
The authenticity of host '192.168.122.123 (192.168.122.123)' can't be established.
ED25519 key fingerprint is SHA256:gRFGvZlUIel5P1EJEdiEEgvXQ48k7iMy9Oz5SDPY2h4.
This key is not known by any other names.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.122.123' (ED25519) to the list of known hosts.
root@192.168.122.123's password: 
Web console: https://localhost:9090/ or https://192.168.122.123:9090/

Last login: Fri Feb 14 19:37:07 2025
root@localhost:~# ls
anaconda-ks.cfg
```

**Explicaci√≥n del comando**:
- `ssh root@192.168.122.123`: Inicia una conexi√≥n SSH como usuario root a la m√°quina virtual
  - `192.168.122.123`: Direcci√≥n IP asignada a la m√°quina virtual en la red NAT

El proceso de primera conexi√≥n SSH incluye:

1. Verificaci√≥n de la autenticidad del host (al ser la primera conexi√≥n, la clave no est√° en known_hosts)
2. Presentaci√≥n de la huella (fingerprint) de la clave del servidor para verificaci√≥n manual
3. Solicitud de confirmaci√≥n para continuar la conexi√≥n
4. Adici√≥n de la clave al archivo de hosts conocidos para futuras conexiones
5. Solicitud de contrase√±a para la autenticaci√≥n
6. Informaci√≥n sobre la consola web disponible en la VM
7. Informaci√≥n sobre el √∫ltimo inicio de sesi√≥n (si lo hubiera)

Tras iniciar sesi√≥n, el comando `ls` muestra un archivo `anaconda-ks.cfg`, que es el archivo de kickstart generado autom√°ticamente durante la instalaci√≥n. Este archivo podr√≠a utilizarse para automatizar futuras instalaciones con la misma configuraci√≥n.

Una vez finalizada la instalaci√≥n del sistema operativo en la m√°quina virtual, se deben realizar los siguientes pasos de configuraci√≥n:

Instalar el servicio qemu-gest-agent

El servicio qemu-guest-agent se utiliza para mejorar la integraci√≥n entre la m√°quina virtual y el sistema anfitri√≥n. Permite realizar tareas como sincronizar la hora, obtener informaci√≥n del sistema y ejecutar comandos en la m√°quina virtual desde el anfitri√≥n. Para instalar este servicio se ejecuta la siguiente orden:

```bash
root@localhost:~# sudo dnf install -y qemu-guest-agent
Actualizando y cargando repositorios:
Repositorios cargados.
Package                  Arch    Version                  Repository        Size
Installing:
qemu-guest-agent        x86_64  2:9.1.2-3.fc41           updates      962.9 KiB



Transaction Summary:
Installing:         1 package



El tama√±o total de paquetes entrantes es 313 KiB. Se necesita descargar 313 KiB.
Despu√©s de esta operaci√≥n, 963 KiB extra ser√°n utilizados (instalar 963 KiB, eliminar 0 B).
[1/1] qemu-guest-agent-2:9.1.2-3.fc41.x 100% | 569.4 KiB/s | 313.2 KiB |  00m01s
--------------------------------------------------------------------------------
[1/1] Total                             100% | 375.9 KiB/s | 313.2 KiB |  00m01s
Ejecutando transacci√≥n
[1/3] Verificar archivos de paquete     100% | 333.0   B/s |   1.0   B |  00m00s
[2/3] Preparar transacci√≥n             100% |  18.0   B/s |   1.0   B |  00m00s
[3/3] Instalando qemu-guest-agent-2:9.1 100% |   1.2 MiB/s | 965.6 KiB |  00m01s
>>> Ejecutando post-install scriptlet: qemu-guest-agent-2:9.1.2-3.fc41.x86_64
>>> Finalizado post-install scriptlet: qemu-guest-agent-2:9.1.2-3.fc41.x86_64
>>> Salida del scriptlet:
>>> Created symlink '/etc/systemd/system/dev-virtio\x2dports-org.qemu.guest_agen
>>> Unit /usr/lib/systemd/system/qemu-guest-agent.service is added as a dependen
>>> 
¬°Completado!
```

Este comando instala el paquete qemu-guest-agent utilizando el gestor de paquetes dnf. 

Habilitar e iniciar el servicio qemu-guest-agent

Una vez instalado el paquete, se debe habilitar e inicializar el servicio qemu-guest-agent para que se ejecute en el arranque del sistema. Para ello, se utiliza el siguiente comando:

```bash
root@localhost:~# sudo systemctl enable --now qemu-guest-agent
Unit /usr/lib/systemd/system/qemu-guest-agent.service is added as a dependency to a non-existent unit dev-virtio\x2dports-org.qemu.guest_agent.0.device.

Este comando habilita e inicia el servicio qemu-guest-agent. La salida del comando indica que el servicio se ha a√±adido como dependencia a una unidad no existente, esto se debe a que el servicio qemu-guest-agent depende de la disponibilidad de dispositivos virtio, que se crean din√°micamente cuando la m√°quina virtual se inicia. 

Establecer el nombre del Sistema

Para establecer el nombre del sistema en la m√°quina virtual como mvp1.vpd.com, se utiliza el siguiente comando:

```bash
root@localhost:~# sudo hostnamectl set-hostname mvp1.vpd.com

Este comando establece el nombre de host est√°tico de la m√°quina virtual como mvp1.vpd.com.

Para verificar que el nombre del sistema se ha establecido correctamente:

```bash
root@localhost:~# hostnamectl
     Static hostname: mvp1.vpd.com
           Icon name: computer-vm
             Chassis: vm üñ¥
          Machine ID: 6d2630bf3d2046a589c37eaa7313994b
             Boot ID: 4ee2bc753f724da7a1b60f1c189b497f
        Product UUID: 77722a52-4d29-4f91-a688-453a1fbfad52
      Virtualization: kvm
    Operating System: Fedora Linux 41 (Server Edition)    
         CPE OS Name: cpe:/o:fedoraproject:fedora:41
      OS Support End: Mon 2025-12-15
OS Support Remaining: 9month 4w 1d
              Kernel: Linux 6.12.11-200.fc41.x86_64
        Architecture: x86-64
     Hardware Vendor: QEMU
      Hardware Model: Standard PC _Q35 + ICH9, 2009_
    Firmware Version: 1.16.3-1.fc39
       Firmware Date: Tue 2014-04-01
        Firmware Age: 10y 10month 2w 1d      
```

Configurar el acceso SSH con clave p√∫blico/privada

Para permitir el acceso SSH desde el sistema anfitri√≥n a la m√°quina virtual utilizando autenticaci√≥n con clave p√∫blica/privada, se siguen estos pasos:

Generar un par de claves SSH en el sistema anfitri√≥n:

```bash
root@lq-d25:/# ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa
Generating public/private rsa key pair.
/root/.ssh/id_rsa already exists.
Overwrite (y/n)? y
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /root/.ssh/id_rsa
Your public key has been saved in /root/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:4zKf+4PBMp2TVNCOTBiZNHFCKfjLvniX91nfaJ7yvLs root@lq-d25.vpc.com
The key's randomart image is:
+---[RSA 4096]----+
|   . oBBoo       |
|  . . =+. o      |
|   . . o +       |
|    .   + .      |
|   . . +So       |
|    o o.B.       |
|   .  o+.+  .    |
|   .o o+o..o.o.+ |
|  ...o .+++. =E+.|
+----[SHA256]-----+
```

El comando ssh-keygen genera un par de claves SSH. 

A continuaci√≥n, se copia la clave p√∫blica al sistema de la m√°quina virtual:

```bash
root@lq-d25:/# ssh-copy-id root@192.168.122.123
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
root@192.168.122.123's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'root@192.168.122.123'"
and check to make sure that only the key(s) you wanted were added. 
```

El comando ssh-copy-id copia la clave p√∫blica al archivo authorized_keys del usuario root en la m√°quina virtual. 

Con eso, se completa la configuraci√≥n del acceso SSH con la clave p√∫blica/privada. Ahora se puede iniciar sesi√≥n en la m√°quina virtual desde el sistema anfitri√≥n utilizando el siguiente comando sin necesidad de introducir la contrase√±a:

```bash
root@lq-d25:/# ssh root@192.168.122.123
Web console: https://localhost:9090/ or https://192.168.122.123:9090/
```

Configurar el Sistema anfitri√≥n para poder realizar conexiones TCP/IP a la m√°quina virtual utilizando el identificador FQDN de esta

Para acceder a la m√°quina virtual mediante SSH utilizando su nombre de dominio mvp1.vpd.com, primero se debe obtener la direcci√≥n IP de la m√°quina virtual:

```bash
root@mvp1:~# ip a | grep 'inet' | grep -v '127.0.0.1'
    inet6 ::1/128 scope host noprefixroute 
    inet 192.168.122.123/24 brd 192.168.122.255 scope global dynamic noprefixroute enp1s0
    inet6 fe80::5054:ff:fe33:86f/64 scope link noprefixroute
```

A continuaci√≥n, se debe editar el archivo /etc/hosts en el sistema anfitri√≥n y a√±adir una l√≠nea que asocie la direcci√≥n IP de la m√°quina virtual con su nombre de dominio. Para ello se utiliza un editor de texto como nano:

```bash
root@lq-d25:/# sudo nano etc/hosts 
```

Se a√±ade la siguiente l√≠nea al final del archivo:

```bash
192.168.122.123‚ÄÉ‚ÄÉmvp1.vpd.com   
```

Donde 192.168.122.123 es la direcci√≥n IP de la m√°quina virtual obtenida anteriormente. 

Despu√©s de guardar los cambios en el archivo se puede comprobar la conexi√≥n con la m√°quina virtual utilizando el comando ping:

```bash
root@lq-d25:/# ping -c 4 mvp1.vpd.com
PING mvp1.vpd.com (192.168.122.123) 56(84) bytes of data.
64 bytes from mvp1.vpd.com (192.168.122.123): icmp_seq=1 ttl=64 time=0.309 ms
64 bytes from mvp1.vpd.com (192.168.122.123): icmp_seq=2 ttl=64 time=0.399 ms
64 bytes from mvp1.vpd.com (192.168.122.123): icmp_seq=3 ttl=64 time=0.227 ms
64 bytes from mvp1.vpd.com (192.168.122.123): icmp_seq=4 ttl=64 time=0.394 ms

--- mvp1.vpd.com ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3059ms
rtt min/avg/max/mdev = 0.227/0.332/0.399/0.070 ms
```

Finalmente, se puede iniciar sesi√≥n en la m√°quina virtual mediante SSH utilizando su nombre de dominio:

```bash
root@lq-d25:/# ssh root@mvp1.vpd.com
The authenticity of host 'mvp1.vpd.com (192.168.122.123)' can't be established.
ED25519 key fingerprint is SHA256:gRFGvZlUIel5P1EJEdiEEgvXQ48k7iMy9Oz5SDPY2h4.
This host key is known by the following other names/addresses:
    ~/.ssh/known_hosts:1: 192.168.122.123
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added 'mvp1.vpd.com' (ED25519) to the list of known hosts.
Web console: https://localhost:9090/ or https://192.168.122.123:9090/

Last login: Fri Feb 14 19:54:07 2025 from 192.168.122.1
```

Se inicia sesi√≥n correctamente en la m√°quina virtual sin necesidad de introducir la contrase√±a, lo que indica que el acceso SSH con clave p√∫blica/privada est√° configurado correctamente.

## Pruebas y Validaci√≥n

Una vez finalizadas las tareas, se procede a la verificaci√≥n de la instalaci√≥n y configuraci√≥n del sistema anfitri√≥n y la m√°quina virtual:

Verificar el modo de ejecuci√≥n de SELinux

Se utiliza el comando getenforce para verificar que SELinux se est√° ejecutando en modo "enforcing".

```bash
root@lq-d25:/# getenforce
Enforcing
```

**Explicaci√≥n del comando**:
- `getenforce`: Herramienta espec√≠fica para consultar el modo de ejecuci√≥n actual de SELinux
- A diferencia de `sestatus` que proporciona informaci√≥n completa, este comando muestra solo el modo actual de funcionamiento
- Posibles valores de salida:
  - `Enforcing`: SELinux est√° aplicando las pol√≠ticas de seguridad (m√°xima protecci√≥n)
  - `Permissive`: SELinux registra las violaciones de pol√≠ticas pero no las bloquea (modo de prueba)
  - `Disabled`: SELinux est√° completamente desactivado (sin protecci√≥n)

El modo `Enforcing` es fundamental en entornos de virtualizaci√≥n para garantizar el aislamiento entre m√°quinas virtuales y el sistema anfitri√≥n, previniendo posibles escaladas de privilegios.

Verificar el estado del servicio virtnetworkd

Se utiliza el comando:

```bash
root@lq-d25:/# systemctl status virtnetworkd
‚óè virtnetworkd.service - Virtualization network daemon
     Loaded: loaded (/usr/lib/systemd/system/virtnetworkd.service; enabled; pre>
    Drop-In: /usr/lib/systemd/system/service.d
             ‚îî‚îÄ10-timeout-abort.conf
     Active: active (running) since Fri 2025-02-14 19:18:01 WET; 1h 9min ago
TriggeredBy: ‚óè virtnetworkd-ro.socket
             ‚óè virtnetworkd-admin.socket
             ‚óè virtnetworkd.socket
       Docs: man:virtnetworkd(8)
             https://libvirt.org
   Main PID: 5729 (virtnetworkd)
      Tasks: 21 (limit: 76221)
     Memory: 12.2M
        CPU: 648ms
     CGroup: /system.slice/virtnetworkd.service
             ‚îú‚îÄ1251 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/defa>
             ‚îú‚îÄ1252 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/defa>
             ‚îî‚îÄ5729 /usr/sbin/virtnetworkd --timeout 120
```

**Explicaci√≥n del comando**:
- `systemctl status virtnetworkd`: Consulta el estado detallado del servicio de red de virtualizaci√≥n
- La informaci√≥n mostrada incluye:
  - Estado del servicio: `active (running)` indica funcionamiento correcto
  - Tiempo de ejecuci√≥n desde el √∫ltimo inicio: `since Fri 2025-02-14 19:18:01`
  - Sockets que pueden activar el servicio: `virtnetworkd-ro.socket`, etc.
  - Utilizaci√≥n de recursos: memoria, CPU y n√∫mero de tareas
  - Procesos asociados: proceso principal y subprocesos como dnsmasq

El servicio `virtnetworkd` es esencial porque gestiona las interfaces de red virtuales que permiten la comunicaci√≥n entre m√°quinas virtuales y con el exterior. Tambi√©n proporciona servicios DHCP y DNS a las VMs.

Verificar la carga de los m√≥dulos del kernel kvm y kvm_amd

```bash
root@lq-d25:/# lsmod | grep kvm
kvm_amd               217088  3
kvm                  1441792  2 kvm_amd
ccp                   180224  1 kvm_amd
```

Verificar la configuraci√≥n de la m√°quina virtual mvp1

Se utiliza el siguiente comando:

```bash
root@lq-d25:/# virsh dominfo mvp1
Id:             -
Nombre:         mvp1
UUID:           77722a52-4d29-4f91-a688-453a1fbfad52
Tipo de sistema operatuvo: hvm
Estado:         apagado
CPU(s):         1
Memoria m√°xima: 2097152 KiB
Memoria utilizada: 2097152 KiB
Persistente:    si
Autoinicio:     desactivar
Guardar administrado: no
Modelo de seguridad: selinux
DOI de seguridad: 0
```

**Explicaci√≥n del comando**:
- `virsh dominfo`: Muestra informaci√≥n detallada sobre un dominio (m√°quina virtual) espec√≠fico
  - `domain` en la terminolog√≠a de libvirt se refiere a una instancia de m√°quina virtual
- `mvp1`: Nombre del dominio sobre el que se solicita informaci√≥n

La salida proporciona datos esenciales sobre la configuraci√≥n de la VM:
- `Id`: El identificador num√©rico del dominio (- indica que est√° apagado)
- `UUID`: Identificador √∫nico universal de la m√°quina virtual
- `Tipo de sistema operativo`: `hvm` indica virtualizaci√≥n completa asistida por hardware
- `Estado`: Situaci√≥n actual de la VM (apagado, en ejecuci√≥n, pausado, etc.)
- `CPU(s)`: N√∫mero de CPUs virtuales asignadas (1 en este caso)
- `Memoria m√°xima/utilizada`: Memoria RAM asignada (2GB aproximadamente)
- `Persistente`: Indica si la definici√≥n de la VM se mantiene tras reiniciar el host
- `Modelo de seguridad`: Mecanismo de aislamiento utilizado (SELinux en este caso)

Esta informaci√≥n es cr√≠tica para verificar que la m√°quina virtual se ha configurado correctamente con los par√°metros especificados durante la creaci√≥n.

Verificar la interfaz de red de la m√°quina virtual mvp1

Se utiliza el comando:

```bash
root@lq-d25:/# virsh domiflist mvp1
Interfaz   Tipo      Fuente    Modelo   MAC
------------------------------------------------------------
-          network   default   virtio   52:54:00:33:08:6f
```

**Explicaci√≥n del comando**:
- `virsh domiflist`: Lista las interfaces de red conectadas a un dominio espec√≠fico
- `mvp1`: Nombre del dominio cuyas interfaces se desean listar

La salida muestra informaci√≥n detallada de la interfaz de red:
- `Tipo`: `network` indica que est√° conectada a una red virtual definida en libvirt
- `Fuente`: `default` es el nombre de la red virtual (t√≠picamente configurada con NAT)
- `Modelo`: `virtio` indica que utiliza el controlador paravirtualizado para mejor rendimiento
- `MAC`: La direcci√≥n MAC asignada a la interfaz virtual

El uso de `virtio` como modelo de dispositivo es importante porque proporciona mejor rendimiento que los dispositivos emulados, ya que la VM utiliza controladores espec√≠ficamente dise√±ados para entornos virtualizados.

Verificar la conectividad de la m√°quina virtual mvp1

Se utiliza el comando ping para verificar la conectividad de la m√°quina virtual mvp1 con el host anfitri√≥n y con el exterior.

Conectividad con el host anfitri√≥n: Se ejecuta el siguiente comando desde el sistema anfitri√≥n.

```bash
root@lq-d25:/# ping -c 4 mvp1.vpd.com
PING mvp1.vpd.com (192.168.122.123) 56(84) bytes of data.
64 bytes from mvp1.vpd.com (192.168.122.123): icmp_seq=1 ttl=64 time=0.252 ms
64 bytes from mvp1.vpd.com (192.168.122.123): icmp_seq=2 ttl=64 time=0.385 ms
64 bytes from mvp1.vpd.com (192.168.122.123): icmp_seq=3 ttl=64 time=0.367 ms
^C
--- mvp1.vpd.com ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2051ms
rtt min/avg/max/mdev = 0.252/0.334/0.385/0.058 ms
```

**Explicaci√≥n del comando**:
- `ping`: Herramienta b√°sica para verificar la conectividad IP mediante paquetes ICMP Echo
- `-c 4`: Limita el env√≠o a 4 paquetes (el comando se interrumpi√≥ manualmente con Ctrl+C despu√©s de 3)
- `mvp1.vpd.com`: El nombre de dominio de la m√°quina virtual (resuelto a trav√©s del archivo hosts)

La salida muestra:
- La resoluci√≥n correcta del nombre `mvp1.vpd.com` a la IP `192.168.122.123`
- Tiempos de respuesta muy bajos (menos de 1ms), t√≠picos de conexiones locales virtualizadas
- 0% de p√©rdida de paquetes, indicando conectividad estable

Este test valida tanto la configuraci√≥n del nombre en `/etc/hosts` como la conectividad IP b√°sica entre el host y la m√°quina virtual.

Conectividad con el exterior: Se ejecuta el siguiente comando desde la m√°quina virtual.

```bash
root@mvp1:~# ping -4 8.8.8.8
PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
64 bytes from 8.8.8.8: icmp_seq=1 ttl=114 time=30.0 ms
64 bytes from 8.8.8.8: icmp_seq=2 ttl=114 time=30.3 ms

^C--- 8.8.8.8 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 30.038/30.147/30.257/0.109 ms
```

**Explicaci√≥n del comando**:
- `ping -4`: Realiza un ping utilizando espec√≠ficamente el protocolo IPv4
- `8.8.8.8`: Direcci√≥n IP de uno de los servidores DNS p√∫blicos de Google, com√∫nmente usado para pruebas de conectividad a Internet

La salida muestra:
- Tiempos de respuesta de aproximadamente 30ms, t√≠picos para conexiones a Internet
- 0% de p√©rdida de paquetes, indicando conectividad estable
- Un valor de TTL (Time To Live) de 114, que representa el n√∫mero de saltos de red restantes

Esta prueba verifica que la configuraci√≥n de NAT funciona correctamente, permitiendo que la m√°quina virtual acceda a Internet a trav√©s del host.

Verificar la instalaci√≥n y el estado del agente qemu-guest-agent

```bash
root@mvp1:~# systemctl status qemu-guest-agent
‚óè qemu-guest-agent.service - QEMU Guest Agent
     Loaded: loaded (/usr/lib/systemd/system/qemu-guest-agent.service; enabled;>
    Drop-In: /usr/lib/systemd/system/service.d
             ‚îî‚îÄ10-timeout-abort.conf, 50-keep-warm.conf
     Active: active (running) since Fri 2025-02-14 20:36:53 WET; 2min 51s ago
Invocation: 55601c5735c74c248c7a6524e9b1e551
   Main PID: 857 (qemu-ga)
      Tasks: 2 (limit: 2307)
     Memory: 2.3M (peak: 2.6M)
        CPU: 5ms
     CGroup: /system.slice/qemu-guest-agent.service
             ‚îî‚îÄ857 /usr/bin/qemu-ga --method=virtio-serial --path=/dev/virtio-p>
```

**Explicaci√≥n del comando**:
- `systemctl status qemu-guest-agent`: Muestra el estado detallado del servicio qemu-guest-agent dentro de la m√°quina virtual
- Este agente facilita la comunicaci√≥n entre el host y la m√°quina virtual para operaciones como:
  - Obtenci√≥n de informaci√≥n de la VM (IP, hostname, usuarios conectados)
  - Ejecuci√≥n de comandos desde el host en la VM
  - Sincronizaci√≥n de tiempo
  - Gesti√≥n de copias de seguridad (quiescing)
  - Apagado ordenado de la VM desde el host

La salida confirma que el servicio est√° activo (`active (running)`), usando aproximadamente 2.3MB de memoria y con un tiempo de CPU m√≠nimo (5ms), lo que indica un funcionamiento eficiente.

Verificar el acceso SSH a la m√°quina virtual

```bash
root@lq-d25:/# ssh root@mvp1.vpd.com
Web console: https://mvp1.vpd.com:9090/ or https://192.168.122.123:9090/

Last login: Fri Feb 14 20:38:37 2025 from 192.168.122.1
```

**Explicaci√≥n del comando**:
- `ssh root@mvp1.vpd.com`: Intenta establecer una conexi√≥n SSH como usuario root al host mvp1.vpd.com
- El acceso sin solicitud de contrase√±a demuestra que:
  1. La autenticaci√≥n por clave p√∫blica est√° correctamente configurada
  2. El nombre de host se resuelve correctamente a la direcci√≥n IP de la VM
  3. El servidor SSH en la m√°quina virtual est√° operativo
  4. La red est√° correctamente configurada para permitir conexiones TCP en el puerto 22

Esta prueba final valida la integraci√≥n completa de todos los componentes configurados: red virtual, resoluci√≥n de nombres, autenticaci√≥n SSH y configuraci√≥n de la m√°quina virtual.

## Soluci√≥n de Problemas Comunes

Durante la configuraci√≥n de un entorno de virtualizaci√≥n KVM pueden surgir diferentes problemas. A continuaci√≥n, se presentan algunas situaciones comunes y sus posibles soluciones:

### Problemas con la virtualizaci√≥n del procesador

**Problema**: El sistema indica que no soporta la virtualizaci√≥n por hardware.

**Soluci√≥n**: 
1. Verificar que el procesador soporta virtualizaci√≥n con el comando `lscpu | grep Virtualization`
2. Si el procesador soporta virtualizaci√≥n pero no aparece habilitada, es posible que est√© desactivada en la BIOS/UEFI. Entrar en la configuraci√≥n de la BIOS/UEFI durante el arranque y activar las opciones de virtualizaci√≥n (VT-x para Intel o AMD-V para AMD).

### Problemas con SELinux

**Problema**: La m√°quina virtual no puede acceder a recursos compartidos.

**Soluci√≥n**:
1. Verificar el estado de `virt_use_nfs` con `getsebool virt_use_nfs`
2. Si est√° desactivado, activarlo con `setsebool -P virt_use_nfs on`
3. Para compartir sistemas de archivos locales: `setsebool -P virt_use_samba on`

### Problemas de red

**Problema**: La m√°quina virtual no tiene conexi√≥n a Internet o al sistema anfitri√≥n.

**Soluci√≥n**:
1. Verificar que el servicio `virtnetworkd` est√° activo: `systemctl status virtnetworkd`
2. Comprobar la configuraci√≥n de red en la m√°quina virtual con `virsh domiflist nombre_vm`
3. Verificar la configuraci√≥n de IP con `ip addr show` dentro de la m√°quina virtual
4. Asegurarse de que el firewall no est√° bloqueando las conexiones: `firewall-cmd --list-all`
5. Si es necesario, permitir el tr√°fico de virtualizaci√≥n: `firewall-cmd --permanent --add-rich-rule='rule service name=libvirt accept'`

### Problemas con qemu-guest-agent

**Problema**: Error al iniciar qemu-guest-agent en la m√°quina virtual.

**Soluci√≥n**:
1. Verificar que el paquete est√° correctamente instalado: `rpm -q qemu-guest-agent`
2. Comprobar que el canal virtio est√° habilitado en la configuraci√≥n de la m√°quina virtual
3. Reiniciar el servicio: `systemctl restart qemu-guest-agent`
4. Si persiste el error, verificar los logs: `journalctl -u qemu-guest-agent`

### Problemas de rendimiento

**Problema**: La m√°quina virtual tiene un rendimiento lento.

**Soluci√≥n**:
1. Verificar la asignaci√≥n de recursos (CPU, memoria) con `virsh dominfo nombre_vm`
2. Ajustar la configuraci√≥n seg√∫n sea necesario con `virsh edit nombre_vm`
3. Comprobar que no hay procesos consumiendo excesivos recursos en el anfitri√≥n con `top` o `htop`
4. Considerar el uso de dispositivos virtio para mejor rendimiento (especialmente para discos e interfaces de red)

## Conclusiones

En esta pr√°ctica se ha completado con √©xito la instalaci√≥n y configuraci√≥n de un entorno de virtualizaci√≥n KVM en Fedora Linux. A continuaci√≥n, se resumen los principales logros y conocimientos adquiridos:

1. **Configuraci√≥n del sistema anfitri√≥n**: Se configur√≥ adecuadamente el sistema operativo base, asegurando que SELinux estuviera en modo enforcing para mantener la seguridad del sistema.

2. **Verificaci√≥n de requisitos**: Se comprob√≥ que el hardware del sistema cumpl√≠a con los requisitos necesarios para la virtualizaci√≥n, destacando la presencia de extensiones de virtualizaci√≥n en el procesador (AMD-V).

3. **Instalaci√≥n de herramientas de virtualizaci√≥n**: Se instalaron correctamente los paquetes necesarios para KVM, tanto con interfaz gr√°fica como en modo headless.

4. **Creaci√≥n de m√°quina virtual**: Se cre√≥ una m√°quina virtual con Fedora Server 41, configurando correctamente sus par√°metros de red, acceso SSH y nombre de host.

5. **Configuraci√≥n avanzada**: Se implement√≥ la autenticaci√≥n SSH por clave p√∫blica/privada y se configur√≥ el sistema para poder acceder a la m√°quina virtual mediante su FQDN.

El entorno implementado proporciona una base s√≥lida para desplegar distintos servicios virtualizados, aprovechando las capacidades de KVM y las herramientas de gesti√≥n proporcionadas por libvirt.

## Bibliograf√≠a

1. Documentaci√≥n oficial de KVM: [https://www.linux-kvm.org/page/Documents](https://www.linux-kvm.org/page/Documents)

2. Documentaci√≥n de Fedora sobre virtualizaci√≥n: [https://docs.fedoraproject.org/en-US/quick-docs/virtualization-getting-started/](https://docs.fedoraproject.org/en-US/quick-docs/virtualization-getting-started/)

3. Red Hat Documentation - Configuraci√≥n y administraci√≥n de la virtualizaci√≥n: [https://access.redhat.com/documentation/es-es/red_hat_enterprise_linux/9/html/configuring_and_managing_virtualization/index](https://access.redhat.com/documentation/es-es/red_hat_enterprise_linux/9/html/configuring_and_managing_virtualization/index)

4. Libvirt Wiki: [https://wiki.libvirt.org/](https://wiki.libvirt.org/)

5. SELinux Project Wiki: [https://selinuxproject.org/page/Main_Page](https://selinuxproject.org/page/Main_Page)
