# Práctica 5: Infraestructura de Red Virtual

## Índice

- [Práctica 5: Infraestructura de Red Virtual](#práctica-5-infraestructura-de-red-virtual)
  - [Índice](#índice)
  - [1. Introducción](#1-introducción)
  - [2. Requisitos Previos](#2-requisitos-previos)
  - [3. Desarrollo de la Práctica](#3-desarrollo-de-la-práctica)
    - [3.1. Preparación Inicial](#31-preparación-inicial)
      - [3.1.1. Creación de la Máquina Virtual mvp5](#311-creación-de-la-máquina-virtual-mvp5)
      - [3.1.2. Configuración de la Consola Serie](#312-configuración-de-la-consola-serie)
    - [3.2. Tarea 1: Creación de una Red de Tipo NAT](#32-tarea-1-creación-de-una-red-de-tipo-nat)
    - [3.3. Tarea 2: Añadir la Primera Interfaz de Red](#33-tarea-2-añadir-la-primera-interfaz-de-red)
    - [3.4. Tarea 3: Creación de una Red Aislada](#34-tarea-3-creación-de-una-red-aislada)
    - [3.5. Tarea 4: Añadir la Segunda Interfaz de Red](#35-tarea-4-añadir-la-segunda-interfaz-de-red)
    - [3.6. Tarea 5: Creación de una Tercera Interfaz de Red de Tipo Bridge](#36-tarea-5-creación-de-una-tercera-interfaz-de-red-de-tipo-bridge)
  - [4. Pruebas y Validación](#4-pruebas-y-validación)
    - [4.1. Validación General del Anfitrión](#41-validación-general-del-anfitrión)
    - [4.2. Validación de la Red NAT (Cluster)](#42-validación-de-la-red-nat-cluster)
    - [4.3. Validación de la Red Aislada (Almacenamiento)](#43-validación-de-la-red-aislada-almacenamiento)
    - [4.4. Validación del Bridge](#44-validación-del-bridge)
    - [4.5. Validación Específica de Redes Libvirt](#45-validación-específica-de-redes-libvirt)
  - [5. Solución de Problemas Comunes](#5-solución-de-problemas-comunes)
    - [Problema 1: Errores durante la creación/clonación de `mvp5`](#problema-1-errores-durante-la-creaciónclonación-de-mvp5)
    - [Problema 2: La consola serie no funciona](#problema-2-la-consola-serie-no-funciona)
    - [Problema 3: Errores durante la creación de la red NAT "Cluster"](#problema-3-errores-durante-la-creación-de-la-red-nat-cluster)
    - [Problema 4: Errores al añadir/configurar la primera interfaz (Red NAT)](#problema-4-errores-al-añadirconfigurar-la-primera-interfaz-red-nat)
    - [Problema 5: Errores durante la creación de la red Aislada "Almacenamiento"](#problema-5-errores-durante-la-creación-de-la-red-aislada-almacenamiento)
    - [Problema 6: Errores al añadir/configurar la segunda interfaz (Red Aislada)](#problema-6-errores-al-añadirconfigurar-la-segunda-interfaz-red-aislada)
  - [6. Conclusiones](#6-conclusiones)
  - [7. Bibliografía](#7-bibliografía)

## 1. Introducción

El objetivo fundamental de esta práctica es conocer los diferentes tipos de redes en entornos de virtualización y saber configurarlas. Libvirt usa el concepto de _switch virtual_, un componente software que opera en el anfitrión, al que se conectan las máquinas virtuales (MVs). El tráfico de red de las MVs es gobernado por este switch.

En Linux, el sistema anfitrión representa el switch virtual mediante una interfaz de red. Cuando el demonio `libvirtd` está activo, la interfaz de red por defecto que representa el switch virtual es `virbr0`. Esta interfaz se puede visualizar en el anfitrión mediante la orden `ip addr show`.

Por defecto, los switches virtuales operan en modo NAT (Network Address Translation). Sin embargo, también se pueden configurar en modo "Red Enrutada" y en modo "Red Aislada". Adicionalmente, es posible configurar una interfaz de red de la máquina virtual para que esté asociada a una interfaz de tipo bridge del anfitrión. En esta práctica se crearán y configurarán diferentes tipos de redes para comprender sus características y aplicaciones.

Información más detallada se encuentra en las siguientes fuentes bibliográficas:

- "Configuring and managing virtualization" [1]. El capítulo 17 de esta guía está dedicado a explicar la gestión de redes para las MVs y es muy recomendable su lectura.
- "Configurando la red IP con nmcli" [2]. Se trata de una guía rápida de Fedora donde se explica la gestión de redes con la interfaz `nmcli` (_NetworkManager Command Line Interface_).
- "Configuring and managing networking" [3]. Se trata de una guía más exhaustiva sobre la configuración de redes en Red Hat. Los capítulos 2 y 6 serán especialmente útiles para el desarrollo de la práctica.

## 2. Requisitos Previos

Para abordar esta práctica se debe haber completado la práctica 1 (Instalación de KVM. Creación e instalación de máquinas virtuales), ya que se utilizará una máquina virtual creada en dicha práctica como base.

## 3. Desarrollo de la Práctica

### 3.1. Preparación Inicial

Antes de comenzar con la configuración específica de las redes, es necesario preparar una máquina virtual (`mvp5`) clonada de `mvp1` y configurar el acceso por consola serie. Esto es crucial ya que la interfaz de red inicial será eliminada.

#### 3.1.1. Creación de la Máquina Virtual mvp5

Se clona la máquina virtual `mvp1` para crear `mvp5`. Posteriormente, se elimina la interfaz de red por defecto para reconfigurarla según los requerimientos de esta práctica.

1.  **Verificar las máquinas virtuales existentes**: Antes de clonar, se listan las MVs disponibles para confirmar el estado actual.

    ```bash
    root@lq-d25:~# virsh list --all
     Id   Nombre                   Estado
    -------------------------------------------
     1    mvp5                     ejecutando
     -    clon_copiando_ficheros   apagado
     -    clon_virt_clone          apagado
     -    clon_virt_manager        apagado
     -    Creacion_virt_install    apagado
     -    mvp1                     apagado
     -    mvp3                     apagado
     -    mvp4_lqd25               apagado
    ```

    **Explicación del comando**:

    - `virsh list --all`: Muestra todas las máquinas virtuales definidas en el sistema, tanto las que están en ejecución como las apagadas.

2.  **Clonar la máquina virtual `mvp1` para crear `mvp5`**: Se utiliza `virt-clone` especificando la MAC y la ruta del nuevo disco.

    ```bash
    root@lq-d25:~# virt-clone --original mvp1 --name mvp5 --file /var/lib/libvirt/images/mvp5.qcow2 --mac=00:16:3e:37:a0:05
    Allocating 'mvp5.qcow2'                                     | 2.0 GB  00:07 ...

    El clon 'mvp5' ha sido creado exitosamente.
    ```

    **Explicación del comando**:

    - `virt-clone`: Herramienta para clonar máquinas virtuales existentes.
    - `--original mvp1`: Especifica la máquina virtual de origen.
    - `--name mvp5`: Define el nombre de la nueva máquina virtual.
    - `--file /var/lib/libvirt/images/mvp5.qcow2`: Especifica la ruta y nombre del archivo de imagen para el nuevo disco virtual.
    - `--mac=00:16:3e:37:a0:05`: Establece una dirección MAC específica y única para la interfaz de red del clon. **Es crucial asignar una MAC diferente a la original para evitar conflictos en la red.**

3.  **Iniciar la máquina virtual `mvp5`**: Se arranca la MV recién clonada.

    ```bash
    root@lq-d25:~# virsh start mvp5
    Se ha iniciado el dominio mvp5
    ```

    **Explicación del comando**:

    - `virsh start mvp5`: Inicia la máquina virtual especificada (`mvp5`).

4.  **Verificar la dirección IP y la interfaz de red inicial**: Se comprueba la configuración de red asignada por defecto tras el clonado.

    ```bash
    root@lq-d25:~# virsh domifaddr mvp5
     Nombre     dirección MAC       Protocol     Address
    -------------------------------------------------------------------------------
     vnet0      00:16:3e:37:a0:05    ipv4         192.168.122.124/24
    ```

    **Explicación del comando**:

    - `virsh domifaddr mvp5`: Muestra las direcciones de las interfaces de red asociadas a la máquina virtual `mvp5`. Se observa la interfaz `vnet0` con la MAC especificada y una IP en la red `default` (192.168.122.0/24).

    ```bash
    root@lq-d25:~# virsh domiflist mvp5
     Interfaz   Tipo      Fuente    Modelo   MAC
    ------------------------------------------------------------
     vnet0      network   default   virtio   00:16:3e:37:a0:05
    ```

    **Explicación del comando**:

    - `virsh domiflist mvp5`: Lista las interfaces de red virtuales (`vnetX`) asociadas a la máquina virtual `mvp5`, indicando su tipo, fuente (red virtual), modelo y dirección MAC.

5.  **Eliminar la interfaz de red por defecto**: Se elimina la interfaz de red tanto de la configuración en ejecución como de la configuración persistente.

    ```bash
    root@lq-d25:~# virsh detach-interface mvp5 network --mac 00:16:3e:37:a0:05
    La interfaz ha sido desmontada exitosamente

    root@lq-d25:~# virsh detach-interface mvp5 network --mac 00:16:3e:37:a0:05 --config
    La interfaz ha sido desmontada exitosamente
    ```

    **Explicación del comando**:

    - `virsh detach-interface mvp5 network --mac <MAC>`: Desconecta una interfaz de red de la máquina virtual `mvp5` identificada por su dirección MAC.
    - `--config`: Aplica el cambio a la configuración persistente de la máquina virtual, asegurando que la interfaz no se vuelva a crear al reiniciar.

6.  **Verificar la eliminación de la interfaz**:

    ```bash
    root@lq-d25:~# virsh domiflist mvp5
     Interfaz   Tipo   Fuente   Modelo   MAC
    ------------------------------------------

    ```

    La salida vacía confirma que `mvp5` ya no tiene interfaces de red asociadas.

> **Nota**: Al eliminar la interfaz de red, la máquina virtual quedará sin conectividad. Para acceder a ella y continuar con la configuración, es imprescindible configurar la consola serie en la siguiente tarea.

#### 3.1.2. Configuración de la Consola Serie

Para poder acceder a la máquina virtual sin interfaz de red, se configura una consola serie. Este acceso es fundamental para las tareas posteriores de configuración de red.

1.  **Acceder a la máquina virtual `mvp5`**: Se utiliza `virt-manager` o `virsh console` (si ya estaba previamente configurada parcialmente o se accede por VNC) para iniciar sesión en la MV.

    ```bash
    root@lq-d25:~# virt-manager
    # O alternativamente, si es posible, conectar vía VNC/SPICE desde virt-manager
    ```

2.  **Editar el archivo de configuración de GRUB**: Dentro de `mvp5`, se modifica el archivo `/etc/default/grub` para habilitar la consola serie en el arranque.

    ```bash
    # Dentro de mvp5
    root@mvp5:~# vi /etc/default/grub
    ```

    Se localiza la línea `GRUB_CMDLINE_LINUX` y se añade `console=ttyS0`. El resultado debería ser similar a:

    ```
    GRUB_CMDLINE_LINUX="console=ttyS0 rd.lvm.lv=fedora/root rhgb quiet"
    ```

    **Explicación**:

    - `console=ttyS0`: Indica al kernel de Linux que utilice el primer puerto serie (`ttyS0`) como una consola del sistema, permitiendo el envío y recepción de mensajes del kernel y el login a través de él.

3.  **Eliminar opciones del kernel conflictivas (opcional pero recomendado)**: Para asegurar que la configuración de la consola serie tenga prioridad, se eliminan opciones previas del kernel almacenadas por GRUB.

    ```bash
    # Dentro de mvp5
    root@mvp5:~# grub2-editenv - unset kernelopts
    ```

    **Explicación del comando**:

    - `grub2-editenv - unset kernelopts`: Elimina la variable `kernelopts` del entorno de GRUB, la cual podría contener parámetros de arranque previos que interfieran con la nueva configuración.

4.  **Regenerar la configuración de GRUB**: Se aplica la configuración modificada generando un nuevo archivo `grub.cfg`.

    ```bash
    # Dentro de mvp5
    root@mvp5:~# grub2-mkconfig -o /boot/grub2/grub.cfg
    Generating grub configuration file ...
    # ... (output abreviado) ...
    done
    ```

    **Explicación del comando**:

    - `grub2-mkconfig -o /boot/grub2/grub.cfg`: Genera el archivo de configuración principal de GRUB (`/boot/grub2/grub.cfg`) basándose en las plantillas y la configuración de `/etc/default/grub`.

5.  **Reiniciar la máquina virtual**: Se reinicia `mvp5` para que arranque con los nuevos parámetros del kernel.

    ```bash
    # Dentro de mvp5
    root@mvp5:~# reboot
    ```

6.  **Probar la conexión a la consola serie desde el anfitrión**: Una vez reiniciada `mvp5`, se intenta la conexión desde el anfitrión `lq-d25`.

    ```bash
    root@lq-d25:~# virsh console mvp5
    Connected to domain 'mvp5'
    Escape character is ^] (Ctrl + ])

    # ... (Mensajes de arranque) ...
    mvp5 login: root
    Contraseña: <introducir_contraseña>
    Last login: Thu Apr  3 19:31:14 on tty1
    [root@mvp5 ~]#
    ```

    **Explicación del comando**:

    - `virsh console mvp5`: Establece una conexión con la consola serie de la máquina virtual `mvp5`.
    - El prompt de login (`mvp5 login:`) confirma que la configuración es exitosa.
    - Para salir de la consola serie, se utiliza la secuencia de escape `Ctrl + ]`.

Ahora se dispone de acceso a la máquina virtual `mvp5` a través de la consola serie, lo que permitirá configurar las interfaces de red en las siguientes tareas.

### 3.2. Tarea 1: Creación de una Red de Tipo NAT

En esta tarea se crea una red virtual de tipo NAT llamada "Cluster". Esta red permitirá a las máquinas virtuales conectadas a ella comunicarse entre sí, con el anfitrión, y acceder a redes externas (como Internet) a través del anfitrión, que realizará NAT.

> **Nota**: Aunque la ficha original de la práctica puede sugerir el uso de `virt-manager`, en esta documentación se utiliza la línea de comandos (`virsh` y archivos de definición XML) por su reproducibilidad y facilidad de documentación, siguiendo las buenas prácticas académicas.

1.  **Verificar las redes virtuales existentes**: Se comprueba qué redes están ya definidas en el sistema.

    ```bash
    root@lq-d25:~# virsh net-list --all
     Nombre    Estado   Inicio automático   Persistente
    -----------------------------------------------------
     default   activo   si                  si
    ```

    **Explicación del comando**:

    - `virsh net-list --all`: Muestra todas las redes virtuales definidas en libvirt, indicando su estado (activa/inactiva), si arrancan automáticamente con el sistema y si su definición es persistente.

2.  **Crear un archivo XML para definir la red "Cluster"**: Se crea un archivo (`cluster-network.xml`) que describe la configuración de la nueva red.

    ```bash
    root@lq-d25:~# cat > cluster-network.xml << EOF
    <network>
      <name>Cluster</name>
      <forward mode='nat'/>
      <bridge name='virbr1' stp='on' delay='0'/>
      <ip address='192.168.140.1' netmask='255.255.255.0'>
        <dhcp>
          <range start='192.168.140.2' end='192.168.140.149'/>
        </dhcp>
      </ip>
    </network>
    EOF
    ```

    **Explicación del archivo XML**:

    - `<network>`: Elemento raíz para la definición de una red virtual.
    - `<name>Cluster</name>`: Define el nombre de la red como "Cluster".
    - `<forward mode='nat'/>`: **Configura el modo de reenvío como NAT**. Esto indica que el tráfico de las MVs hacia redes externas será traducido (NAT) por el anfitrión.
    - `<bridge name='virbr1' stp='on' delay='0'/>`: Define un _switch virtual_ implementado como un bridge de Linux llamado `virbr1`. `stp='on'` habilita el protocolo Spanning Tree para prevenir bucles, y `delay='0'` establece el tiempo de espera del bridge.
    - `<ip address='192.168.140.1' netmask='255.255.255.0'>`: Asigna la dirección IP `192.168.140.1` con máscara `255.255.255.0` (equivalente a `/24`) a la interfaz del bridge (`virbr1`) en el anfitrión. Esta IP actuará como puerta de enlace para las MVs conectadas.
    - `<dhcp>`: Habilita el servicio DHCP integrado de libvirt (dnsmasq) para esta red.
    - `<range start='192.168.140.2' end='192.168.140.149'/>`: Define el rango de direcciones IP (`192.168.140.2` a `192.168.140.149`) que el servidor DHCP podrá asignar a las MVs que se conecten a esta red.

3.  **Definir la red a partir del archivo XML**: Se carga la definición de la red en libvirt.

    ```bash
    root@lq-d25:~# virsh net-define cluster-network.xml
    La red Cluster se encuentra definida desde cluster-network.xml
    ```

    **Explicación del comando**:

    - `virsh net-define cluster-network.xml`: Lee el archivo XML especificado y crea una definición persistente para la red "Cluster" en la configuración de libvirt. La red aún no está activa.

4.  **Iniciar la red**: Se activa la red "Cluster", creando el bridge `virbr1` y iniciando el proceso `dnsmasq` asociado.

    ```bash
    root@lq-d25:~# virsh net-start Cluster
    La red Cluster se ha iniciado
    ```

    **Explicación del comando**:

    - `virsh net-start Cluster`: Inicia la red virtual especificada.

5.  **Configurar la red para inicio automático**: Se asegura que la red "Cluster" se inicie automáticamente cuando el servicio `libvirtd` arranque.

    ```bash
    root@lq-d25:~# virsh net-autostart Cluster
    La red Cluster ha sido marcada para iniciarse automáticamente
    ```

    **Explicación del comando**:

    - `virsh net-autostart Cluster`: Marca la red especificada para que se inicie automáticamente con el sistema.

6.  **Verificar la creación y estado de la red**: Se confirma que la red "Cluster" aparece en la lista, está activa y configurada para autoarranque.

    ```bash
    root@lq-d25:~# virsh net-list --all
     Nombre    Estado   Inicio automático   Persistente
    -----------------------------------------------------
     Cluster   activo   si                  si
     default   activo   si                  si
    ```

7.  **Ver los detalles de la red**: Se muestra información específica sobre la red "Cluster".

    ```bash
    root@lq-d25:~# virsh net-info Cluster
    Nombre:         Cluster
    UUID:           7ee051d7-e38d-45ab-a26c-232a51e5162e
    Activar:        si
    Persistente:    si
    Autoinicio:     si
    Puente:         virbr1
    ```

    **Explicación del comando**:

    - `virsh net-info Cluster`: Proporciona detalles clave de la red, como su UUID, estado y el nombre del bridge asociado (`virbr1`).

8.  **Verificar la configuración del bridge en el sistema anfitrión**: Se utiliza `ip addr` para confirmar que la interfaz `virbr1` ha sido creada en el anfitrión y tiene la IP correcta.
    ```bash
    root@lq-d25:~# ip addr show virbr1
    7: virbr1: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
        link/ether 52:54:00:f5:97:55 brd ff:ff:ff:ff:ff:ff
        inet 192.168.140.1/24 brd 192.168.140.255 scope global virbr1
           valid_lft forever preferred_lft forever
    ```
    **Explicación de la salida**:
    - Se muestra la interfaz `virbr1` con la dirección MAC asignada por libvirt.
    - Tiene la dirección IP `192.168.140.1/24` configurada, como se definió en el XML.
    - El estado `DOWN` y `NO-CARRIER` es normal cuando no hay ninguna MV conectada todavía al bridge. Cambiará a `UP` cuando se conecte la primera MV.

Se ha creado exitosamente la red NAT "Cluster", que ahora está lista para ser utilizada por la máquina virtual `mvp5`.

### 3.3. Tarea 2: Añadir la Primera Interfaz de Red

En esta tarea se añade una interfaz de red a la máquina virtual `mvp5` para conectarla a la red NAT "Cluster" creada en la tarea anterior. Se configurará para obtener una dirección IP automáticamente vía DHCP.

1.  **Añadir la interfaz de red a `mvp5`**: Se utiliza `virsh attach-interface` para conectar la MV a la red "Cluster".

    ```bash
    root@lq-d25:~# virsh attach-interface mvp5 network Cluster --model virtio --config
    La interfaz ha sido asociada exitosamente
    ```

    **Explicación del comando**:

    - `virsh attach-interface mvp5`: Comando para añadir una interfaz de red a la MV `mvp5`.
    - `network Cluster`: Especifica que la interfaz debe conectarse a la red virtual `Cluster`. El tipo `network` indica conexión a una red gestionada por libvirt.
    - `--model virtio`: **Establece el modelo del controlador de red como `virtio`**. Este es un controlador paravirtualizado que ofrece un rendimiento significativamente mejor que los emulados (como `e1000` o `rtl8139`) al requerir controladores específicos en el sistema operativo invitado.
    - `--config`: Hace que la adición de la interfaz sea persistente en la configuración de la MV.

2.  **Verificar la interfaz añadida**: Se lista las interfaces de `mvp5` para ver la nueva interfaz (aún sin nombre `vnetX` asignado porque la MV necesita reiniciarse para detectarla activamente).

    ```bash
    root@lq-d25:~# virsh domiflist mvp5
     Interfaz   Tipo      Fuente    Modelo   MAC
    ------------------------------------------------------------
     -          network   Cluster   virtio   52:54:00:bd:89:a1
    ```

    **Nota**: Libvirt asigna automáticamente una dirección MAC si no se especifica una. El guión (`-`) en la columna `Interfaz` indica que aún no está activa en el host.

3.  **Reiniciar la máquina virtual**: Es necesario reiniciar `mvp5` para que el sistema operativo invitado detecte y configure la nueva interfaz de red.

    ```bash
    root@lq-d25:~# virsh reboot mvp5
    El dominio mvp5 está siendo reiniciado
    ```

4.  **Conectarse a la máquina virtual**: Se accede a `mvp5` mediante la consola serie configurada previamente.

    ```bash
    root@lq-d25:~# virsh console mvp5
    Connected to domain 'mvp5'
    Escape character is ^] (Ctrl + ])
    # ... login ...
    [root@mvp5 ~]#
    ```

5.  **Verificar la configuración de red en `mvp5` (Comprobación 1)**: Dentro de la MV, se utiliza `ip addr` para ver las interfaces y sus direcciones IP.

    ```bash
    [root@mvp5 ~]# ip addr
    1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
        inet 127.0.0.1/8 scope host lo
           valid_lft forever preferred_lft forever
        inet6 ::1/128 scope host noprefixroute
           valid_lft forever preferred_lft forever
    2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        link/ether 52:54:00:bd:89:a1 brd ff:ff:ff:ff:ff:ff
        inet 192.168.140.17/24 brd 192.168.140.255 scope global dynamic noprefixroute enp1s0
           valid_lft 3590sec preferred_lft 3590sec
        inet6 fe80::5054:ff:febd:89a1/64 scope link noprefixroute
           valid_lft forever preferred_lft forever
    ```

    **Explicación de la salida**:

    - La nueva interfaz es detectada por el sistema operativo como `enp1s0` (el nombre puede variar).
    - Tiene la dirección MAC `52:54:00:bd:89:a1` asignada por libvirt.
    - **Ha obtenido automáticamente la dirección IP `192.168.140.17/24`** del servidor DHCP de la red "Cluster" (`192.168.140.1`). La palabra `dynamic` confirma que fue obtenida por DHCP.
    - El estado es `UP`, indicando que la interfaz está activa y funcional.
    - El tiempo de validez (`valid_lft`) indica la duración de la concesión DHCP.

6.  **Configurar el archivo `/etc/hosts` en el anfitrión**: Para facilitar el acceso por nombre, se añade una entrada en el archivo `/etc/hosts` del anfitrión (`lq-d25`) que mapea la IP de la MV a un nombre de host.

    ```bash
    # En el anfitrión lq-d25
    root@lq-d25:~# echo "192.168.140.17 mvp5i1.vpd.com mvp5i1" >> /etc/hosts

    root@lq-d25:~# cat /etc/hosts | grep mvp5i1
    192.168.140.17 mvp5i1.vpd.com mvp5i1
    ```

    **Explicación**:

    - Se añade una línea al archivo `/etc/hosts` asociando la IP `192.168.140.17` con el FQDN `mvp5i1.vpd.com` y el alias `mvp5i1`. Esto permite usar estos nombres en lugar de la IP para referirse a la primera interfaz de `mvp5`.

7.  **Verificar la conectividad desde el anfitrión a la MV (Comprobación 2)**: Se utiliza `ping` desde el anfitrión hacia el nombre configurado para la MV.

    ```bash
    # En el anfitrión lq-d25
    root@lq-d25:~# ping -c 4 mvp5i1.vpd.com
    PING mvp5i1.vpd.com (192.168.140.17) 56(84) bytes of data.
    64 bytes from mvp5i1.vpd.com (192.168.140.17): icmp_seq=1 ttl=64 time=0.280 ms
    64 bytes from mvp5i1.vpd.com (192.168.140.17): icmp_seq=2 ttl=64 time=0.437 ms
    64 bytes from mvp5i1.vpd.com (192.168.140.17): icmp_seq=3 ttl=64 time=0.242 ms
    64 bytes from mvp5i1.vpd.com (192.168.140.17): icmp_seq=4 ttl=64 time=0.226 ms

    --- mvp5i1.vpd.com ping statistics ---
    4 packets transmitted, 4 received, 0% packet loss, time 3092ms
    rtt min/avg/max/mdev = 0.226/0.296/0.437/0.083 ms
    ```

    **Resultado**: La MV responde correctamente a los pings (0% packet loss), confirmando la conectividad básica entre el anfitrión y la MV a través de la red "Cluster".

8.  **Verificar el acceso a Internet desde la MV (Comprobación 3)**: Dentro de `mvp5`, se intenta hacer ping a un destino externo.

    ```bash
    # Dentro de mvp5
    [root@mvp5 ~]# ping -c 4 google.es
    PING google.es (142.250.184.163) 56(84) bytes of data.
    64 bytes from mad07s23-in-f3.1e100.net (142.250.184.163): icmp_seq=1 ttl=114 time=29.7 ms
    # ... (más respuestas) ...

    --- google.es ping statistics ---
    4 packets transmitted, 4 received, 0% packet loss, time 3004ms
    rtt min/avg/max/mdev = 29.738/30.110/30.441/0.267 ms
    ```

    **Resultado**: La MV puede alcanzar `google.es`, confirmando que la ruta usada es vía `192.168.140.1`.

9.  **Verificar la configuración de red completa en la MV**: Se revisan la tabla de rutas y la configuración de DNS.

    ```bash
    # Dentro de mvp5
    [root@mvp5 ~]# ip route
    default via 192.168.140.1 dev enp1s0 proto dhcp src 192.168.140.17 metric 100
    192.168.140.0/24 dev enp1s0 proto kernel scope link src 192.168.140.17 metric 100
    ```

    **Explicación de la tabla de rutas**:

    - `default via 192.168.140.1 dev enp1s0`: La ruta por defecto (para cualquier destino no local) apunta a `192.168.140.1` (la IP del bridge `virbr1` en el anfitrión) a través de la interfaz `enp1s0`. Fue configurada vía DHCP (`proto dhcp`).
    - `192.168.140.0/24 dev enp1s0`: Ruta para la red local `192.168.140.0/24`, accesible directamente a través de `enp1s0`.
    - `metric 100`: La métrica indica la prioridad de la ruta (menor es más preferible).

    ```bash
    # Dentro de mvp5
    [root@mvp5 ~]# cat /etc/resolv.conf
    # Generated by NetworkManager
    search .
    nameserver 192.168.140.1
    ```

    **Explicación de resolv.conf**:

    - `nameserver 192.168.140.1`: El servidor DNS configurado (probablemente también vía DHCP) es la IP del bridge `virbr1`. El proceso `dnsmasq` gestionado por libvirt actúa como servidor DNS y DHCP para la red virtual.

Estas comprobaciones confirman que la primera interfaz de red se ha configurado correctamente, conectada a la red NAT "Cluster", obteniendo IP por DHCP y con acceso tanto al anfitrión como a Internet.

### 3.4. Tarea 3: Creación de una Red Aislada

En esta tarea se crea una segunda red virtual llamada "Almacenamiento". A diferencia de la red "Cluster", esta será una **red aislada**. Las máquinas virtuales conectadas a ella podrán comunicarse entre sí y con el anfitrión, pero **no tendrán conectividad con redes externas** (como Internet) a través de esta red. Además, **no se configurará un servidor DHCP** para ella; las IPs deberán asignarse manualmente.

1.  **Crear un archivo XML para definir la red "Almacenamiento"**: Se crea el archivo `almacenamiento-network.xml`.

    ```bash
    root@lq-d25:~# cat > almacenamiento-network.xml << EOF
    <network>
      <name>Almacenamiento</name>
      <bridge name='virbr2' stp='on' delay='0'/>
      <ip address='10.22.122.1' netmask='255.255.255.0'>
      </ip>
    </network>
    EOF
    ```

    **Explicación del archivo XML**:

    - `<name>Almacenamiento</name>`: Define el nombre de la red.
    - `<bridge name='virbr2' stp='on' delay='0'/>`: Define un bridge `virbr2` para esta red.
    - `<ip address='10.22.122.1' netmask='255.255.255.0'>`: Asigna la IP `10.22.122.1/24` al bridge `virbr2` en el anfitrión.
    - **Ausencia de `<forward mode='...'/>`**: La omisión de la etiqueta `<forward>` es lo que define esta red como **aislada**. Libvirt no configurará reenvío ni NAT para el tráfico de esta red hacia el exterior.
    - **Ausencia de `<dhcp>`**: La omisión de la sección `<dhcp>` indica que no se iniciará un servidor DHCP para esta red.

2.  **Definir la red a partir del archivo XML**:

    ```bash
    root@lq-d25:~# virsh net-define almacenamiento-network.xml
    La red Almacenamiento se encuentra definida desde almacenamiento-network.xml
    ```

3.  **Iniciar la red**:

    ```bash
    root@lq-d25:~# virsh net-start Almacenamiento
    La red Almacenamiento ha sido iniciada
    ```

4.  **Configurar la red para inicio automático**:

    ```bash
    root@lq-d25:~# virsh net-autostart Almacenamiento
    La red Almacenamiento ha sido marcada para autoarranque
    ```

5.  **Verificar la creación y estado de la red**:

    ```bash
    root@lq-d25:~# virsh net-list --all
     Nombre           Estado   Inicio automático   Persistente
    ------------------------------------------------------------
     Almacenamiento   activo   si                  si
     Cluster          activo   si                  si
     default          activo   si                  si
    ```

    La red "Almacenamiento" aparece activa y configurada correctamente.

6.  **Ver los detalles de la red**:

    ```bash
    root@lq-d25:~# virsh net-info Almacenamiento
    Nombre:         Almacenamiento
    UUID:           5c2735be-366d-4c47-907c-8fbfcf600175
    Activar:        si
    Persistente:    si
    Autoinicio:     si
    Puente:         virbr2
    ```

    Confirma que el bridge asociado es `virbr2`.

7.  **Verificar la configuración del bridge en el sistema anfitrión**:
    ```bash
    root@lq-d25:~# ip addr show virbr2
    19: virbr2: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
        link/ether 52:54:00:fc:4f:fa brd ff:ff:ff:ff:ff:ff
        inet 10.22.122.1/24 brd 10.22.122.255 scope global virbr2
           valid_lft forever preferred_lft forever
    ```
    **Explicación de la salida**:
    - La interfaz `virbr2` se ha creado con la IP `10.22.122.1/24`.
    - Al igual que `virbr1`, su estado inicial es `DOWN` / `NO-CARRIER` hasta que se conecte una MV.

Se ha creado correctamente la red aislada "Almacenamiento", lista para conectar la segunda interfaz de `mvp5`.

### 3.5. Tarea 4: Añadir la Segunda Interfaz de Red

Se añade una segunda interfaz de red a `mvp5`, conectándola a la red aislada "Almacenamiento". Dado que esta red no tiene DHCP, la dirección IP deberá configurarse manualmente dentro de la máquina virtual.

1.  **Añadir la interfaz de red a `mvp5`**: Se conecta la MV a la red "Almacenamiento".

    ```bash
    root@lq-d25:~# virsh attach-interface mvp5 network Almacenamiento --model virtio --config
    La interfaz ha sido asociada exitosamente
    ```

    **Explicación del comando**:

    - Similar a la Tarea 2, pero conectando a `network Almacenamiento`.

2.  **Verificar las interfaces de `mvp5`**:

    ```bash
    root@lq-d25:~# virsh domiflist mvp5
     Interfaz   Tipo      Fuente           Modelo   MAC
    -------------------------------------------------------------------
     vnet0      network   Cluster          virtio   52:54:00:bd:89:a1
     -          network   Almacenamiento   virtio   52:54:00:e4:bf:90
    ```

    Ahora `mvp5` tiene dos interfaces definidas: una conectada a "Cluster" (ya activa como `vnet0`) y la nueva conectada a "Almacenamiento" (aún inactiva).

3.  **Reiniciar la máquina virtual**: Necesario para que `mvp5` detecte la nueva interfaz.

    ```bash
    root@lq-d25:~# virsh reboot mvp5
    El dominio mvp5 está siendo reiniciado
    ```

4.  **Conectarse a la máquina virtual**: Vía consola serie.

    ```bash
    root@lq-d25:~# virsh console mvp5
    # ... login ...
    [root@mvp5 ~]#
    ```

5.  **Verificar la detección de la nueva interfaz (Comprobación 1)**: Dentro de `mvp5`, se usa `ip addr`.

    ```bash
    [root@mvp5 ~]# ip addr
    1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 ...
       inet 127.0.0.1/8 ...
    2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> ... state UP ...
       link/ether 52:54:00:bd:89:a1 ...
       inet 192.168.140.17/24 ... dynamic ... enp1s0
    3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> ... state UP ... # Almacenamiento (Aislada)
       link/ether 52:54:00:e4:bf:90 ...
       inet 10.22.122.2/24 ... noprefixroute enp7s0
    ```

    **Explicación de la salida**:

    - Se detecta una nueva interfaz, nombrada `enp7s0` (el nombre puede variar).
    - Tiene la MAC `52:54:00:e4:bf:90` asignada por libvirt.
    - Está `UP`.
    - **No tiene dirección IPv4 asignada**, como era de esperar, ya que la red "Almacenamiento" no tiene DHCP. Solo tiene la dirección IPv6 link-local (`fe80::...`).

6.  **Configurar la dirección IP estática**: Se utiliza `nmcli` (NetworkManager Command Line Interface) para configurar la dirección IP estática `10.22.122.2/24` para la interfaz `enp7s0`.

    ```bash
    # Dentro de mvp5
    [root@mvp5 ~]# nmcli connection add type ethernet con-name Almacenamiento ifname enp7s0 ipv4.method manual ipv4.addresses 10.22.122.2/24
    Connection 'Almacenamiento' (uuid) successfully added.
    ```

    **Explicación del comando**:

    - `nmcli connection add`: Añade una nueva configuración de conexión de NetworkManager.
    - `type ethernet`: Especifica el tipo de conexión.
    - `con-name Almacenamiento`: Asigna un nombre descriptivo ("Almacenamiento") a esta configuración de conexión.
    - `ifname enp7s0`: Asocia esta configuración a la interfaz física `enp7s0`.
    - `ipv4.method manual`: Establece el método de configuración IPv4 como manual (estático).
    - `ipv4.addresses 10.22.122.2/24`: Asigna la dirección IP `10.22.122.2` con la máscara de red `/24`.

    ```bash
    # Dentro de mvp5
    [root@mvp5 ~]# nmcli connection up Almacenamiento
    Conexión activada con éxito (ruta activa D-Bus: /org/freedesktop/NetworkManager/ActiveConnection/...)
    ```

    **Explicación del comando**:

    - `nmcli connection up Almacenamiento`: Activa la conexión de NetworkManager llamada "Almacenamiento", aplicando la configuración IP a la interfaz `enp7s0`.

7.  **Verificar la configuración de la interfaz `enp7s0`**:

    ```bash
    # Dentro de mvp5
    [root@mvp5 ~]# ip addr show enp7s0
    3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        link/ether 52:54:00:e4:bf:90 brd ff:ff:ff:ff:ff:ff
        inet 10.22.122.2/24 brd 10.22.122.255 scope global noprefixroute enp7s0
           valid_lft forever preferred_lft forever
        inet6 fe80::8046:bb5d:3e14:c0dc/64 scope link noprefixroute
           valid_lft forever preferred_lft forever
    ```

    **Resultado**: La interfaz `enp7s0` ahora tiene la dirección IPv4 estática `10.22.122.2/24` configurada correctamente.

8.  **Configurar el archivo `/etc/hosts` en el anfitrión**: Se añade una entrada para la segunda interfaz de `mvp5`.

    ```bash
    # En el anfitrión lq-d25
    root@lq-d25:~# echo "10.22.122.2 mvp5i2.vpd.com mvp5i2" >> /etc/hosts

    root@lq-d25:~# cat /etc/hosts | grep mvp5
    192.168.140.17 mvp5i1.vpd.com mvp5i1
    10.22.122.2 mvp5i2.vpd.com mvp5i2
    ```

    Ahora se pueden usar `mvp5i2.vpd.com` o `mvp5i2` para referirse a la IP `10.22.122.2`.

9.  **Verificar la conectividad desde el anfitrión a la MV (Comprobación 2)**: Se hace ping desde el anfitrión a la IP de la segunda interfaz usando el nombre definido.

    ```bash
    # En el anfitrión lq-d25
    root@lq-d25:~# ping -c 4 mvp5i2.vpd.com
    PING mvp5i2.vpd.com (10.22.122.2) 56(84) bytes of data.
    64 bytes from mvp5i2.vpd.com (10.22.122.2): icmp_seq=1 ttl=64 time=0.259 ms
    # ... (más respuestas) ...

    --- mvp5i2.vpd.com ping statistics ---
    4 packets transmitted, 4 received, 0% packet loss, time 3089ms
    rtt min/avg/max/mdev = 0.259/0.310/0.324/0.010 ms
    ```

    **Resultado**: El ping es exitoso, confirmando la conectividad entre el anfitrión y la segunda interfaz de `mvp5` a través de la red "Almacenamiento".

10. **Verificar la conectividad desde la MV al anfitrión (Comprobación 3)**: Se hace ping desde `mvp5` a la IP del bridge `virbr2` en el anfitrión.

    ```bash
    # Dentro de mvp5
    [root@mvp5 ~]# ping -c 4 10.22.122.1
    PING 10.22.122.1 (10.22.122.1) 56(84) bytes of data.
    64 bytes from 10.22.122.1: icmp_seq=1 ttl=64 time=0.152 ms
    # ... (más respuestas) ...

    --- 10.22.122.1 ping statistics ---
    4 packets transmitted, 4 received, 0% packet loss, time 3005ms
    rtt min/avg/max/mdev = 0.152/0.243/0.343/0.073 ms
    ```

    **Resultado**: El ping es exitoso, confirmando la comunicación bidireccional dentro de la red aislada "Almacenamiento".

11. **Intentar acceder a Internet desde la MV (Comprobación 4)**: Se verifica que esta interfaz no tiene acceso a redes externas. Se comprueba qué ruta se usaría para alcanzar una IP externa.
    ```bash
    # Dentro de mvp5
    [root@mvp5 ~]# ip route get 8.8.8.8
    8.8.8.8 via 192.168.140.1 dev enp1s0 proto dhcp src 192.168.140.17 metric 102 # Ruta vía Cluster
    ```
    **Explicación**:
    - El comando `ip route get 8.8.8.8` consulta la tabla de enrutamiento para determinar cómo se alcanzaría la dirección `8.8.8.8` (Google DNS).
    - La salida indica que **se utilizaría la interfaz `enp1s0` (conectada a la red NAT "Cluster")** a través de la puerta de enlace `192.168.140.1`.
    - Esto confirma que la interfaz `enp7s0` (conectada a la red aislada "Almacenamiento") **no se utiliza para el tráfico hacia Internet**, como se esperaba. La red aislada funciona correctamente.

Se ha configurado exitosamente la segunda interfaz de `mvp5`, conectada a la red aislada "Almacenamiento" con una IP estática, permitiendo la comunicación interna pero no el acceso externo a través de ella.

### 3.6. Tarea 5: Creación de una Tercera Interfaz de Red de Tipo Bridge

En esta tarea se configura una tercera interfaz de red para `mvp5` utilizando un modo diferente: **Bridge**. A diferencia de las redes NAT y Aislada (que usan _switches virtuales_ gestionados por libvirt como `virbrX`), una interfaz de tipo bridge conecta la máquina virtual directamente a la red física del anfitrión, como si fuera otro equipo físico en esa misma red.

Esto implica crear un bridge en el sistema anfitrión (`bridge0`) y añadir la interfaz de red física del anfitrión (p. ej., `enp6s0`) a este bridge. Luego, la interfaz virtual de la MV se conectará a este mismo bridge `bridge0`.

1.  **Crear el bridge `bridge0` en el anfitrión**: Se utiliza `nmcli` para crear una nueva conexión de tipo bridge.

    ```bash
    # En el anfitrión lq-d25
    root@lq-d25:~# nmcli con add type bridge con-name bridge0 ifname bridge0
    Conexión «bridge0» (uuid) añadida con éxito.
    ```

    **Explicación del comando**:

    - `nmcli con add`: Añade una nueva conexión de NetworkManager.
    - `type bridge`: Especifica que el tipo de conexión es un bridge.
    - `con-name bridge0`: Nombre de la conexión de NetworkManager.
    - `ifname bridge0`: Nombre de la interfaz de red del bridge que se creará.

2.  **Añadir la interfaz física del anfitrión al bridge**: La interfaz física principal del anfitrión (en este caso `enp6s0`) se añade como "esclava" al bridge `bridge0`. Esto significa que `bridge0` tomará el control de la conectividad física.

    ```bash
    # En el anfitrión lq-d25
    root@lq-d25:~# nmcli con add type bridge-slave con-name bridge0-port1 ifname enp6s0 master bridge0
    Conexión «bridge0-port1» (uuid) añadida con éxito.
    ```

    **Explicación del comando**:

    - `nmcli con add type bridge-slave`: Añade una conexión de tipo "esclavo de bridge".
    - `con-name bridge0-port1`: Nombre descriptivo para la conexión del puerto del bridge.
    - `ifname enp6s0`: Interfaz física que se añadirá al bridge. **Debe reemplazarse por el nombre real de la interfaz física del anfitrión si es diferente.**
    - `master bridge0`: Especifica que esta interfaz (`enp6s0`) será parte del bridge `bridge0`.

3.  **Desactivar la conexión original de la interfaz física**: Para evitar conflictos, se desactiva la configuración de red que `enp6s0` pudiera tener previamente.

    ```bash
    # En el anfitrión lq-d25
    # Identificar el nombre de la conexión original asociada a enp6s0
    root@lq-d25:~# nmcli con show --active | grep enp6s0
    # Suponiendo que la conexión se llama 'enp6s0' o similar
    root@lq-d25:~# nmcli con down 'enp6s0' # Usar el nombre de conexión real
    ```

    **Nota**: El nombre de la conexión original puede variar. `nmcli con show --active` ayuda a identificarlo.

4.  **Activar el bridge y el puerto**: Se activan las nuevas conexiones para `bridge0` y su puerto.

    ```bash
    # En el anfitrión lq-d25
    root@lq-d25:~# nmcli con up bridge0
    Conexión activada con éxito...
    root@lq-d25:~# nmcli con up bridge0-port1
    Conexión activada con éxito...
    ```

    > **Importante**: Al activar el bridge, este intentará obtener una dirección IP de la red física (normalmente vía DHCP). La interfaz física `enp6s0` ya no tendrá su propia IP; la IP de la red física estará ahora asignada a la interfaz `bridge0`. **Puede haber una breve interrupción de la conectividad de red del anfitrión** mientras se realiza este cambio.

5.  **Verificar la configuración del bridge**: Se comprueba el estado de las conexiones y la IP asignada a `bridge0`.

    ```bash
    # En el anfitrión lq-d25
    root@lq-d25:~# nmcli device status
    DEVICE   TYPE      STATE                   CONNECTION
    bridge0  bridge    conectado               bridge0
    enp6s0   ethernet  conectado (degradado)   bridge0-port1 # 'conectado' indica que es parte del bridge
    lo       loopback  conectado (externally)  lo
    virbr0   bridge    conectado (externally)  virbr0
    virbr1   bridge    conectado (externally)  virbr1
    virbr2   bridge    conectado (externally)  virbr2
    # ... (interfaces vnetX de las MVs) ...
    ```

    ```bash
    # En el anfitrión lq-d25
    root@lq-d25:~# ip addr show bridge0
    3: bridge0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
        link/ether 08:bf:b8:ee:b1:69 brd ff:ff:ff:ff:ff:ff # MAC de enp6s0
        inet 10.140.92.125/24 brd 10.140.92.255 scope global dynamic noprefixroute bridge0
           valid_lft 8501sec preferred_lft 8501sec
        inet6 fe80::e6e7:c28d:331c:1d14/64 scope link noprefixroute
           valid_lft forever preferred_lft forever
    ```

    **Explicación de la salida**:

    - `nmcli device status` muestra `bridge0` como conectado y `enp6s0` asociado a él.
    - `ip addr show bridge0` muestra que la interfaz `bridge0` está `UP` y ha obtenido la dirección IP `10.140.92.125/24` (en este ejemplo) de la red física. La MAC de `bridge0` suele ser la misma que la de la primera interfaz física añadida (`enp6s0`).

6.  **Generar una dirección MAC única para la MV**: Es **muy importante** usar una MAC única y perteneciente al rango asignado por la organización (si aplica) para la interfaz bridge de la MV, ya que estará directamente en la red física.

    ```bash
    # En el anfitrión lq-d25 (Ejemplo, puede requerir un script o asignación manual)
    # python ./macgen.py # Script hipotético para generar MACs válidas
    # O asignar manualmente una MAC válida:
    MAC_BRIDGE="00:16:3e:6b:8b:d9" # Ejemplo de MAC válida OUI de Red Hat
    ```

7.  **Añadir la tercera interfaz a `mvp5` conectada al bridge**: Se utiliza `virsh attach-interface` pero especificando `type bridge`.

    ```bash
    # En el anfitrión lq-d25
    root@lq-d25:~# virsh attach-interface mvp5 bridge bridge0 --model virtio --mac "$MAC_BRIDGE" --config
    La interfaz ha sido asociada exitosamente
    ```

    **Explicación del comando**:

    - `virsh attach-interface mvp5`: Añade interfaz a `mvp5`.
    - `bridge bridge0`: Especifica el **tipo `bridge`** y el **nombre del bridge del anfitrión (`bridge0`)** al que se conectará.
    - `--model virtio`: Usa el controlador paravirtualizado.
    - `--mac "$MAC_BRIDGE"`: **Asigna la MAC específica y única** generada/asignada en el paso anterior.
    - `--config`: Hace el cambio persistente.

8.  **Verificar las interfaces de `mvp5` en el anfitrión**:

    ```bash
    root@lq-d25:~# virsh domiflist mvp5
     Interfaz   Tipo      Fuente           Modelo   MAC
    -------------------------------------------------------------------
     vnet0      network   Cluster          virtio   52:54:00:bd:89:a1
     vnet1      network   Almacenamiento   virtio   52:54:00:e4:bf:90
     -          bridge    bridge0          virtio   00:16:3e:6b:8b:d9
    ```

    Se observa la nueva interfaz de tipo `bridge` conectada a `bridge0`.

9.  **Reiniciar la máquina virtual**: Para que `mvp5` detecte la nueva interfaz.

    ```bash
    root@lq-d25:~# virsh reboot mvp5
    El dominio mvp5 está siendo reiniciado
    ```

10. **Conectarse a la máquina virtual**: Vía consola serie.

    ```bash
    root@lq-d25:~# virsh console mvp5
    # ... login ...
    [root@mvp5 ~]#
    ```

11. **Verificar la configuración de red en `mvp5` (Comprobación 1)**: Se comprueba la detección y configuración de la nueva interfaz.

    ```bash
    # Dentro de mvp5
    [root@mvp5 ~]# ip addr
    1: lo: <LOOPBACK,UP,LOWER_UP> ...
       inet 127.0.0.1/8 ...
    2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> ... state UP ... # Cluster (NAT)
       link/ether 52:54:00:bd:89:a1 ...
       inet 192.168.140.17/24 ... dynamic ... enp1s0
    3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> ... state UP ... # Almacenamiento (Aislada)
       link/ether 52:54:00:e4:bf:90 ...
       inet 10.22.122.2/24 ... noprefixroute enp7s0
    4: enp8s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000 # Bridge
       link/ether 00:16:3e:6b:8b:d9 brd ff:ff:ff:ff:ff:ff
       inet 10.140.92.178/24 brd 10.140.92.255 scope global dynamic noprefixroute enp8s0
          valid_lft 10392sec preferred_lft 10392sec
       inet6 fe80::f0dc:8e70:5631:d16d/64 scope link noprefixroute
          valid_lft forever preferred_lft forever
    ```

    **Explicación de la salida**:

    - Se detecta la tercera interfaz como `enp8s0` (el nombre puede variar).
    - Tiene la MAC `00:16:3e:6b:8b:d9` asignada manualmente.
    - Está `UP`.
    - **Ha obtenido la dirección IP `10.140.92.178/24` dinámicamente (`dynamic`)**. Esta IP proviene del servidor DHCP de la red física del laboratorio/anfitrión, a la que ahora `mvp5` está directamente conectada a través de `bridge0`.

12. **Configurar el archivo `/etc/hosts` en el anfitrión**: Se añade la entrada para la tercera interfaz.

    ```bash
    # En el anfitrión lq-d25
    root@lq-d25:~# echo "10.140.92.178 mvp5i3.vpd.com mvp5i3" >> /etc/hosts

    root@lq-d25:~# cat /etc/hosts | grep mvp5
    192.168.140.17 mvp5i1.vpd.com mvp5i1
    10.22.122.2 mvp5i2.vpd.com mvp5i2
    10.140.92.178 mvp5i3.vpd.com mvp5i3
    ```

13. **Verificar la conectividad desde el anfitrión a la MV (Comprobación 2)**: Ping a la IP de la interfaz bridge.

    ```bash
    # En el anfitrión lq-d25
    root@lq-d25:~# ping -c 4 mvp5i3.vpd.com
    PING mvp5i3.vpd.com (10.140.92.178) 56(84) bytes of data.
    64 bytes from mvp5i3.vpd.com (10.140.92.178): icmp_seq=1 ttl=64 time=0.058 ms
    # ... (más respuestas, asumiendo que el ping a .178 funciona) ...

    --- mvp5i3.vpd.com ping statistics ---
    4 packets transmitted, 4 received, 0% packet loss, time ...
    rtt min/avg/max/mdev = ... ms
    ```

    **Resultado**: El ping debe ser exitoso, confirmando la conectividad directa en la red física.

14. **Verificar el acceso a Internet desde la MV (Comprobación 3)**: Se comprueba si la MV accede a Internet, posiblemente ahora a través de la interfaz bridge.

    ```bash
    # Dentro de mvp5
    [root@mvp5 ~]# ping -c 4 google.es
    PING google.es (142.250.184.163) 56(84) bytes of data.
    64 bytes from mad07s23-in-f3.1e100.net (142.250.184.163): icmp_seq=1 ttl=114 time=30.3 ms
    # ... (más respuestas) ...

    --- google.es ping statistics ---
    4 packets transmitted, 4 received, 0% packet loss, time 3005ms
    rtt min/avg/max/mdev = 29.762/30.058/30.297/0.238 ms
    ```

    **Resultado**: El acceso a Internet es exitoso. La ruta utilizada dependerá de la tabla de enrutamiento de la MV.

15. **Verificar la conectividad con otro equipo en la red física (Comprobación 4 - Opcional)**: Si se conoce la IP de otro equipo en la misma red física (p. ej., el anfitrión del profesor `10.10.14.1` mencionado en `p5_temp.md`, aunque la IP `10.22.122.1` usada en el ping de `p5_temp.md` corresponde a `virbr2`), se puede intentar hacer ping.

    ```bash
    # Dentro de mvp5
    [root@mvp5 ~]# ping -c 4 10.10.14.1 # IP ejemplo red física
    # O la IP usada en p5_temp.md (que era virbr2)
    [root@mvp5 ~]# ping -c 4 10.22.122.1 # Ping a virbr2 (red Almacenamiento)
    PING 10.22.122.1 (10.22.122.1) 56(84) bytes of data.
    64 bytes from 10.22.122.1: icmp_seq=1 ttl=64 time=0.172 ms
    # ... (más respuestas) ...

    --- 10.22.122.1 ping statistics ---
    4 packets transmitted, 4 received, 0% packet loss, time 3005ms
    rtt min/avg/max/mdev = 0.172/0.233/0.336/0.062 ms
    ```

    **Resultado**: El ping a otros equipos en la red física (a través de `enp8s0`) o a las redes virtuales del anfitrión (a través de `enp1s0` o `enp7s0`) debería funcionar según la tabla de rutas.

16. **Comprobar la tabla de enrutamiento en la MV**: Es crucial ver qué ruta por defecto se está utilizando ahora que hay múltiples interfaces con acceso potencial a la red.
    ```bash
    # Dentro de mvp5
    [root@mvp5 ~]# ip route
    default via 10.140.92.1 dev enp8s0 proto dhcp src 10.140.92.178 metric 102 # Ruta vía Bridge
    default via 192.168.140.1 dev enp1s0 proto dhcp src 192.168.140.17 metric 100 # Ruta vía NAT
    10.22.122.0/24 dev enp7s0 proto kernel scope link src 10.22.122.2 metric 101 # Ruta red Aislada
    10.140.92.0/24 dev enp8s0 proto kernel scope link src 10.140.92.178 metric 102 # Ruta red Física (Bridge)
    192.168.140.0/24 dev enp1s0 proto kernel scope link src 192.168.140.17 metric 100 # Ruta red NAT
    ```
    **Explicación de la tabla de rutas (ejemplo)**:
    - Pueden aparecer **múltiples rutas por defecto**, una por cada interfaz que obtuvo una puerta de enlace vía DHCP (en este caso `enp1s0` y `enp8s0`).
    - El sistema operativo utiliza la ruta por defecto con la **métrica más baja** (más preferible). En este ejemplo, la ruta a través de `enp1s0` (NAT, métrica 100) sería preferida sobre la ruta a través de `enp8s0` (Bridge, métrica 102). Los valores de métrica pueden variar.
    - Aparecen también las rutas directas para cada una de las subredes conectadas (`10.22.122.0/24`, `10.140.92.0/24`, `192.168.140.0/24`).
    - **Es importante gestionar las métricas** si se desea priorizar una ruta de salida sobre otra. Esto se puede hacer en la configuración de NetworkManager (`nmcli con mod <conexion> ipv4.route-metric <valor>`).

Se ha configurado exitosamente la tercera interfaz de `mvp5`, conectada directamente a la red física del anfitrión mediante un bridge, obteniendo una IP de esa red y permitiendo la comunicación como un equipo más en la LAN.

## 4. Pruebas y Validación

Esta sección consolida las verificaciones realizadas durante el desarrollo y añade comprobaciones finales del estado del sistema anfitrión y las configuraciones de red de libvirt.

### 4.1. Validación General del Anfitrión

Se verifica el estado final de las interfaces y rutas en el sistema anfitrión (`lq-d25`).

1.  **Listar interfaces y direcciones IP**:

    ```bash
    root@lq-d25:~# ip -br addr show
    lo               UNKNOWN        127.0.0.1/8 ::1/128
    enp6s0           UP             # Interfaz física, ahora parte del bridge
    bridge0          UP             10.140.92.125/24 fe80::e6e7:c28d:331c:1d14/64 # Bridge con IP de la red física
    virbr1           UP             192.168.140.1/24 # Bridge red NAT "Cluster"
    virbr0           DOWN           192.168.122.1/24 # Bridge red "default" (si existe y está inactiva)
    virbr2           UP             10.22.122.1/24 # Bridge red Aislada "Almacenamiento"
    vnet0            UNKNOWN        fe80::fc54:ff:febd:89a1/64 # Tap interfaz mvp5 -> Cluster
    vnet1            UNKNOWN        fe80::fc54:ff:fee4:bf90/64 # Tap interfaz mvp5 -> Almacenamiento
    vnet2            UNKNOWN        fe80::fc16:3eff:fe6b:8bd9/64 # Tap interfaz mvp5 -> bridge0
    ```

    **Explicación**: Se observan todas las interfaces: loopback (`lo`), física (`enp6s0`), el bridge físico (`bridge0`), los bridges virtuales de libvirt (`virbrX`) y las interfaces `vnetX` (tap) que conectan la MV a los bridges correspondientes.

2.  **Verificar tabla de enrutamiento del anfitrión**:

    ```bash
    root@lq-d25:~# ip route
    default via 10.140.92.1 dev bridge0 proto dhcp src 10.140.92.125 metric 425 # Ruta defecto vía bridge físico
    10.22.122.0/24 dev virbr2 proto kernel scope link src 10.22.122.1 # Ruta red Aislada
    10.140.92.0/24 dev bridge0 proto kernel scope link src 10.140.92.125 metric 425 # Ruta red Física
    192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1 linkdown # Ruta red Default (inactiva)
    192.168.140.0/24 dev virbr1 proto kernel scope link src 192.168.140.1 # Ruta red NAT
    ```

    **Explicación**: La ruta por defecto del anfitrión ahora pasa por `bridge0`. Existen rutas específicas para cada red virtual conectada (`virbr1`, `virbr2`).

3.  **Verificar configuración del cortafuegos**:

    ```bash
    root@lq-d25:~# firewall-cmd --list-all
    FedoraServer (default, active)
      target: default
      ingress-priority: 0
      egress-priority: 0
      icmp-block-inversion: no
      interfaces: bridge0 enp6s0 # Ambas asociadas a la zona
      sources:
      services: cockpit dhcpv6-client libvirt ssh # Servicios permitidos
      ports: 49152-49216/tcp # Puertos para migración (si se configuraron)
      protocols:
      forward: yes # Reenvío habilitado (necesario para NAT)
      masquerade: no # Masquerading (NAT) no habilitado globalmente aquí (libvirt lo gestiona)
      forward-ports:
      source-ports:
      icmp-blocks:
      rich rules:
    ```

    **Explicación**: Confirma qué interfaces están asociadas a la zona activa y qué servicios/puertos están permitidos. El reenvío (`forward: yes`) es necesario para que las redes NAT funcionen.

4.  **Verificar reglas de NAT (iptables/nftables)**: Libvirt configura automáticamente reglas de NAT para las redes de tipo NAT.

    ```bash
    # Para sistemas con iptables
    root@lq-d25:~# iptables -t nat -L POSTROUTING -v
    Chain POSTROUTING (policy ACCEPT ...)
     pkts bytes target     prot opt in     out     source               destination
     # ... Reglas LIBVIRT_PRT ...

    root@lq-d25:~# iptables -t nat -L LIBVIRT_PRT -v
    Chain LIBVIRT_PRT (1 references)
     pkts bytes target     prot opt in     out     source               destination
    # ... Reglas RETURN para tráfico local ...
        0     0 MASQUERADE  tcp  --  any    any     192.168.140.0/24    !192.168.140.0/24     masq ports: 1024-65535 # NAT para TCP red Cluster
      663 50388 MASQUERADE  udp  --  any    any     192.168.140.0/24    !192.168.140.0/24     masq ports: 1024-65535 # NAT para UDP red Cluster
        0     0 MASQUERADE  all  --  any    any     192.168.140.0/24    !192.168.140.0/24     # NAT para otros protocolos red Cluster
    # ... (Reglas similares para red 'default' si existe y es NAT) ...
    ```

    **Explicación**: Se buscan las reglas `MASQUERADE` en la tabla `nat`, cadena `POSTROUTING` (o las cadenas personalizadas de libvirt como `LIBVIRT_PRT`). Estas reglas son las que realizan la traducción de direcciones para el tráfico saliente de las MVs en redes NAT (como "Cluster"). No debería haber reglas `MASQUERADE` para la red aislada ("Almacenamiento") ni para la red bridge.

### 4.2. Validación de la Red NAT (Cluster)

Se verifica la funcionalidad específica de la interfaz conectada a la red NAT.

- **Conectividad Anfitrión <-> MV**: `ping 192.168.140.17` (o `mvp5i1`) desde el anfitrión.
- **Conectividad MV -> Anfitrión**: `ping 192.168.140.1` desde `mvp5`.
- **Acceso a Internet**: `ping google.es` desde `mvp5` (verificando que la ruta usada es vía `192.168.140.1`).
- **Servicio DHCP**: Verificar la concesión activa.
  ```bash
  root@lq-d25:~# virsh net-dhcp-leases Cluster
   Expiry Time           dirección MAC       Protocol   IP address          Hostname   Client ID or DUID
  -----------------------------------------------------------------------------------------------------------
   2025-04-11 21:31:52   52:54:00:bd:89:a1   ipv4       192.168.140.17/24   mvp5       01:52:54:00:bd:89:a1 # Hostname puede variar
  ```

### 4.3. Validación de la Red Aislada (Almacenamiento)

Se verifica la funcionalidad específica de la interfaz conectada a la red aislada.

- **Conectividad Anfitrión <-> MV**: `ping 10.22.122.2` (o `mvp5i2`) desde el anfitrión.
- **Conectividad MV -> Anfitrión**: `ping 10.22.122.1` desde `mvp5`.
- **Ausencia de Acceso a Internet**: `ip route get 8.8.8.8` desde `mvp5` debe mostrar que la ruta _no_ usa la interfaz `enp7s0` (la conectada a esta red).
- **Ausencia de DHCP**: Verificar que no hay concesiones DHCP para esta red:
  ```bash
  root@lq-d25:~# virsh net-dhcp-leases Almacenamiento
   Expiry Time           dirección MAC       Protocol   IP address          Hostname   Client ID or DUID
  -----------------------------------------------------------------------------------------------------------
  # (Salida vacía)
  ```

### 4.4. Validación del Bridge

Se verifica la funcionalidad específica de la interfaz conectada al bridge físico.

- **Conectividad Anfitrión <-> MV**: `ping 10.140.92.178` (o `mvp5i3`) desde el anfitrión.
- **Conectividad MV -> Anfitrión/Red Física**: `ping 10.140.92.1` (gateway físico) y `ping <IP_otro_equipo_físico>` desde `mvp5`.
- **Acceso a Internet**: `ping google.es` desde `mvp5` (verificar tabla de rutas para ver si esta interfaz es la preferida).
- **DHCP de Red Física**: Comprobar la IP asignada a `enp8s0` en `mvp5` (`ip addr`) y confirmar que es de la red física y dinámica.

### 4.5. Validación Específica de Redes Libvirt

Se examina la configuración interna de las redes gestionadas por libvirt.

1.  **Exportar configuración XML de las redes**:

    ```bash
    root@lq-d25:~# virsh net-dumpxml Cluster
    <network connections='1'> <!-- 'connections' indica cuántas MVs están conectadas -->
      <name>Cluster</name>
      <uuid>eca302af-e62c-4b0f-9823-6f06fa5e9282</uuid>
      <forward mode='nat'> <!-- Confirma modo NAT -->
        <nat>
          <port start='1024' end='65535'/>
        </nat>
      </forward>
      <bridge name='virbr1' stp='on' delay='0'/>
      <mac address='52:54:00:f5:97:55'/>
      <ip address='192.168.140.1' netmask='255.255.255.0'>
        <dhcp>
          <range start='192.168.140.2' end='192.168.140.149'/> <!-- Confirma rango DHCP -->
        </dhcp>
      </ip>
    </network>

    root@lq-d25:~# virsh net-dumpxml Almacenamiento
    <network connections='1'>
      <name>Almacenamiento</name>
      <uuid>1f78a08c-c862-4419-98ed-bde618c88f6e</uuid>
      <!-- Ausencia de 'forward' confirma red aislada -->
      <bridge name='virbr2' stp='on' delay='0'/>
      <mac address='52:54:00:fc:4f:fa'/>
      <domain name='Almacenamiento'/> <!-- Dominio DNS opcional para la red -->
      <ip address='10.22.122.1' netmask='255.255.255.0'>
        <!-- Ausencia de 'dhcp' confirma DHCP desactivado -->
      </ip>
    </network>
    ```

    **Explicación**: `virsh net-dumpxml <red>` muestra la configuración XML activa de la red, permitiendo verificar modos, IPs, bridges y configuraciones DHCP.

2.  **Verificar procesos `dnsmasq`**: Libvirt utiliza `dnsmasq` para los servicios DHCP/DNS de las redes virtuales.

    ```bash
    root@lq-d25:~# ps aux | grep dnsmasq | grep libvirt
    dnsmasq     1261  ... /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/Cluster.conf ... # Proceso para red Cluster
    root        1262  ... /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/Cluster.conf ...
    # ... (Proceso similar para red 'default' si existe y tiene DHCP) ...
    # Ausencia de proceso para 'Almacenamiento.conf' confirma que no tiene DHCP/DNS activo
    ```

    **Explicación**: Debe haber un proceso `dnsmasq` por cada red virtual con DHCP y/o DNS habilitado, apuntando a su archivo de configuración específico en `/var/lib/libvirt/dnsmasq/`.

3.  **Verificar archivo de concesiones DHCP**: Se puede inspeccionar el archivo de estado que `dnsmasq` mantiene para cada red.
    ```bash
    root@lq-d25:~# cat /var/lib/libvirt/dnsmasq/Cluster.status # Nombre puede variar (virbr1.status?)
    [
      {
        "ip-address": "192.168.140.17",
        "mac-address": "52:54:00:bd:89:a1",
        "hostname": "mvp5", # Hostname puede variar
        "client-id": "01:52:54:00:bd:89:a1",
        "expiry-time": 1744403512 # Timestamp de expiración
      }
    ]
    ```
    **Explicación**: Muestra las concesiones DHCP activas registradas por `dnsmasq` para esa red específica.

Estas validaciones confirman que las diferentes configuraciones de red (NAT, Aislada, Bridge) se han implementado correctamente tanto a nivel de máquina virtual como de anfitrión y libvirt.

## 5. Solución de Problemas Comunes

Esta sección describe posibles problemas encontrados durante la práctica y sus soluciones, basadas en las secciones "Recuperación en caso de errores" de `p5_temp.md`.

### Problema 1: Errores durante la creación/clonación de `mvp5`

**Síntomas**:

- El comando `virt-clone` falla.
- La máquina `mvp5` no se crea o no arranca correctamente después de la clonación.
- Errores relacionados con el archivo de almacenamiento (`mvp5.qcow2`).

**Solución**:

1.  **Detener** la máquina virtual `mvp5` si está parcialmente creada o en ejecución:
    ```bash
    root@lq-d25:~# virsh destroy mvp5 # Detiene forzosamente
    ```
2.  **Eliminar** completamente la definición y el almacenamiento asociado a `mvp5`:
    ```bash
    root@lq-d25:~# virsh undefine mvp5 --remove-all-storage
    ```
3.  **Verificar** que la máquina ha sido eliminada:
    ```bash
    root@lq-d25:~# virsh list --all | grep mvp5 # No debería aparecer
    ```
4.  **Reintentar** la clonación asegurándose de que la MV original (`mvp1`) existe y es válida, y que la ruta del archivo de destino es correcta y tiene permisos adecuados:
    ```bash
    root@lq-d25:~# virt-clone --original mvp1 --name mvp5 --file /var/lib/libvirt/images/mvp5.qcow2 --mac=00:16:3e:37:a0:05
    ```

### Problema 2: La consola serie no funciona

**Síntomas**:

- El comando `virsh console mvp5` no conecta o no muestra el prompt de login.
- No hay salida en la consola serie durante el arranque de `mvp5`.

**Solución**:

1.  **Acceder** a la máquina virtual utilizando un método alternativo (VNC/SPICE a través de `virt-manager`).
2.  **Verificar** la configuración en `/etc/default/grub` dentro de `mvp5`:
    - Asegurarse de que `GRUB_CMDLINE_LINUX` contiene `console=ttyS0`.
    - Comprobar que no hay errores tipográficos.
3.  **Regenerar** la configuración de GRUB:
    ```bash
    # Dentro de mvp5
    root@mvp5:~# grub2-mkconfig -o /boot/grub2/grub.cfg
    ```
4.  **Reiniciar** la máquina virtual:
    ```bash
    # Dentro de mvp5
    root@mvp5:~# reboot
    ```
5.  **Si persiste el problema**, considerar volver al estado inicial eliminando y clonando `mvp5` de nuevo (ver Problema 1) y repetir cuidadosamente los pasos de configuración de la consola serie.

### Problema 3: Errores durante la creación de la red NAT "Cluster"

**Síntomas**:

- El comando `virsh net-define` o `virsh net-start` falla.
- La red "Cluster" no aparece en `virsh net-list` o no está activa.
- La interfaz `virbr1` no se crea en el anfitrión o no tiene la IP correcta.

**Solución**:

1.  **Detener** la red si está parcialmente activa:
    ```bash
    root@lq-d25:~# virsh net-destroy Cluster
    ```
2.  **Eliminar** la definición de la red:
    ```bash
    root@lq-d25:~# virsh net-undefine Cluster
    ```
3.  **Verificar** que la red ha sido eliminada:
    ```bash
    root@lq-d25:~# virsh net-list --all | grep Cluster
    ```
4.  **Revisar** cuidadosamente el archivo XML de definición (`cluster-network.xml`) en busca de errores de sintaxis o configuración (nombre del bridge, IP, rango DHCP).
5.  **Reintentar** la definición, inicio y configuración de autoarranque:
    ```bash
    root@lq-d25:~# virsh net-define cluster-network.xml
    root@lq-d25:~# virsh net-start Cluster
    root@lq-d25:~# virsh net-autostart Cluster
    ```
6.  **Verificar** la creación de `virbr1` y su IP con `ip addr show virbr1`.

### Problema 4: Errores al añadir/configurar la primera interfaz (Red NAT)

**Síntomas**:

- El comando `virsh attach-interface` falla.
- La MV `mvp5` no detecta la interfaz `enp1s0` tras reiniciar.
- La interfaz `enp1s0` no obtiene una dirección IP vía DHCP.
- No hay conectividad entre el anfitrión y `mvp5` o desde `mvp5` a Internet.

**Solución**:

1.  **Verificar** que la red "Cluster" está activa (`virsh net-list`).
2.  **Verificar** las interfaces actuales de `mvp5` (`virsh domiflist mvp5`).
3.  Si la interfaz existe pero está mal configurada, **eliminarla** (usando su MAC):
    ```bash
    # Obtener MAC de virsh domiflist
    MAC_IF1="52:54:00:bd:89:a1"
    root@lq-d25:~# virsh detach-interface mvp5 network --mac "$MAC_IF1"
    root@lq-d25:~# virsh detach-interface mvp5 network --mac "$MAC_IF1" --config
    ```
4.  **Reintentar** añadir la interfaz:
    ```bash
    root@lq-d25:~# virsh attach-interface mvp5 network Cluster --model virtio --config
    ```
5.  **Reiniciar** `mvp5`.
6.  **Dentro de `mvp5`**:
    - Verificar con `ip addr`.
    - Asegurarse de que NetworkManager está gestionando la interfaz (`nmcli device status`).
    - Forzar la obtención de DHCP si es necesario: `nmcli connection down <conexion_enp1s0>; nmcli connection up <conexion_enp1s0>` o `dhclient enp1s0`.
    - Verificar la configuración del firewall dentro de `mvp5` (aunque por defecto debería permitir DHCP).
7.  **En el anfitrión**:
    - Verificar el proceso `dnsmasq` para la red "Cluster" (`ps aux | grep dnsmasq | grep Cluster`).
    - Verificar las concesiones DHCP (`virsh net-dhcp-leases Cluster`).
    - Verificar las reglas de `iptables` NAT.

### Problema 5: Errores durante la creación de la red Aislada "Almacenamiento"

**Síntomas**:

- Similar al Problema 3, pero para la red "Almacenamiento".
- La interfaz `virbr2` no se crea o no tiene la IP `10.22.122.1`.

**Solución**:

1.  **Detener** la red:
    ```bash
    root@lq-d25:~# virsh net-destroy Almacenamiento
    ```
2.  **Eliminar** la definición:
    ```bash
    root@lq-d25:~# virsh net-undefine Almacenamiento
    ```
3.  **Verificar** eliminación:
    ```bash
    root@lq-d25:~# virsh net-list --all | grep Almacenamiento
    ```
4.  **Revisar** el archivo XML (`almacenamiento-network.xml`), asegurándose de que **no** contiene la etiqueta `<forward>` ni la sección `<dhcp>`. Verificar la IP y el nombre del bridge (`virbr2`).
5.  **Reintentar** definición, inicio y autoarranque:
    ```bash
    root@lq-d25:~# virsh net-define almacenamiento-network.xml
    root@lq-d25:~# virsh net-start Almacenamiento
    root@lq-d25:~# virsh net-autostart Almacenamiento
    ```
6.  **Verificar** `virbr2` con `ip addr show virbr2`.

### Problema 6: Errores al añadir/configurar la segunda interfaz (Red Aislada)

**Síntomas**:

- El comando `virsh attach-interface` falla.
- La MV `mvp5` no detecta la interfaz `enp7s0` tras reiniciar.
- El comando `nmcli connection add` o `nmcli connection up` para la IP estática falla.
- No hay conectividad entre el anfitrión y `mvp5` en la red `10.22.122.0/24`.

**Solución**:

1.  **Verificar** que la red "Almacenamiento" está activa (`virsh net-list`).
2.  **Verificar** las interfaces actuales de `mvp5` (`virsh domiflist mvp5`).
3.  Si la interfaz existe pero está mal configurada, **eliminarla** (usando su MAC):
    ```bash
    # Obtener MAC de virsh domiflist
    MAC_IF2="52:54:00:e4:bf:90"
    root@lq-d25:~# virsh detach-interface mvp5 network --mac "$MAC_IF2"
    root@lq-d25:~# virsh detach-interface mvp5 network --mac "$MAC_IF2" --config
    ```
4.  **Reintentar** añadir la interfaz:
    ```bash
    root@lq-d25:~# virsh attach-interface mvp5 network Almacenamiento --model virtio --config
    ```
5.  **Reiniciar** `mvp5`.
6.  **Dentro de `mvp5`**:
    - Verificar detección con `ip addr`.
    - **Eliminar** cualquier configuración de conexión previa para `enp7s0` si existe: `nmcli connection delete Almacenamiento` (usar el nombre o UUID real).
    - **Reintentar** crear la conexión estática:
      ```bash
      [root@mvp5 ~]# nmcli connection add type ethernet con-name Almacenamiento ifname enp7s0 ipv4.method manual ipv4.addresses 10.22.122.2/24
      [root@mvp5 ~]# nmcli connection up Almacenamiento
      ```
    - Verificar la IP con `ip addr show enp7s0`.
    - Comprobar conectividad con `ping 10.22.122.1`.

> **Nota**: Para la Tarea 5 (Interfaz Bridge), no se proporcionaron pasos de recuperación específicos en `p5_temp.md`. Los problemas comunes implicarían errores en la creación del bridge (`bridge0`) en el anfitrión, errores al añadir la interfaz física (`enp6s0`) al bridge, fallos al obtener IP por DHCP en la red física, o conflictos de MAC si no se asignó una única. La solución generalmente implica deshacer los pasos de `nmcli con add/mod` y `virsh attach-interface` y reintentarlos cuidadosamente.

## 6. Conclusiones

Esta práctica ha permitido explorar y configurar los tipos fundamentales de redes virtuales disponibles en entornos KVM/libvirt: NAT, Aislada y Bridge. Cada tipo ofrece diferentes niveles de conectividad y aislamiento, adecuados para distintos escenarios de uso.

- La **Red NAT** (ej. "Cluster") es útil para proporcionar acceso a Internet a las máquinas virtuales de forma sencilla, sin requerir IPs adicionales de la red física y ofreciendo un nivel básico de aislamiento.
- La **Red Aislada** (ej. "Almacenamiento") es ideal para crear segmentos de red privados entre máquinas virtuales y el anfitrión, por ejemplo, para tráfico de almacenamiento o gestión, sin exponerlos a redes externas. La configuración manual de IP es necesaria al no disponer de DHCP.
- La **Interfaz Bridge** conecta la máquina virtual directamente a la red física del anfitrión, comportándose como un equipo más en la LAN. Esto simplifica el acceso a la MV desde otros equipos de la red física, pero requiere una gestión cuidadosa de las direcciones IP y MAC para evitar conflictos.

Se ha trabajado con herramientas de línea de comandos como `virsh` para la gestión de redes y máquinas virtuales, y `nmcli` para la configuración de red tanto en el anfitrión como en el invitado. La definición de redes mediante archivos XML ha demostrado ser un método eficaz y reproducible para la configuración de infraestructuras virtuales.

La configuración de la consola serie se reveló como un paso preparatorio esencial, permitiendo el acceso a la máquina virtual incluso después de eliminar su conectividad de red inicial.

Comprender las diferencias entre estos tipos de redes y saber cómo configurarlas es fundamental para diseñar e implementar infraestructuras virtualizadas seguras, eficientes y adaptadas a las necesidades específicas de cada aplicación o servicio.

## 7. Bibliografía

1. Red Hat Enterprise Linux 9. (2024). "Configuring and managing virtualization. Setting up your host, creating and administering virtual machines, and understanding virtualization features". Red Hat. Disponible en: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_virtualization/index [accedido el 24/03/2025]

2. Gregory, R., Boy, P. (2023). "Configurando la red con nmcli". Fedora Project. Disponible en: https://docs.fedoraproject.org/es/quick-docs/configuring-ip-networking-with-nmcli/ [accedido el 24/03/2025]

3. Red Hat Enterprise Linux 9. (2024). "Configuring and managing networking. Managing network interfaces and advanced networking features". Red Hat. Disponible en: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_networking/index [accedido el 24/03/2025]
