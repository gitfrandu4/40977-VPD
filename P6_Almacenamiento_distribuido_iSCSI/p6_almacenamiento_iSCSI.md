
# Pr√°ctica 6: Instalaci√≥n de un servicio de almacenamiento iSCSI

## Tabla de Contenidos

- [1. Introducci√≥n](#1-introducci√≥n)
- [2. Requisitos previos](#2-requisitos-previos)
- [3. Plan de actividades y orientaciones](#3-plan-de-actividades-y-orientaciones)
  - [Tarea 1: Creaci√≥n de la infraestructura b√°sica iSCSI](#tarea-1-creaci√≥n-de-la-infraestructura-b√°sica-iscsi)
  - [Tarea 2: Instalaci√≥n y configuraci√≥n de servicio iSCSI en el nodo target](#tarea-2-instalaci√≥n-y-configuraci√≥n-de-servicio-iscsi-en-el-nodo-target)
  - [Tarea 3: Instalaci√≥n del soporte iSCSI en los nodos initiator](#tarea-3-instalaci√≥n-del-soporte-iscsi-en-los-nodos-initiator)
  - [Tarea 4: Creaci√≥n de sistema de archivos tipo ext4 en la unidad iSCSI importada](#tarea-4-creaci√≥n-de-sistema-de-archivos-tipo-ext4-en-la-unidad-iscsi-importada)
  - [Tarea 5: Exportaci√≥n del segundo disco iSCSI y creaci√≥n de un volumen l√≥gico](#tarea-5-exportaci√≥n-del-segundo-disco-iscsi-y-creaci√≥n-de-un-volumen-l√≥gico)
- [4. Pruebas y Validaci√≥n](#4-pruebas-y-validaci√≥n)
- [5. Conclusiones](#5-conclusiones)
- [6. Bibliograf√≠a](#6-bibliograf√≠a)

## 1. Introducci√≥n

Esta pr√°ctica aborda la implementaci√≥n de un servicio de almacenamiento distribuido utilizando el protocolo iSCSI (Internet Small Computer System Interface). El protocolo iSCSI es una tecnolog√≠a de almacenamiento en red que permite el acceso a dispositivos de almacenamiento a trav√©s de redes TCP/IP est√°ndar, facilitando la creaci√≥n de Redes de √Årea de Almacenamiento (SAN) sin requerir hardware especializado como en las redes Fibre Channel tradicionales.

En este entorno, se configurar√° un nodo exportador de almacenamiento (target) y dos nodos que importar√°n y utilizar√°n dicho almacenamiento (initiators). Esta configuraci√≥n permite demostrar las capacidades de iSCSI para proporcionar almacenamiento compartido en entornos empresariales, donde varios servidores pueden acceder y utilizar el mismo espacio de almacenamiento de forma concurrente.

## 2. Requisitos previos

Para la realizaci√≥n de esta pr√°ctica se requiere:

- Un sistema anfitri√≥n con KVM (Kernel-based Virtual Machine) correctamente instalado y configurado.
- Conocimientos b√°sicos sobre redes TCP/IP y sistemas de almacenamiento.
- Familiaridad con el sistema operativo Fedora/RHEL y la administraci√≥n de sistemas Linux.
- Tres m√°quinas virtuales que funcionar√°n como:
  - Un servidor de almacenamiento (target iSCSI)
  - Dos nodos clientes (initiators iSCSI)
- Dos redes virtuales configuradas:
  - Red de almacenamiento (10.22.122.0/24): dedicada al tr√°fico iSCSI
  - Red de cluster (192.168.140.0/24): para comunicaci√≥n general entre nodos

Ran tool
## 3. Plan de actividades y orientaciones

### Tarea 1: Creaci√≥n de la infraestructura b√°sica iSCSI

En esta tarea se implementar√° la infraestructura b√°sica necesaria para el servicio iSCSI, que incluye la creaci√≥n de las m√°quinas virtuales, la adici√≥n de discos y la configuraci√≥n de las interfaces de red.

#### Creaci√≥n de m√°quinas virtuales mediante clonado

```bash
root@lq-d25:~# virt-clone --original mvp1 --name Almacenamiento --file /var/lib/libvirt/images/Almacenamiento.qcow2 --mac 00:16:3e:76:57:a0
Allocating 'Almacenamiento.qcow2'                           | 1.6 GB  00:00 ... 

El clon 'Almacenamiento' ha sido creado exitosamente.
```

**Explicaci√≥n del comando**:
- `virt-clone`: Herramienta para clonar m√°quinas virtuales existentes
- `--original mvp1`: Especifica la m√°quina virtual de origen a clonar
- `--name Almacenamiento`: Asigna el nombre "Almacenamiento" a la nueva m√°quina virtual
- `--file /var/lib/libvirt/images/Almacenamiento.qcow2`: Especifica la ruta y nombre del archivo de imagen del disco para la nueva VM
- `--mac 00:16:3e:76:57:a0`: Asigna una direcci√≥n MAC espec√≠fica a la interfaz de red de la m√°quina clonada

```bash
root@lq-d25:~# virt-clone --original mvp1 --name Nodo1 --file /var/lib/libvirt/images/Nodo1.qcow2 --mac 00:16:3e:2a:bc:c9
Allocating 'Nodo1.qcow2'                                    | 1.6 GB  00:00 ... 

El clon 'Nodo1' ha sido creado exitosamente.
```

```bash
root@lq-d25:~# virt-clone --original mvp1 --name Nodo2 --file /var/lib/libvirt/images/Nodo2.qcow2 --mac 00:16:3e:35:90:9c
Allocating 'Nodo2.qcow2'                                    | 1.5 GB  00:00 ... 

El clon 'Nodo2' ha sido creado exitosamente.
```

#### Configuraci√≥n del nodo Almacenamiento (target)

Primero se elimina el dispositivo CD-ROM y se reconfigura la red:

```bash
root@lq-d25:~# virsh detach-disk Almacenamiento sda --persistent
El disco ha sido desmontado exitosamente

root@lq-d25:~# virsh domiflist Almacenamiento
 Interfaz   Tipo      Fuente    Modelo   MAC
------------------------------------------------------------
 -          network   default   virtio   00:16:3e:76:57:a0

root@lq-d25:~# virsh detach-interface Almacenamiento --type network --mac 00:16:3e:76:57:a0 --persistent
La interfaz ha sido desmontada exitosamente
```

**Explicaci√≥n de los comandos**:
- `virsh detach-disk`: Elimina un dispositivo de disco de una m√°quina virtual
  - `--persistent`: Hace que el cambio persista despu√©s de reiniciar la VM
- `virsh domiflist`: Muestra las interfaces de red de una m√°quina virtual
- `virsh detach-interface`: Elimina una interfaz de red de la m√°quina virtual
  - `--type network`: Especifica que es una interfaz de tipo red
  - `--mac`: Identifica la interfaz a trav√©s de su direcci√≥n MAC

Luego se crean y a√±aden los vol√∫menes de almacenamiento adicionales:

```bash
root@lq-d25:~# virsh vol-create-as default vol1_p6.qcow2 1G --format qcow2
Se ha creado el volumen vol1_p6.qcow2

root@lq-d25:~# virsh vol-create-as default vol2_p6.img 1G --format raw
Se ha creado el volumen vol2_p6.img

root@lq-d25:~# virsh attach-disk Almacenamiento /var/lib/libvirt/images/vol1_p6.qcow2 sda --driver qemu --type disk --subdriver qcow2 --persistent
El disco ha sido asociado exitosamente

root@lq-d25:~# virsh attach-disk Almacenamiento /var/lib/libvirt/images/vol2_p6.img sdb --driver qemu --type disk --subdriver raw --persistent
El disco ha sido asociado exitosamente
```

**Explicaci√≥n de los comandos**:
- `virsh vol-create-as`: Crea un nuevo volumen en el pool de almacenamiento
  - `default`: Nombre del pool de almacenamiento donde se crea el volumen
  - `vol1_p6.qcow2`: Nombre del nuevo volumen
  - `1G`: Tama√±o del volumen (1 Gigabyte)
  - `--format qcow2`: Formato del disco, QCOW2 permite caracter√≠sticas como snapshots y thin provisioning
  - `--format raw`: Formato raw es m√°s simple pero tiene mejor rendimiento
- `virsh attach-disk`: Asocia un dispositivo de almacenamiento a una m√°quina virtual
  - `--driver qemu`: Especifica el controlador para acceder al disco
  - `--type disk`: Define el dispositivo como un disco completo
  - `--subdriver qcow2/raw`: Especifica el formato del archivo de imagen

Finalmente, se a√±aden las interfaces de red:

```bash
root@lq-d25:~# virsh attach-interface Almacenamiento --type network --source Almacenamiento --model virtio --persistent
La interfaz ha sido asociada exitosamente

root@lq-d25:~# virsh attach-interface Almacenamiento --type network --source Cluster --model virtio --persistent
La interfaz ha sido asociada exitosamente
```

**Explicaci√≥n del comando**:
- `virsh attach-interface`: A√±ade una interfaz de red a una m√°quina virtual
  - `--type network`: Define el tipo de interfaz como red virtual 
  - `--source Almacenamiento/Cluster`: Especifica la red virtual a la que se conectar√°
  - `--model virtio`: Utiliza el modelo de virtualizaci√≥n paravirtualizado para mejor rendimiento
  - `--persistent`: Garantiza que la configuraci√≥n persista despu√©s de reiniciar

#### Configuraci√≥n de interfaces de red en el nodo Almacenamiento

```bash
[root@mvp1 ~]# hostnamectl set-hostname almacenamiento.vpd.com
```

```bash
[root@mvp1 ~]# nmcli con add type ethernet con-name enp8s0 ifname enp8s0 ipv4.method manual ipv4.addresses 10.22.122.10/24
Conexi√≥n ¬´enp8s0¬ª (5dbc051d-b4f5-4990-905f-472a9ef38463) a√±adida con √©xito.
[root@mvp1 ~]# nmcli con up enp8s0
Conexi√≥n activada con √©xito (ruta activa D-Bus: /org/freedesktop/NetworkManager/ActiveConnection/8)
```

**Explicaci√≥n del comando**:
- `hostnamectl set-hostname`: Configura el nombre de host del sistema
- `nmcli con add`: Crea una nueva configuraci√≥n de conexi√≥n de red
  - `type ethernet`: Especifica que es una conexi√≥n cableada
  - `con-name enp8s0`: Nombre de la conexi√≥n
  - `ifname enp8s0`: Interfaz f√≠sica a la que se aplica la configuraci√≥n
  - `ipv4.method manual`: Configura la direcci√≥n IP de forma manual, sin DHCP
  - `ipv4.addresses 10.22.122.10/24`: Establece la direcci√≥n IP y m√°scara de red
- `nmcli con up`: Activa la conexi√≥n configurada

```bash
[root@mvp1 ~]# nmcli con add type ethernet con-name enp9s0 ifname enp9s0 ipv4.method auto
Conexi√≥n ¬´enp9s0¬ª (d39257b3-3a45-4262-91ff-9401751d0e90) a√±adida con √©xito.
[root@mvp1 ~]# nmcli con up enp9s0
Conexi√≥n activada con √©xito (ruta activa D-Bus: /org/freedesktop/NetworkManager/ActiveConnection/9)
```

**Explicaci√≥n del comando**:
- `ipv4.method auto`: Configura la interfaz para obtener su direcci√≥n IP autom√°ticamente v√≠a DHCP

##### Validaciones:

```bash
[root@mvp1 ~]# hostnamectl
     Static hostname: almacenamiento.vpd.com
           Icon name: computer-vm
             Chassis: vm üñ¥
          Machine ID: 6d2630bf3d2046a589c37eaa7313994b
             Boot ID: 3392fb66d8a846e28e308c314b46b009
        Product UUID: 9ac8572e-eda5-44f9-8282-5acbbb68449e
      Virtualization: kvm
    Operating System: Fedora Linux 41 (Server Edition)    
         CPE OS Name: cpe:/o:fedoraproject:fedora:41
      OS Support End: Mon 2025-12-15
OS Support Remaining: 7month 3w
              Kernel: Linux 6.12.11-200.fc41.x86_64
        Architecture: x86-64
     Hardware Vendor: QEMU
      Hardware Model: Standard PC _Q35 + ICH9, 2009_
    Firmware Version: 1.16.3-1.fc39
       Firmware Date: Tue 2014-04-01
        Firmware Age: 11y 3w 3d                           
[root@mvp1 ~]# nmcli device status
DEVICE  TYPE      STATE                   CONNECTION 
enp9s0  ethernet  conectado               enp9s0     
enp8s0  ethernet  conectado               enp8s0     
lo      loopback  connected (externally)  lo         
[root@mvp1 ~]# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host noprefixroute 
       valid_lft forever preferred_lft forever
2: enp8s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:48:9c:c6 brd ff:ff:ff:ff:ff:ff
    inet 10.22.122.10/24 brd 10.22.122.255 scope global noprefixroute enp8s0
       valid_lft forever preferred_lft forever
    inet6 fe80::37b9:2f10:35ed:596c/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
3: enp9s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:52:44:62 brd ff:ff:ff:ff:ff:ff
    inet 192.168.140.80/24 brd 192.168.140.255 scope global dynamic noprefixroute enp9s0
       valid_lft 2218sec preferred_lft 2218sec
    inet6 fe80::3f3d:da7f:2578:6085/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
[root@mvp1 ~]# ping -c 3 8.8.8.8
PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
64 bytes from 8.8.8.8: icmp_seq=1 ttl=114 time=33.8 ms
64 bytes from 8.8.8.8: icmp_seq=2 ttl=114 time=34.3 ms

--- 8.8.8.8 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 33.839/34.090/34.341/0.251 ms
[root@mvp1 ~]# nmcli con show --active
NAME    UUID                                  TYPE      DEVICE 
enp9s0  d39257b3-3a45-4262-91ff-9401751d0e90  ethernet  enp9s0 
enp8s0  5dbc051d-b4f5-4990-905f-472a9ef38463  ethernet  enp8s0 
lo      4ccd3034-48b7-4ca9-af3e-6c44e0aa7acb  loopback  lo     
[root@mvp1 ~]# ip route
default via 192.168.140.1 dev enp9s0 proto dhcp src 192.168.140.80 metric 103 
10.22.122.0/24 dev enp8s0 proto kernel scope link src 10.22.122.10 metric 102 
192.168.140.0/24 dev enp9s0 proto kernel scope link src 192.168.140.80 metric 103 
```

#### Configuraci√≥n de nodos initiator (Nodo1 y Nodo2)

La configuraci√≥n de los nodos iniciadores es similar a la del nodo target, comenzando por eliminar el dispositivo CD-ROM y reconfigurar las interfaces de red:

```bash
root@lq-d25:~# virsh detach-disk Nodo1 sda --persistent
El disco ha sido desmontado exitosamente

root@lq-d25:~# virsh domiflist Nodo1
 Interfaz   Tipo      Fuente    Modelo   MAC
------------------------------------------------------------
 -          network   default   virtio   00:16:3e:2a:bc:c9

root@lq-d25:~# virsh detach-interface Nodo1 --type network --mac 00:16:3e:2a:bc:c9 --persistent
La interfaz ha sido desmontada exitosamente

root@lq-d25:~# virsh attach-interface Nodo1 --type network --source Almacenamiento --model virtio --persistent
La interfaz ha sido asociada exitosamente

root@lq-d25:~# virsh attach-interface Nodo1 --type network --source Cluster --model virtio --persistent
La interfaz ha sido asociada exitosamente

root@lq-d25:~# virsh domiflist Nodo1
 Interfaz   Tipo      Fuente           Modelo   MAC
-------------------------------------------------------------------
 -          network   Almacenamiento   virtio   52:54:00:f9:be:78
 -          network   Cluster          virtio   52:54:00:05:fd:ec
```

Se repite el procedimiento para el segundo nodo:

```bash
root@lq-d25:~# virsh detach-disk Nodo2 sda --persistent
El disco ha sido desmontado exitosamente

root@lq-d25:~# virsh domiflist Nodo2
 Interfaz   Tipo      Fuente    Modelo   MAC
------------------------------------------------------------
 -          network   default   virtio   00:16:3e:35:90:9c

root@lq-d25:~# virsh detach-interface Nodo2 --type network --mac 00:16:3e:35:90:9c --persistent
La interfaz ha sido desmontada exitosamente

root@lq-d25:~# virsh attach-interface Nodo2 --type network --source Almacenamiento --model virtio --persistent
La interfaz ha sido asociada exitosamente

root@lq-d25:~# virsh attach-interface Nodo2 --type network --source Cluster --model virtio --persistent
La interfaz ha sido asociada exitosamente

root@lq-d25:~# virsh domiflist Nodo2
 Interfaz   Tipo      Fuente           Modelo   MAC
-------------------------------------------------------------------
 -          network   Almacenamiento   virtio   52:54:00:a3:82:83
 -          network   Cluster          virtio   52:54:00:33:fd:52
```

#### Configuraci√≥n de red en los nodos initiator

Procedemos a configurar las direcciones IP en ambos nodos. Primero para Nodo1:

```bash
[root@mvp1 ~]# hostnamectl set-hostname nodo1.vpd.com
[root@mvp1 ~]# nmcli con add type ethernet con-name enp1s0 ifname enp1s0 ipv4.method manual ipv4.addresses 10.22.122.11/24
Aviso: hay otra conexi√≥n con el nombre ¬´enp1s0¬ª. Haga referencia a la conexi√≥n por su uuid ¬´a8ab3908-5526-4f91-8171-762e42ac49ea¬ª
Conexi√≥n ¬´enp1s0¬ª (a8ab3908-5526-4f91-8171-762e42ac49ea) a√±adida con √©xito.

[root@nodo1 ~]# nmcli con add type ethernet con-name enp7s0 ifname enp7s0 ipv4.method auto
[root@nodo1 ~]# nmcli con up enp7s0
Conexi√≥n activada con √©xito (ruta activa D-Bus: /org/freedesktop/NetworkManager/ActiveConnection/9)
```

Verificaci√≥n de la configuraci√≥n en Nodo1:

```bash
[root@nodo1 ~]# ip a show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host noprefixroute 
       valid_lft forever preferred_lft forever
2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:f9:be:78 brd ff:ff:ff:ff:ff:ff
    inet 10.22.122.11/24 brd 10.22.122.255 scope global noprefixroute enp1s0
       valid_lft forever preferred_lft forever
    inet6 fe80::698a:e0e6:e8b0:d494/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:05:fd:ec brd ff:ff:ff:ff:ff:ff
    inet 192.168.140.116/24 brd 192.168.140.255 scope global dynamic noprefixroute enp7s0
       valid_lft 3154sec preferred_lft 3154sec
    inet6 fe80::81:d839:b5cd:6ff5/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
```

Validaciones adicionales en Nodo1:

```bash
[root@nodo1 ~]# ip addr show enp1s0 
2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:f9:be:78 brd ff:ff:ff:ff:ff:ff
    inet 10.22.122.11/24 brd 10.22.122.255 scope global noprefixroute enp1s0
       valid_lft forever preferred_lft forever
    inet6 fe80::698a:e0e6:e8b0:d494/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
[root@nodo1 ~]# ip addr show enp7s0 
3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:05:fd:ec brd ff:ff:ff:ff:ff:ff
    inet 192.168.140.116/24 brd 192.168.140.255 scope global dynamic noprefixroute enp7s0
       valid_lft 2937sec preferred_lft 2937sec
    inet6 fe80::81:d839:b5cd:6ff5/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
[root@nodo1 ~]# ping 8.8.8.8
PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
64 bytes from 8.8.8.8: icmp_seq=1 ttl=114 time=34.0 ms
64 bytes from 8.8.8.8: icmp_seq=2 ttl=114 time=34.1 ms

--- 8.8.8.8 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 34.041/34.078/34.115/0.037 ms
```

Configuraci√≥n de red para Nodo2:

```bash
[root@mvp1 ~]# nmcli con add type ethernet con-name enp1s0 ifname enp1s0 ipv4.method manual ipv4.addresses 10.22.122.12/24
Conexi√≥n ¬´enp1s0¬ª (e695cda7-3af3-4246-b5fb-88d7f0ecfbcd) a√±adida con √©xito.
[root@mvp1 ~]# nmcli con add type ethernet con-name enp7s0 ifname enp7s0 ipv4.method auto
[root@mvp1 ~]# nmcli con up enp7s0
Conexi√≥n activada con √©xito (ruta activa D-Bus: /org/freedesktop/NetworkManager/ActiveConnection/6)

[root@mvp1 ~]# ip a show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host noprefixroute 
       valid_lft forever preferred_lft forever
2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:a3:82:83 brd ff:ff:ff:ff:ff:ff
    inet 10.22.122.12/24 brd 10.22.122.255 scope global noprefixroute enp1s0
       valid_lft forever preferred_lft forever
    inet6 fe80::4c9f:ac5d:73af:d24f/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:33:fd:52 brd ff:ff:ff:ff:ff:ff
    inet 192.168.140.120/24 brd 192.168.140.255 scope global dynamic noprefixroute enp7s0
       valid_lft 3429sec preferred_lft 3429sec
    inet6 fe80::da77:319b:6a4c:659/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
```

Verificaci√≥n de Nodo2:

```bash
[root@mvp1 ~]# hostname
nodo2.vpd.com
[root@mvp1 ~]# ip addr show enp1s0 
2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:a3:82:83 brd ff:ff:ff:ff:ff:ff
    inet 10.22.122.12/24 brd 10.22.122.255 scope global noprefixroute enp1s0
       valid_lft forever preferred_lft forever
    inet6 fe80::4c9f:ac5d:73af:d24f/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
[root@mvp1 ~]# ip addr show enp7s0 
3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:33:fd:52 brd ff:ff:ff:ff:ff:ff
    inet 192.168.140.120/24 brd 192.168.140.255 scope global dynamic noprefixroute enp7s0
       valid_lft 3120sec preferred_lft 3120sec
    inet6 fe80::da77:319b:6a4c:659/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
[root@mvp1 ~]# ping 8.8.8.8
PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
64 bytes from 8.8.8.8: icmp_seq=1 ttl=114 time=33.9 ms
64 bytes from 8.8.8.8: icmp_seq=2 ttl=114 time=34.2 ms

--- 8.8.8.8 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 33.855/34.041/34.228/0.186 ms
```

Ran tool
### Tarea 2: Instalaci√≥n y configuraci√≥n de servicio iSCSI en el nodo target

Esta tarea se enfoca en la instalaci√≥n y configuraci√≥n del servicio iSCSI en el nodo target, que ser√° el encargado de exportar los dispositivos de almacenamiento a los nodos initiator.

#### Paso 1: Instalaci√≥n del software iSCSI en el nodo target

```bash
[root@almacenamiento ~]# dnf install targetcli
Actualizando y cargando repositorios:
...
Installing:
 targetcli                 noarch 2.1.58-3.fc41           fedora       275.7 KiB
...
Installing:
 targetcli                 noarch 2.1.58-3.fc41           fedora       275.7 KiB
```

**Explicaci√≥n del comando**:
- `dnf install targetcli`: Instala el paquete targetcli que proporciona una interfaz para configurar el target iSCSI en Linux
  - Este paquete proporciona una shell interactiva para la configuraci√≥n del subsistema kernel target (LIO)

#### Paso 2: Inicio del servicio target

```bash
[root@almacenamiento ~]# systemctl start target
```

**Explicaci√≥n del comando**:
- `systemctl start target`: Inicia el servicio iSCSI target en el sistema
  - El servicio target proporciona la funcionalidad necesaria para exportar almacenamiento block a trav√©s de la red

#### Paso 3: Configuraci√≥n del arranque autom√°tico

```bash
[root@almacenamiento ~]# systemctl enable target
Created symlink '/etc/systemd/system/multi-user.target.wants/target.service' ‚Üí '/usr/lib/systemd/system/target.service'.
```

**Explicaci√≥n del comando**:
- `systemctl enable target`: Configura el servicio para que se inicie autom√°ticamente durante el arranque del sistema
  - Crea un enlace simb√≥lico en el directorio de servicios del nivel de ejecuci√≥n multi-usuario

#### Paso 4: Configuraci√≥n del cortafuegos

```bash
[root@almacenamiento ~]# firewall-cmd --permanent --add-port=3260/tcp
success
[root@almacenamiento ~]# firewall-cmd --reload
success
```

**Explicaci√≥n del comando**:
- `firewall-cmd --permanent --add-port=3260/tcp`: Abre el puerto 3260/TCP en el cortafuegos
  - `--permanent`: Hace que la regla persista despu√©s de reiniciar el servicio firewall
  - `3260/tcp`: Puerto est√°ndar utilizado por el protocolo iSCSI
- `firewall-cmd --reload`: Recarga la configuraci√≥n del cortafuegos para aplicar los cambios inmediatamente

#### Paso 5: Configuraci√≥n del recurso de almacenamiento a exportar

```bash
[root@almacenamiento ~]# targetcli
targetcli shell version 2.1.58
Copyright 2011-2013 by Datera, Inc and others.
For help on commands, type 'help'.
/> ls
o- / ..................................................................... [...]
  o- backstores .......................................................... [...]
  | o- block .............................................. [Storage Objects: 0]
  | o- fileio ............................................. [Storage Objects: 0]
  | o- pscsi .............................................. [Storage Objects: 0]
  | o- ramdisk ............................................ [Storage Objects: 0]
  o- iscsi ........................................................ [Targets: 0]
  o- loopback ..................................................... [Targets: 0]
  o- vhost ........................................................ [Targets: 0]
```

**Explicaci√≥n del comando**:
- `targetcli`: Inicia la interfaz de configuraci√≥n del target iSCSI
- `ls`: Muestra la estructura jer√°rquica de configuraci√≥n del target, que incluye:
  - `backstores`: Dispositivos de almacenamiento de respaldo
  - `iscsi`: Configuraci√≥n de targets iSCSI
  - `loopback` y `vhost`: Otros tipos de target disponibles

A continuaci√≥n, se configura el dispositivo de respaldo:

```bash
/> cd /backstores/block 
/backstores/block>
```

```bash
/backstores/block> create name=discosda dev=/dev/sdb
```

**Explicaci√≥n del comando**:
- `cd /backstores/block`: Navega al directorio de configuraci√≥n de dispositivos de respaldo de tipo bloque
- `create name=discosda dev=/dev/sdb`: Crea un objeto de almacenamiento block
  - `name=discosda`: Nombre l√≥gico asignado al dispositivo
  - `dev=/dev/sdb`: Dispositivo f√≠sico que se exportar√° a trav√©s de iSCSI

Luego se crea el target iSCSI:

```bash
/backstores/block> cd /iscsi 
/iscsi> create wwn=iqn.2025-04.com.vpd:discosda
Created target iqn.2025-04.com.vpd:discosda.
Created TPG 1.
Global pref auto_add_default_portal=true
Created default portal listening on all IPs (0.0.0.0), port 3260.
/iscsi> ls
o- iscsi .......................................................... [Targets: 1]
  o- iqn.2025-04.com.vpd:discosda .................................... [TPGs: 1]
    o- tpg1 ............................................. [no-gen-acls, no-auth]
      o- acls ........................................................ [ACLs: 0]
      o- luns ........................................................ [LUNs: 0]
      o- portals .................................................. [Portals: 1]
        o- 0.0.0.0:3260 ................................................... [OK]
```

**Explicaci√≥n del comando**:
- `cd /iscsi`: Navega al directorio de configuraci√≥n de targets iSCSI
- `create wwn=iqn.2025-04.com.vpd:discosda`: Crea un nuevo target iSCSI
  - `wwn=iqn.2025-04.com.vpd:discosda`: Identificador √∫nico del target (IQN)
    - `iqn`: Internet Qualified Name, formato est√°ndar para nombres iSCSI
    - `2025-04`: Fecha de registro del dominio (a√±o-mes)
    - `com.vpd`: Dominio invertido del propietario
    - `discosda`: Nombre espec√≠fico del target

Se configura el portal de red:

```bash
/iscsi> cd iqn.2025-04.com.vpd:discosda/tpg1/portals
/iscsi/iqn.20.../tpg1/portals> delete 0.0.0.0 3260
Deleted network portal 0.0.0.0:3260
```

```bash
/iscsi/iqn.20.../tpg1/portals> create 10.22.122.10
Using default IP port 3260
Created network portal 10.22.122.10:3260.
```

**Explicaci√≥n del comando**:
- `cd iqn.2025-04.com.vpd:discosda/tpg1/portals`: Navega a la configuraci√≥n de portales del target
- `delete 0.0.0.0 3260`: Elimina el portal por defecto que escucha en todas las interfaces
- `create 10.22.122.10`: Crea un nuevo portal que solo escucha en la direcci√≥n espec√≠fica
  - Esta configuraci√≥n aumenta la seguridad al limitar el acceso solo a trav√©s de la red de almacenamiento

Se configura el LUN (Logical Unit Number):

```bash
/iscsi/iqn.20.../tpg1/portals> cd ../luns 
/iscsi/iqn.20...sda/tpg1/luns> create /backstores/block/discosda 
Created LUN 0.
```

**Explicaci√≥n del comando**:
- `cd ../luns`: Navega a la configuraci√≥n de LUNs
- `create /backstores/block/discosda`: Asocia el objeto de almacenamiento creado anteriormente con el LUN 0
  - El LUN es un identificador num√©rico usado para distinguir entre m√∫ltiples dispositivos l√≥gicos exportados

Finalmente, se configuran las ACLs (Access Control Lists):

```bash
/iscsi/iqn.20...sda/tpg1/luns> cd ../acls 
/iscsi/iqn.20...sda/tpg1/acls> create wwn=iqn.2025-04.com.vpd:nodo1
Created Node ACL for iqn.2025-04.com.vpd:nodo1
Created mapped LUN 0.
/iscsi/iqn.20...sda/tpg1/acls> create wwn=iqn.2025-04.com.vpd:nodo2
Created Node ACL for iqn.2025-04.com.vpd:nodo2
Created mapped LUN 0.
```

**Explicaci√≥n del comando**:
- `cd ../acls`: Navega a la configuraci√≥n de listas de control de acceso
- `create wwn=iqn.2025-04.com.vpd:nodo1`: Crea una entrada ACL para el primer nodo initiator
- `create wwn=iqn.2025-04.com.vpd:nodo2`: Crea una entrada ACL para el segundo nodo initiator
  - Las ACLs definen qu√© initiators pueden conectarse al target
  - Cada entrada corresponde al nombre IQN de un nodo client

Se guarda la configuraci√≥n y se sale de la interfaz:

```
/iscsi/iqn.20...sda/tpg1/acls> cd /
/> saveconfig 
Configuration saved to /etc/target/saveconfig.json
/> exit
Global pref auto_save_on_exit=true
Last 10 configs saved in /etc/target/backup/.
Configuration saved to /etc/target/saveconfig.json
[root@almacenamiento ~]# 
```

**Explicaci√≥n del comando**:
- `cd /`: Vuelve al directorio ra√≠z de la configuraci√≥n
- `saveconfig`: Guarda la configuraci√≥n actual en el archivo `/etc/target/saveconfig.json`
- `exit`: Sale de la interfaz de configuraci√≥n

> **Nota**: La configuraci√≥n del target iSCSI se guarda autom√°ticamente al salir, pero es una buena pr√°ctica guardarla expl√≠citamente antes de salir para asegurarse de que todos los cambios est√°n persistidos.

Ran tool
### Tarea 3: Instalaci√≥n del soporte iSCSI en los nodos initiator

Esta tarea se centra en la instalaci√≥n y configuraci√≥n del software necesario en los nodos initiator para que puedan conectarse al target iSCSI y utilizar el almacenamiento compartido.

#### Paso 1: Instalaci√≥n del software del cliente iSCSI

```bash
[root@nodo1 ~]# dnf install iscsi-initiator-utils
Actualizando y cargando repositorios:
 Fedora 41 openh264 (From Cisco) - x86_ 100% |   3.1 KiB/s | 989.0   B |  00m00s
 Fedora 41 - x86_64 - Updates           100% |  40.7 KiB/s |  23.9 KiB |  00m01s
 Fedora 41 - x86_64                     100% |  84.9 KiB/s |  26.2 KiB |  00m00s
 Fedora 41 - x86_64 - Updates           100% |   3.2 MiB/s |   8.5 MiB |  00m03s
Repositorios cargados.
Package "iscsi-initiator-utils-6.2.1.10-0.gitd0f04ae.fc41.1.x86_64" is already installed.

Nothing to do.
```

**Explicaci√≥n del comando**:
- `dnf install iscsi-initiator-utils`: Instala el paquete con las utilidades necesarias para que el sistema act√∫e como initiator iSCSI
  - Este paquete incluye herramientas como `iscsiadm` para la administraci√≥n de sesiones iSCSI y el demonio `iscsid` que gestiona las conexiones

#### Paso 2: Ejecuci√≥n del servicio iscsid

```bash
[root@nodo1 ~]# systemctl restart iscsid
```

**Explicaci√≥n del comando**:
- `systemctl restart iscsid`: Reinicia el servicio iscsid, que es el demonio que gestiona las conexiones iSCSI
  - Este servicio es fundamental para mantener las sesiones iSCSI y gestionar la reconexi√≥n autom√°tica

Como resultado de la ejecuci√≥n del servicio se crear√° el archivo de configuraci√≥n `/etc/iscsi/initiatorname.iscsi` que contiene el nombre IQN del initiator:

```bash
[root@nodo1 ~]# cat /etc/iscsi/initiatorname.iscsi 
InitiatorName=iqn.1994-05.com.redhat:265ed95a5ff3
```

```bash
[root@nodo2 ~]# cat /etc/iscsi/initiatorname.iscsi 
InitiatorName=iqn.1994-05.com.redhat:a66dcccd74dc
```

#### Paso 3: Configuraci√≥n del nombre del nodo

Para ser coherente con la configuraci√≥n de ACLs en el target, es necesario modificar el nombre del initiator:

Nodo 1:

```bash
[root@nodo1 ~]# cat /etc/iscsi/initiatorname.iscsi 
InitiatorName=iqn.2025-04.com.vpd:nodo1
```

Nodo 2:

```
[root@nodo2 ~]# cat /etc/iscsi/initiatorname.iscsi 
InitiatorName=iqn.2025-04.com.vpd:nodo2
```

**Explicaci√≥n**:
- El archivo `/etc/iscsi/initiatorname.iscsi` se ha editado para establecer nombres IQN personalizados
- El formato utilizado es: `iqn.a√±o-mes.dominio.invertido:nombre`
- Estos nombres deben coincidir exactamente con los configurados en las ACLs del target

#### Paso 4: Descubrimiento de LUNs exportados

```bash
[root@nodo1 ~]# iscsiadm --mode discovery --type sendtargets --portal 10.22.122.10 --discover
10.22.122.10:3260,1 iqn.2025-04.com.vpd:discosda
```

```bash
[root@nodo2 ~]# iscsiadm --mode discovery --type sendtargets --portal 10.22.122.10 --discover
10.22.122.10:3260,1 iqn.2025-04.com.vpd:discosda
```

**Explicaci√≥n del comando**:
- `iscsiadm --mode discovery --type sendtargets --portal 10.22.122.10 --discover`: Realiza un descubrimiento de targets iSCSI disponibles
  - `--mode discovery`: Opera en modo descubrimiento para buscar targets
  - `--type sendtargets`: Utiliza el m√©todo SendTargets, el m√°s com√∫n para descubrimiento iSCSI
  - `--portal 10.22.122.10`: Direcci√≥n IP del target iSCSI a consultar
  - `--discover`: Indica que se debe realizar una operaci√≥n de descubrimiento

#### Paso 5: Conexi√≥n de unidades LUNs

```bash
[root@nodo1 ~]# sudo iscsiadm --mode node --targetname iqn.2025-04.com.vpd:discosda --portal 10.22.122.10 --login
Logging in to [iface: default, target: iqn.2025-04.com.vpd:discosda, portal: 10.22.122.10,3260]
Login to [iface: default, target: iqn.2025-04.com.vpd:discosda, portal: 10.22.122.10,3260] successful.
```

```bash
[root@nodo2 ~]# sudo iscsiadm --mode node --targetname iqn.2025-04.com.vpd:discosda --portal 10.22.122.10 --login
Logging in to [iface: default, target: iqn.2025-04.com.vpd:discosda, portal: 10.22.122.10,3260]
Login to [iface: default, target: iqn.2025-04.com.vpd:discosda, portal: 10.22.122.10,3260] successful.
```

**Explicaci√≥n del comando**:
- `iscsiadm --mode node --targetname iqn.2025-04.com.vpd:discosda --portal 10.22.122.10 --login`: Inicia una sesi√≥n iSCSI con el target
  - `--mode node`: Opera en modo nodo, para gestionar sesiones iSCSI
  - `--targetname iqn.2025-04.com.vpd:discosda`: Especifica el nombre IQN del target al que conectarse
  - `--portal 10.22.122.10`: Direcci√≥n IP y puerto del portal iSCSI
  - `--login`: Indica que se debe iniciar sesi√≥n en el target

#### Paso 6: Comprobaci√≥n de la conexi√≥n

Verificar en los logs del sistema:

```bash
[root@nodo1 ~]# sudo journalctl --since today | grep -i iscsi
abr 25 19:15:55 nodo1.vpd.com sudo[1323]:     root : TTY=ttyS0 ; PWD=/root ; USER=root ; COMMAND=/usr/sbin/iscsiadm --mode node --targetname iqn.2025-04.com.vpd:discosda --portal 10.22.122.10 --login
abr 25 19:15:55 nodo1.vpd.com kernel: scsi host6: iSCSI Initiator over TCP/IP
abr 25 19:15:55 nodo1.vpd.com iscsid[1286]: iscsid: Connection1:0 to [target: iqn.2025-04.com.vpd:discosda, portal: 10.22.122.10,3260] through [iface: default] is operational now
```

```bash
[root@nodo2 ~]# sudo journalctl --since today | grep -i iscsi
abr 25 19:16:00 nodo2.vpd.com sudo[1243]:     root : TTY=ttyS0 ; PWD=/root ; USER=root ; COMMAND=/usr/sbin/iscsiadm --mode node --targetname iqn.2025-04.com.vpd:discosda --portal 10.22.122.10 --login
abr 25 19:16:00 nodo2.vpd.com kernel: scsi host6: iSCSI Initiator over TCP/IP
abr 25 19:16:00 nodo2.vpd.com iscsid[1234]: iscsid: Connection1:0 to [target: iqn.2025-04.com.vpd:discosda, portal: 10.22.122.10,3260] through [iface: default] is operational now
```

**Explicaci√≥n del comando**:
- `journalctl --since today | grep -i iscsi`: Filtra los logs del sistema para mostrar las entradas relacionadas con iSCSI
  - La salida muestra la creaci√≥n del host SCSI virtual para la conexi√≥n iSCSI
  - Tambi√©n confirma que la conexi√≥n est√° operativa a trav√©s de la interfaz predeterminada

Verificaci√≥n de sesiones activas:

```bash
[root@nodo1 ~]#  iscsiadm -m session 
tcp: [1] 10.22.122.10:3260,1 iqn.2025-04.com.vpd:discosda (non-flash)
```

```
[root@nodo2 ~]# iscsiadm -m session 
tcp: [1] 10.22.122.10:3260,1 iqn.2025-04.com.vpd:discosda (non-flash)
```

**Explicaci√≥n del comando**:
- `iscsiadm -m session`: Muestra informaci√≥n sobre las sesiones iSCSI activas
  - `tcp: [1]`: Indica el n√∫mero de sesi√≥n
  - `10.22.122.10:3260,1`: Muestra la IP, puerto y grupo del portal
  - `iqn.2025-04.com.vpd:discosda`: Nombre del target conectado
  - `(non-flash)`: Indica que no se trata de un dispositivo flash optimizado

Verificaci√≥n de dispositivos disponibles con lsblk:

```
[root@nodo1 ~]# lsblk -o NAME,TRAN,SIZE,TYPE,MOUNTPOINT
NAME            TRAN    SIZE TYPE MOUNTPOINT
sda             iscsi     1G disk 
zram0                   1,9G disk [SWAP]
vda             virtio   10G disk 
‚îú‚îÄvda1          virtio    1M part 
‚îú‚îÄvda2          virtio    1G part /boot
‚îî‚îÄvda3          virtio    9G part 
  ‚îî‚îÄfedora-root           9G lvm  /
```

```
[root@nodo2 ~]# lsblk -o NAME,TRAN,SIZE,TYPE,MOUNTPOINT
NAME            TRAN    SIZE TYPE MOUNTPOINT
sda             iscsi     1G disk 
zram0                   1,9G disk [SWAP]
vda             virtio   10G disk 
‚îú‚îÄvda1          virtio    1M part 
‚îú‚îÄvda2          virtio    1G part /boot
‚îî‚îÄvda3          virtio    9G part 
  ‚îî‚îÄfedora-root           9G lvm  /
```

**Explicaci√≥n del comando**:
- `lsblk -o NAME,TRAN,SIZE,TYPE,MOUNTPOINT`: Muestra informaci√≥n detallada sobre los dispositivos de bloques
  - `NAME`: Nombre del dispositivo
  - `TRAN`: Tipo de transporte (iscsi para dispositivos iSCSI)
  - `SIZE`: Tama√±o del dispositivo
  - `TYPE`: Tipo de dispositivo (disk, part, lvm)
  - `MOUNTPOINT`: Punto de montaje, si est√° montado

> **Nota**: En la salida se puede ver que el dispositivo `sda` est√° disponible con transporte `iscsi` y un tama√±o de 1G, que corresponde al disco exportado desde el target.

Ran tool

Ran tool
### Tarea 4: Creaci√≥n de sistema de archivos tipo ext4 en la unidad iSCSI importada

En esta tarea se formatear√° la unidad iSCSI importada con un sistema de archivos ext4 y se montar√° para su uso.

```bash
[root@nodo1 ~]# mkfs.ext4 /dev/sda
mke2fs 1.47.1 (20-May-2024)
Se est√° creando un sistema de ficheros con 262144 bloques de 4k y 65536 nodos-i
UUID del sistema de ficheros: 733dd613-51be-4533-9245-f71c2afe9bb2
Respaldos del superbloque guardados en los bloques: 
	32768, 98304, 163840, 229376

Reservando las tablas de grupo: hecho                           
Escribiendo las tablas de nodos-i: hecho                           
Creando el fichero de transacciones (8192 bloques): hecho
Escribiendo superbloques y la informaci√≥n contable del sistema de ficheros: hecho
```

**Explicaci√≥n del comando**:
- `mkfs.ext4 /dev/sda`: Crea un sistema de archivos ext4 en el dispositivo iSCSI
  - Este comando formatea el dispositivo completo, destruyendo cualquier dato existente
  - Los par√°metros se establecen autom√°ticamente basados en el tama√±o del dispositivo
  - El sistema generar√° un UUID √∫nico para el sistema de archivos
  - Los respaldos del superbloque permiten recuperar el sistema de archivos en caso de da√±o

```bash
[root@nodo1 ~]# mount /dev/sda /mnt
```

**Explicaci√≥n del comando**:
- `mount /dev/sda /mnt`: Monta el sistema de archivos reci√©n creado en el directorio /mnt
  - Esta operaci√≥n hace que el sistema de archivos sea accesible a trav√©s del punto de montaje /mnt
  - No se especifican opciones adicionales, por lo que se utilizan las opciones por defecto

> **Nota**: El sistema de archivos ext4 es una buena elecci√≥n para almacenamiento compartido b√°sico debido a su amplia compatibilidad y soporte para recuperaci√≥n despu√©s de fallos.

### Tarea 5: Exportaci√≥n del segundo disco iSCSI y creaci√≥n de un volumen l√≥gico

Esta tarea demuestra una configuraci√≥n m√°s avanzada, donde se exporta un segundo disco iSCSI y se crea un volumen l√≥gico (LVM) que puede ser compartido entre m√∫ltiples nodos.

#### 1. Exportaci√≥n del disco /dev/sdb desde el nodo target

```bash
[root@almacenamiento ~]# targetcli
targetcli shell version 2.1.58
Copyright 2011-2013 by Datera, Inc and others.
For help on commands, type 'help'.

/> cd backstores/block/
/backstores/block> 

/backstores/block> create name=discosdb dev=/dev/sdb
Created block storage object discosdb using /dev/sdb.

/backstores/block> cd /iscsi

/iscsi> create wwn=iqn.2025-04.com.vpd:servidorapache
Created target iqn.2025-04.com.vpd:servidorapache.
Created TPG 1.
Default portal not created, TPGs within a target cannot share ip:port.
```

**Explicaci√≥n del comando**:
- `create name=discosdb dev=/dev/sdb`: Crea un objeto de almacenamiento con nombre "discosdb" respaldado por el dispositivo f√≠sico /dev/sdb
- `create wwn=iqn.2025-04.com.vpd:servidorapache`: Crea un segundo target iSCSI con un identificador diferente para el segundo disco
  - El mensaje "Default portal not created..." indica que no se cre√≥ un portal por defecto porque no se pueden compartir las mismas combinaciones IP:puerto entre diferentes targets

```bash
/iscsi> cd iqn.2025-04.com.vpd:servidorapache/tpg1/portals

/iscsi/iqn.20.../tpg1/portals> delete 0.0.0.0 3260
No such NetworkPortal in configfs: /sys/kernel/config/target/iscsi/iqn.2025-04.com.vpd:servidorapache/tpgt_1/np/0.0.0.0:3260

/iscsi/iqn.20.../tpg1/portals> create 10.22.122.10
Using default IP port 3260
Created network portal 10.22.122.10:3260.
```

**Explicaci√≥n del comando**:
- La eliminaci√≥n del portal por defecto falla porque no se cre√≥ autom√°ticamente
- `create 10.22.122.10`: Crea un portal espec√≠fico en la direcci√≥n IP de la red de almacenamiento
  - Utiliza el puerto predeterminado 3260, que es el est√°ndar para iSCSI

```bash
/iscsi/iqn.20.../tpg1/portals> cd ..
/iscsi/iqn.20...orapache/tpg1> cd luns 
/iscsi/iqn.20...che/tpg1/luns> create /backstores/block/discosdb
Created LUN 0.
```

**Explicaci√≥n del comando**:
- `create /backstores/block/discosdb`: Asocia el objeto de almacenamiento al LUN 0 del target
  - La numeraci√≥n de LUNs comienza desde 0 y es visible para los initiators

```bash
/iscsi/iqn.20...che/tpg1/luns> cd ..
/iscsi/iqn.20...orapache/tpg1> cd acls 
/iscsi/iqn.20...che/tpg1/acls> create wwn=iqn.2025-04.com.vpd:nodo1
Created Node ACL for iqn.2025-04.com.vpd:nodo1
Created mapped LUN 0.
/iscsi/iqn.20...che/tpg1/acls> create wwn=iqn.2025-04.com.vpd:nodo2
Created Node ACL for iqn.2025-04.com.vpd:nodo2
Created mapped LUN 0.
```

**Explicaci√≥n del comando**:
- `create wwn=iqn.2025-04.com.vpd:nodo1`: Crea una ACL para el primer nodo initiator
- `create wwn=iqn.2025-04.com.vpd:nodo2`: Crea una ACL para el segundo nodo initiator
  - Las ACLs definen qu√© initiators pueden acceder al target
  - Para cada ACL se crea autom√°ticamente un mapeo al LUN 0

```bash
/iscsi/iqn.20...che/tpg1/acls> cd /
/> saveconfig 
Configuration saved to /etc/target/saveconfig.json
/> exit
Global pref auto_save_on_exit=true
Last 10 configs saved in /etc/target/backup/.
Configuration saved to /etc/target/saveconfig.json
```

**Explicaci√≥n del comando**:
- `saveconfig`: Guarda la configuraci√≥n de forma expl√≠cita antes de salir
- Al salir, se muestra que la configuraci√≥n se guarda autom√°ticamente y se mantienen las √∫ltimas 10 versiones

#### 2. Comprobaci√≥n de la exportaci√≥n en los nodos initiator

```bash
[root@nodo1 ~]# iscsiadm --mode discovery --type sendtargets --portal 10.22.122.10 --discover
10.22.122.10:3260,1 iqn.2025-04.com.vpd:discosda
10.22.122.10:3260,1 iqn.2025-04.com.vpd:servidorapache
```

```bash
[root@nodo2 ~]# iscsiadm --mode discovery --type sendtargets --portal 10.22.122.10 --discover
10.22.122.10:3260,1 iqn.2025-04.com.vpd:discosda
10.22.122.10:3260,1 iqn.2025-04.com.vpd:servidorapache
```

**Explicaci√≥n del comando**:
- Ahora el descubrimiento muestra dos targets disponibles desde el mismo portal:
  - El target original `iqn.2025-04.com.vpd:discosda`
  - El nuevo target `iqn.2025-04.com.vpd:servidorapache`

```bash
[root@nodo1 ~]# iscsiadm --mode node \
         --targetname iqn.2025-04.com.vpd:servidorapache \
         --portal 10.22.122.10 --login
Logging in to [iface: default, target: iqn.2025-04.com.vpd:servidorapache, portal: 10.22.122.10,3260]
Login to [iface: default, target: iqn.2025-04.com.vpd:servidorapache, portal: 10.22.122.10,3260] successful.
```

```bash
[root@nodo2 ~]# iscsiadm --mode node \
         --targetname iqn.2025-04.com.vpd:servidorapache \
         --portal 10.22.122.10 --login
Logging in to [iface: default, target: iqn.2025-04.com.vpd:servidorapache, portal: 10.22.122.10,3260]
Login to [iface: default, target: iqn.2025-04.com.vpd:servidorapache, portal: 10.22.122.10,3260] successful.
```

**Explicaci√≥n del comando**:
- Con estos comandos, ambos nodos inician sesi√≥n con el nuevo target iSCSI
- La opci√≥n `--targetname` especifica exactamente con qu√© target se desea conectar

Verificaci√≥n:

```bash
[root@nodo1 ~]# lsblk -o NAME,TRAN,SIZE,TYPE
NAME            TRAN    SIZE TYPE
sda             iscsi     1G disk
sdb             iscsi     1G disk
zram0                   1,9G disk
vda             virtio   10G disk
‚îú‚îÄvda1          virtio    1M part
‚îú‚îÄvda2          virtio    1G part
‚îî‚îÄvda3          virtio    9G part
  ‚îî‚îÄfedora-root           9G lvm
```

```
[root@nodo2 ~]# lsblk -o NAME,TRAN,SIZE,TYPE
NAME            TRAN    SIZE TYPE
sda             iscsi     1G disk
sdb             iscsi     1G disk
zram0                   1,9G disk
vda             virtio   10G disk
‚îú‚îÄvda1          virtio    1M part
‚îú‚îÄvda2          virtio    1G part
‚îî‚îÄvda3          virtio    9G part
  ‚îî‚îÄfedora-root           9G lvm
```

**Explicaci√≥n del comando**:
- Despu√©s de conectarse, aparece un segundo dispositivo iSCSI (`sdb`) en ambos nodos
- La columna `TRAN` muestra que ambos dispositivos son de tipo `iscsi`

#### 3. Creaci√≥n del volumen l√≥gico

##### I. Creaci√≥n del volumen f√≠sico

```bash
[root@nodo1 ~]# pvcreate /dev/sdb
  Physical volume "/dev/sdb" successfully created.
```

**Explicaci√≥n del comando**:
- `pvcreate /dev/sdb`: Inicializa el dispositivo iSCSI como un volumen f√≠sico de LVM
  - Escribe una cabecera PV al inicio del dispositivo
  - Esta es la base para la creaci√≥n posterior de vol√∫menes l√≥gicos

##### II. Creaci√≥n del grupo de vol√∫menes ApacheVG

```bash
[root@nodo1 ~]# vgcreate --setautoactivation n --locktype none ApacheVG /dev/sdb
  Volume group "ApacheVG" successfully created
```

**Explicaci√≥n del comando**:
- `vgcreate`: Crea un grupo de vol√∫menes donde se pueden crear vol√∫menes l√≥gicos
  - `--setautoactivation n`: Desactiva la activaci√≥n autom√°tica del VG en el arranque
    - Esto es esencial para entornos compartidos para evitar activaciones conflictivas
  - `--locktype none`: No utiliza bloqueo, permitiendo que el VG sea activado desde cualquier host
    - Esta es una configuraci√≥n simplificada; en producci√≥n, normalmente se usar√≠a un sistema de bloqueo distribuido
  - `ApacheVG`: Nombre del grupo de vol√∫menes
  - `/dev/sdb`: Volumen f√≠sico a incluir en el grupo

##### III. Creaci√≥n del volumen l√≥gico ApacheLV

```bash
[root@nodo1 ~]# lvcreate -n ApacheLV -L 900M ApacheVG
```

**Explicaci√≥n del comando**:
- `lvcreate`: Crea un volumen l√≥gico dentro del grupo de vol√∫menes
  - `-n ApacheLV`: Especifica el nombre del volumen l√≥gico
  - `-L 900M`: Define el tama√±o del volumen (900 MiB)
  - `ApacheVG`: Nombre del grupo de vol√∫menes donde se crear√° el LV

```bash
[root@nodo1 ~]# lvs
  LV       VG       Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  ApacheLV ApacheVG -wi-a----- 900,00m                                                    
  root     fedora   -wi-ao----  <9,00g
```

**Explicaci√≥n del comando**:
- `lvs`: Muestra informaci√≥n sobre los vol√∫menes l√≥gicos
  - Se muestra el volumen ApacheLV reci√©n creado con un tama√±o de 900 MB
  - Tambi√©n se muestra el volumen l√≥gico root del sistema, que pertenece a otro grupo de vol√∫menes (fedora)

##### IV. Creaci√≥n del sistema de archivos XFS

```bash
[root@nodo1 ~]# mkfs.xfs /dev/ApacheVG/ApacheLV
meta-data=/dev/ApacheVG/ApacheLV isize=512    agcount=4, agsize=57600 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=1
         =                       reflink=1    bigtime=1 inobtcount=1 nrext64=1
data     =                       bsize=4096   blocks=230400, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=16384, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
```

**Explicaci√≥n del comando**:
- `mkfs.xfs`: Crea un sistema de archivos XFS en el volumen l√≥gico
  - XFS es especialmente adecuado para entornos compartidos y vol√∫menes grandes
  - La salida muestra los diversos par√°metros del sistema de archivos:
    - `isize=512`: Tama√±o de los inodos
    - `agcount=4`: N√∫mero de grupos de asignaci√≥n
    - `crc=1`: Se utilizan checksums CRC para validar datos
    - `reflink=1`: Soporte para copias eficientes (reflink)

##### V. Prueba de montaje y validaci√≥n

```bash
[root@nodo1 ~]# mount /dev/ApacheVG/ApacheLV /mnt
```

**Explicaci√≥n del comando**:
- Monta el volumen l√≥gico en el directorio /mnt para acceder al sistema de archivos

```bash
[root@nodo1 ~]# pvs
  PV         VG       Fmt  Attr PSize   PFree 
  /dev/sdb   ApacheVG lvm2 a--  992,00m 92,00m
  /dev/vda3  fedora   lvm2 a--   <9,00g     0 
[root@nodo1 ~]# vgs
  VG       #PV #LV #SN Attr   VSize   VFree 
  ApacheVG   1   1   0 wz--n- 992,00m 92,00m
  fedora     1   1   0 wz--n-  <9,00g     0 
[root@nodo1 ~]# lvs
  LV       VG       Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  ApacheLV ApacheVG -wi-ao---- 900,00m                                                    
  root     fedora   -wi-ao----  <9,00g
```

**Explicaci√≥n de los comandos**:
- `pvs`: Muestra informaci√≥n sobre vol√∫menes f√≠sicos
  - `/dev/sdb` es parte del grupo ApacheVG con espacio libre de 92 MB
- `vgs`: Muestra informaci√≥n sobre grupos de vol√∫menes
  - `ApacheVG` tiene 1 PV, 1 LV, y atributo "wz--n-" (write, resizable, sin autoactivaci√≥n)
- `lvs`: Muestra informaci√≥n detallada sobre vol√∫menes l√≥gicos
  - El atributo "-wi-ao----" indica "writable, active, open"

```bash
[root@nodo1 ~]# mount | grep ApacheVG
/dev/mapper/ApacheVG-ApacheLV on /mnt type xfs (rw,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,noquota)
```

**Explicaci√≥n del comando**:
- Verifica que el volumen est√° correctamente montado
- Muestra las opciones de montaje del sistema de archivos XFS

#### 4. Configuraci√≥n en el segundo nodo initiator

```bash
[root@nodo2 ~]# lvmdevices --adddev /dev/sdb
```

**Explicaci√≥n del comando**:
- `lvmdevices --adddev /dev/sdb`: A√±ade el dispositivo iSCSI al inventario de dispositivos LVM en el segundo nodo
  - Esto es necesario para que LVM reconozca y pueda gestionar este dispositivo

#### 5. Comprobaci√≥n de activaci√≥n en ambos nodos

```bash
[root@nodo1 ~]# vgchange -ay ApacheVG 
  1 logical volume(s) in volume group "ApacheVG" now active
[root@nodo1 ~]# mount /dev/ApacheVG/ApacheLV /mnt
[root@nodo1 ~]# df -h /mnt
S.ficheros                    Tama√±o Usados  Disp Uso% Montado en
/dev/mapper/ApacheVG-ApacheLV   836M    49M  788M   6% /mnt
```

```bash
[root@nodo2 ~]# vgchange -ay ApacheVG 
  1 logical volume(s) in volume group "ApacheVG" now active
[root@nodo2 ~]# mount /dev/ApacheVG/ApacheLV /mnt
[root@nodo2 ~]# df -h /mnt
S.ficheros                    Tama√±o Usados  Disp Uso% Montado en
/dev/mapper/ApacheVG-ApacheLV   836M    49M  788M   6% /mnt
```

**Explicaci√≥n del comando**:
- `vgchange -ay ApacheVG`: Activa el grupo de vol√∫menes en el nodo
  - `-a`: Cambia el estado de activaci√≥n
  - `y`: Activo (yes)
- `mount /dev/ApacheVG/ApacheLV /mnt`: Monta el volumen l√≥gico
- `df -h /mnt`: Muestra informaci√≥n sobre el espacio utilizado
  - El sistema de archivos XFS tiene un tama√±o total de 836 MB, con 49 MB utilizados

> **Nota**: Es importante resaltar que en entornos de producci√≥n, cuando m√∫ltiples nodos acceden al mismo volumen l√≥gico, se necesita un sistema de archivos en cluster o un mecanismo de bloqueo para prevenir la corrupci√≥n de datos. En este ejemplo simplificado, los nodos deber√≠an coordinarse manualmente para evitar acceder al mismo tiempo al volumen.

## 4. Pruebas y Validaci√≥n

Esta secci√≥n demuestra que la infraestructura iSCSI se ha configurado correctamente y que los vol√∫menes est√°n accesibles y funcionales desde ambos nodos.

### Comprobaci√≥n del nodo target (Almacenamiento)

Verificar las interfaces de red:

```bash
[root@almacenamiento ~]# ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host noprefixroute 
       valid_lft forever preferred_lft forever
2: enp8s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:48:9c:c6 brd ff:ff:ff:ff:ff:ff
    inet 10.22.122.10/24 brd 10.22.122.255 scope global noprefixroute enp8s0
       valid_lft forever preferred_lft forever
    inet6 fe80::37b9:2f10:35ed:596c/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
3: enp9s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:52:44:62 brd ff:ff:ff:ff:ff:ff
    inet 192.168.140.80/24 brd 192.168.140.255 scope global dynamic noprefixroute enp9s0
       valid_lft 3588sec preferred_lft 3588sec
    inet6 fe80::3f3d:da7f:2578:6085/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
```

```
[root@almacenamiento ~]# nmcli device status 
DEVICE  TYPE      STATE                   CONNECTION 
enp9s0  ethernet  conectado               enp9s0     
enp8s0  ethernet  conectado               enp8s0     
lo      loopback  connected (externally)  lo
```

Discos presentes:

```
[root@almacenamiento ~]# lsblk -o NAME,TRAN,SIZE,TYPE
NAME            TRAN    SIZE TYPE
sda             spi       1G disk
sdb             spi       1G disk
zram0                   1,9G disk
vda             virtio   10G disk
‚îú‚îÄvda1          virtio    1M part
‚îú‚îÄvda2          virtio    1G part
‚îî‚îÄvda3          virtio    9G part
  ‚îî‚îÄfedora-root           9G lvm
```

Configuraci√≥n iSCSI exportada:

```bash
[root@almacenamiento ~]# targetcli /iscsi ls
o- iscsi .......................................................... [Targets: 2]
  o- iqn.2025-04.com.vpd:discosda .................................... [TPGs: 1]
  | o- tpg1 ............................................. [no-gen-acls, no-auth]
  |   o- acls ........................................................ [ACLs: 2]
  |   | o- iqn.2025-04.com.vpd:nodo1 .......................... [Mapped LUNs: 1]
  |   | | o- mapped_lun0 ............................ [lun0 block/discosda (rw)]
  |   | o- iqn.2025-04.com.vpd:nodo2 .......................... [Mapped LUNs: 1]
  |   |   o- mapped_lun0 ............................ [lun0 block/discosda (rw)]
  |   o- luns ........................................................ [LUNs: 1]
  |   | o- lun0 ................. [block/discosda (/dev/sda) (default_tg_pt_gp)]
  |   o- portals .................................................. [Portals: 1]
  |     o- 10.22.122.10:3260 .............................................. [OK]
  o- iqn.2025-04.com.vpd:servidorapache .............................. [TPGs: 1]
    o- tpg1 ............................................. [no-gen-acls, no-auth]
      o- acls ........................................................ [ACLs: 2]
      | o- iqn.2025-04.com.vpd:nodo1 .......................... [Mapped LUNs: 1]
      | | o- mapped_lun0 ............................ [lun0 block/discosdb (rw)]
      | o- iqn.2025-04.com.vpd:nodo2 .......................... [Mapped LUNs: 1]
      |   o- mapped_lun0 ............................ [lun0 block/discosdb (rw)]
      o- luns ........................................................ [LUNs: 1]
      | o- lun0 ................. [block/discosdb (/dev/sdb) (default_tg_pt_gp)]
      o- portals .................................................. [Portals: 1]
        o- 10.22.122.10:3260 .............................................. [OK]
```

### Comprobaci√≥n del nodo initiator (Nodo1)

Interfaces de red:

```bash
[root@nodo1 ~]# ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host noprefixroute 
       valid_lft forever preferred_lft forever
2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:f9:be:78 brd ff:ff:ff:ff:ff:ff
    inet 10.22.122.11/24 brd 10.22.122.255 scope global noprefixroute enp1s0
       valid_lft forever preferred_lft forever
    inet6 fe80::698a:e0e6:e8b0:d494/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:05:fd:ec brd ff:ff:ff:ff:ff:ff
    inet 192.168.140.116/24 brd 192.168.140.255 scope global dynamic noprefixroute enp7s0
       valid_lft 3154sec preferred_lft 3154sec
    inet6 fe80::81:d839:b5cd:6ff5/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
```

Probar escritura en el sistema de archivos ext4:

```bash
[root@nodo1 ~]# mount /dev/sda /mnt
[root@nodo1 ~]# echo "VPD1" > /mnt/test1
[root@nodo1 ~]# umount /mnt
```

Probar escritura en el volumen l√≥gico:

```bash
[root@nodo1 ~]# vgchange -ay ApacheVG 
  1 logical volume(s) in volume group "ApacheVG" now active
[root@nodo1 ~]# mount /dev/ApacheVG/ApacheLV /mnt
[root@nodo1 ~]# echo "VPD2" > /mnt/test2
[root@nodo1 ~]# umount /mnt
```

### Comprobaci√≥n del nodo initiator (Nodo2)

Verificar que podemos acceder a los datos escritos desde Nodo1:

```bash
[root@nodo2 ~]# mount /dev/sda /mnt
[root@nodo2 ~]# cat /mnt/test1 
VPD1
[root@nodo2 ~]# umount /mnt
```

```bash
[root@nodo2 ~]# vgchange -ay ApacheVG 
  1 logical volume(s) in volume group "ApacheVG" now active
[root@nodo2 ~]# mount /dev/ApacheVG/ApacheLV /mnt
[root@nodo2 ~]# cat /mnt/test2 
VPD2
```

Estas pruebas confirman que:
1. Los dispositivos iSCSI se han exportado correctamente desde el nodo target
2. Ambos nodos initiator pueden conectarse a los dispositivos iSCSI
3. Se puede crear y utilizar un sistema de archivos ext4 directamente sobre un dispositivo iSCSI
4. Se puede crear un volumen l√≥gico sobre un dispositivo iSCSI y utilizarlo desde m√∫ltiples nodos
5. Los datos escritos por un nodo son visibles por otro nodo

## 5. Conclusiones

> Las siguientes conclusiones sintetizan los aspectos m√°s relevantes y las buenas pr√°cticas extra√≠das de la puesta en marcha del servicio de almacenamiento iSCSI siguiendo la documentaci√≥n oficial de RHEL¬†7.

1. **Entorno virtualizado reproducible**  
   Se ha desplegado satisfactoriamente una topolog√≠a completa (target‚ÄØ+‚ÄØdos initiators) utilizando KVM y redes aisladas, lo que permite repetir la pr√°ctica de forma fiable y segura.

2. **iSCSI como soluci√≥n SAN flexible**  
   La implementaci√≥n de LIO mediante `targetcli` demuestra que, con software incluido de serie en RHEL‚ÄØ7, es posible construir una SAN IP de bajo coste y alto rendimiento, prescindiendo de hardware Fibre Channel dedicado.

3. **Control de acceso robusto**  
   La combinaci√≥n de IQN √∫nicos, ACL por iniciador y el uso recomendado de autenticaci√≥n‚ÄØCHAP (pendiente de implementaci√≥n en la pr√°ctica) ofrece una capa s√≥lida de seguridad para entornos multi‚Äëcliente.

4. **Almacenamiento de bloques exportado y gestionado**  
   Se han exportado discos‚ÄØiSCSI tanto como dispositivos individuales como a trav√©s de LVM, evidenciando la versatilidad del esquema backend de LIO (fileio, block, etc.) descrito en la gu√≠a de administraci√≥n de almacenamiento de Red¬†Hat.

5. **Sistemas de archivos adecuados al caso de uso**  
   EXT4 proporciona simplicidad y compatibilidad; XFS otorga escalabilidad y rendimiento. Elegir el sistema de archivos apropiado seg√∫n la carga de trabajo y si habr√° acceso concurrente es fundamental.

6. **LVM sobre iSCSI para mayor flexibilidad**  
   El uso de `pvcreate`‚ÄØ‚Üí‚ÄØ`vgcreate`‚ÄØ‚Üí‚ÄØ`lvcreate` sobre los LUN remotos permite ampliar o reducir capacidad sin interrumpir el servicio, conforme a la filosof√≠a de gesti√≥n din√°mica de vol√∫menes de RHEL¬†7.

7. **Almacenamiento compartido y coherencia de datos**  
   Se verific√≥ acceso concurrente desde ambos nodos; sin embargo, para entornos productivos se debe a√±adir bloqueo en cl√∫ster (p.ej. CLVM‚ÄØ+‚ÄØdlm o GFS2) para evitar corrupci√≥n de datos.

8. **Buenas pr√°cticas operativas**  
   - Dedicaci√≥n de VLAN o red f√≠sica separada para tr√°fico iSCSI.  
   - Habilitaci√≥n de multipath‚ÄëIO para alta disponibilidad.  
   - Monitorizaci√≥n continua de sesiones con `iscsiadm -m session` y del estado de LIO con `targetcli /iscsi ls`.  
   - Respaldos peri√≥dicos del archivo `/etc/target/saveconfig.json`.

Con esta pr√°ctica el alumno ha aprendido a dise√±ar, desplegar y validar una soluci√≥n de almacenamiento iSCSI completa, asentando las bases para evolucionar hacia arquitecturas de alta disponibilidad basadas en RHEL‚ÄØ7.

## 6. Bibliograf√≠a

1. Red Hat Documentation. (2023). "Storage Administration Guide. RED HAT ENTERPRISE LINUX 7". Disponible en: https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/storage_administration_guide/index [accedido el 02/04/2025]

2. Fedora Project. (2024). "Fedora 41 System Administrator's Guide: Storage". Disponible en: https://docs.fedoraproject.org/en-US/fedora/f41/system-administrators-guide/storage/

3. Linux Foundation. (2024). "LVM Administrator's Guide". Linux Documentation Project. Disponible en: https://tldp.org/HOWTO/LVM-HOWTO/

4. Open-iSCSI Project. (2023). "Open-iSCSI Administrator's Manual". Disponible en: https://github.com/open-iscsi/open-iscsi/blob/master/doc/README

5. Target Framework. (2023). "targetcli-fb Reference Manual". Disponible en: http://linux-iscsi.org/wiki/Targetcli
