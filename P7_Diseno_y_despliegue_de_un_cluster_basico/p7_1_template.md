# Pr치ctica 7.1: Dise침o y despliegue de la infraestructura de un cl칰ster b치sico para proporcionar un servicio en alta disponibilidad

## 칈ndice de contenidos

- [Pr치ctica 7.1: Dise침o y despliegue de la infraestructura de un cl칰ster b치sico para proporcionar un servicio en alta disponibilidad](#pr치ctica-71-dise침o-y-despliegue-de-la-infraestructura-de-un-cl칰ster-b치sico-para-proporcionar-un-servicio-en-alta-disponibilidad)
  - [칈ndice de contenidos](#칤ndice-de-contenidos)
  - [1. Introducci칩n](#1-introducci칩n)
  - [2. Requisitos previos](#2-requisitos-previos)
  - [3. Desarrollo de la pr치ctica](#3-desarrollo-de-la-pr치ctica)
    - [Fase 1. Creaci칩n de la infraestructura b치sica del cl칰ster](#fase-1-creaci칩n-de-la-infraestructura-b치sica-del-cl칰ster)
      - [Tarea 1.1. Preparaci칩n de la infraestructura](#tarea-11-preparaci칩n-de-la-infraestructura)
    - [Comandos detallados por pasos](#comandos-detallados-por-pasos)
    - [Fase 2. Instalaci칩n del servidor Apache](#fase-2-instalaci칩n-del-servidor-apache)
      - [Tarea 2.1. Instalaci칩n y configuraci칩n del servidor Apache](#tarea-21-instalaci칩n-y-configuraci칩n-del-servidor-apache)
    - [Comandos detallados para Tarea 2.1](#comandos-detallados-para-tarea-21)
      - [Tarea 2.2. Configuraci칩n del almacenamiento compartido para Apache](#tarea-22-configuraci칩n-del-almacenamiento-compartido-para-apache)
    - [Comandos detallados para Fase 2](#comandos-detallados-para-fase-2)
    - [Paso 12: Copia de seguridad (estado\_2)](#paso-12-copia-de-seguridad-estado_2)
  - [4. Pruebas y validaci칩n](#4-pruebas-y-validaci칩n)
    - [Pruebas de conectividad](#pruebas-de-conectividad)
    - [Verificaci칩n de Apache](#verificaci칩n-de-apache)
    - [Verificaci칩n de almacenamiento compartido](#verificaci칩n-de-almacenamiento-compartido)
    - [Recomendaci칩n](#recomendaci칩n)
  - [5. Conclusiones](#5-conclusiones)
  - [6. Bibliograf칤a](#6-bibliograf칤a)

## 1. Introducci칩n

El objetivo fundamental de esta actividad es dise침ar y desplegar un cl칰ster en el que se ejecute un servidor web Apache en alta disponibilidad.

Para ello, en la primera parte de la actividad se deber치 dise침ar la infraestructura b치sica del cl칰ster para dar soporte a dicho servicio: n칰mero de nodos, rol de cada nodo, infraestructura de red necesaria, etc. En segundo lugar, una vez definido el dise침o del cl칰ster, se deber치 desplegar toda la infraestructura b치sica: creaci칩n de las m치quinas, instalaci칩n del SO en los nodos, creaci칩n de la infraestructura de red, etc.

Una vez dise침ada y desplegada la infraestructura b치sica del cl칰ster, se debe proceder a la instalaci칩n de los m칩dulos software para el soporte de computaci칩n en cl칰ster, poner en marcha dichos m칩dulos software y finalmente crear un cl칰ster que proporcione el servicio indicado en alta disponibilidad (esto 칰ltimo se realizar치 en la Pr치ctica 7.2).

Informaci칩n m치s detallada se encuentra en las siguientes fuentes bibliogr치ficas:

- "High Availability Add-On Overview". En este manual de Red Hat se introduce el conjunto de herramientas y componentes de Red Hat 7 que dan soporte a la computaci칩n en Cl칰ster (denominado High Avalilability Add-On). Su lectura proporciona una visi칩n general de los diferentes componentes del conjunto de herramientas y sus funciones.

- "High Availability Add-On Administration". En el cap칤tulo 1 de este manual de administraci칩n de Red Hat se explica el proceso de despliegue e instalaci칩n del conjunto de herramientas y componentes de Red Hat 7 que dan soporte a la computaci칩n en Cl칰ster (denominado High Avalilability Add-On). Adem치s, en el cap칤tulo 2 se explica, como ejemplo, c칩mo realizar el despliegue de un servidor apache en alta disponibilidad con Red Hat High Avalilability Add-On.

- "Configuring and managing high availability clusters". Esta fuente bibliogr치fica contiene contenidos de las dos fuentes anteriores pero actualizados a la versi칩n Red Hat Enterprise Linux 9.

## 2. Requisitos previos

Para abordar esta pr치ctica es muy recomendable haber ejecutado todo el plan de pr치cticas previo propuesto en la asignatura, pues se trata de un conjunto de pr치cticas que pretende proporcionar los conocimientos necesarios para poder afrontar con garant칤a el desarrollo de la actividad que se plantea: la puesta en funcionamiento de un servicio en alta disponibilidad con el conjunto de herramientas proporcionado por Red Hat, High Avalability Add-On, a trav칠s de una infraestructura virtual soportada por KVM.

Para acometer esta actividad, debe partir de los resultados obtenidos en la pr치ctica sobre almacenamiento iSCSI. Concretamente debe reutilizar la red aislada y las tres m치quinas virtuales desplegadas en esta actividad pr치ctica. **Si a칰n no ha culminado correctamente dicha actividad, real칤cela y entonces acometa esta actividad**.

## 3. Desarrollo de la pr치ctica

### Fase 1. Creaci칩n de la infraestructura b치sica del cl칰ster

Se deber치 dise침ar un servicio en alta disponibilidad en el que intervendr치n tres m치quinas, de las cuales dos de ellas formar치n un cl칰ster. Se tendr치 como requerimiento que el tr치fico de control del cl칰ster, el de acceso al almacenamiento compartido y el generado por el servicio ofrecido al exterior, no deber치n emplear la misma infraestructura de red. Adem치s, cada nodo deber치 tener acceso a los repositorios de software externos a trav칠s de una infraestructura independiente de las anteriores.

El rol de cada nodo es el siguiente:

- Un nodo dar치 servicio de almacenamiento compartido al cl칰ster mediante la tecnolog칤a iSCSI. Este nodo no formar치 parte del cl칰ster. Este nodo ser치 una m치quina virtual cuyo nombre debe ser Almacenamiento y el nombre de dominio completamente cualificado ser치 `almacenamiento.vpd.com`. Debe utilizar el nodo target (m치quina Almacenamiento) desplegado en la pr치ctica iSCSI.

- Dos nodos dar치n el servicio. El servicio proporcionado ser치 un servidor web Apache. Estos nodos ser치n dos m치quinas virtuales cuyos nombres deben ser Nodo1 y Nodo2 y los nombres de dominio completamente cualificados ser치n `nodo1.vpd.com` y `nodo2.vpd.com` respectivamente. Debe utilizar los dos nodos initiators (m치quinas Nodo1 y Nodo2) que despleg칩 en la pr치ctica sobre almacenamiento iSCSI.

Las caracter칤sticas de la infraestructura que se utilizar치 en esta actividad se resumen a continuaci칩n:

- Los tres nodos (m치quinas virtuales) deben tener los siguientes recursos: 1 CPU, 2GB RAM, 1 disco de 10GB de tipo virtio y una instalaci칩n m칤nima de Fedora Server 41 actualizada.

- Tres redes con las siguientes caracter칤sticas:

  - La red de tipo NAT de nombre Cluster utilizada en la pr치ctica sobre almacenamiento iSCSI.
  - La red aislada de nombre Almacenamiento utilizada en la pr치ctica anterior.
  - Otra red aislada de nombre Control con direcci칩n de red 10.22.132.0/24 y sin servicio DHCP activo.

- El nodo `almacenamiento.vpd.com` tendr치 dos interfaces de red. La primera interfaz estar치 conectada a la red Almacenamiento con la direcci칩n 10.22.122.10. La segunda interfaz estar치 conectada a la red NAT Cluster que permitir치 la conexi칩n con el exterior. Como ya se ha comentado, debe utilizar la m치quina Almacenamiento desplegada en la pr치ctica sobre almacenamiento iSCSI para este nodo.

- El nodo `nodo1.vpd.com` tendr치 tres interfaces de red. La primera interfaz estar치 conectada a la red aislada Almacenamiento con la direcci칩n 10.22.122.11. La segunda interfaz estar치 conectada a la red NAT Cluster, posibilitando la conexi칩n con el exterior. Por 칰ltimo, la tercera interfaz estar치 conectada a la red aislada Control con la direcci칩n 10.22.132.11. Como ya se ha comentado, debe utilizar la m치quina Nodo1 desplegada en la pr치ctica sobre almacenamiento iSCSI para este nodo. Sin embargo, tenga en cuenta que debe variar su configuraci칩n para que se cumplan las especificaciones relativas a las interfaces de red.

- El nodo `nodo2.vpd.com` tendr치 tres interfaces de red. La primera interfaz estar치 conectada a la red Almacenamiento con la direcci칩n 10.22.122.12. La segunda interfaz estar치 conectada a la red NAT Cluster, posibilitando la conexi칩n con el exterior. Por 칰ltimo, la tercera interfaz estar치 conectada a la red aislada Control, con la direcci칩n 10.22.132.12. Como ya se ha comentado, debe utilizar la m치quina Nodo2 desplegada en la pr치ctica sobre almacenamiento iSCSI para este nodo. Sin embargo, tenga en cuenta que debe variar su configuraci칩n para que se cumplan las especificaciones relativas a las interfaces de red.

#### Tarea 1.1. Preparaci칩n de la infraestructura

Plan de trabajo para esta fase (asumiendo que se parte de los resultados de la pr치ctica sobre iSCSI):

1. Crear la red privada Control.

```control.xml
root@lq-d25:~# cat control.xml 
<network>
	<name>Control</name>
	<bridge name='virbr3' stp='on' delay='0'/>
	<ip address='10.22.132.1' netmask='255.255.255.0'>
	</ip>
</network>
```

```bash
root@lq-d25:~# virsh net-define control.xml 
La red Control se encuentra definida desde control.xml

root@lq-d25:~# virsh net-start Control 
La red Control se ha iniciado

root@lq-d25:~# virsh net-autostart Control 
La red Control ha sido marcada para iniciarse autom치ticamente
```

```bash
root@lq-d25:~# virsh net-list --all
 Nombre           Estado   Inicio autom치tico   Persistente
------------------------------------------------------------
 Almacenamiento   activo   si                  si
 Cluster          activo   si                  si
 Control          activo   si                  si
 default          activo   si                  si
```

```bash
root@lq-d25:~# virsh net-info Control 
Nombre:         Control
UUID:           710208f0-b476-41f4-9a2d-20596c791dbf
Activar:        si
Persistente:    si
Autoinicio:     si
Puente:         virbr3
```

```bash
root@lq-d25:~# ip addr show virbr3
7: virbr3: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether 52:54:00:7c:d5:29 brd ff:ff:ff:ff:ff:ff
    inet 10.22.132.1/24 brd 10.22.132.255 scope global virbr3
       valid_lft forever preferred_lft forever
```


2. En Nodo1 y Nodo2 a침adir una nueva interfaz de red.

**En nodo 1**:

```bash
root@lq-d25:~# sudo virsh attach-interface --domain Nodo1 --type network --source Control --model virtio --config
La interfaz ha sido asociada exitosamente
```

```bash
[root@nodo1 ~]# nmcli connection add type ethernet con-name Control ifname enp8s0 ipv4.m
ipv4.may-fail  ipv4.method
```

```bash
[root@nodo1 ~]# nmcli connection add type ethernet con-name Control ifname enp8s0 ipv4.method manual ipv4.addresses 10.122.132.11/24
Conexi칩n 춺Control췉 (5bd4dd51-3ae1-4952-96b0-f1b552750a84) a침adida con 칠xito.
```

```bash
[root@nodo1 ~]# nmcli connection up Control
Conexi칩n activada con 칠xito (ruta activa D-Bus: /org/freedesktop/NetworkManager/ActiveConnection/9)
```

```bash
[root@nodo1 ~]# ip addr show enp8s0 
4: enp8s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:af:20:ef brd ff:ff:ff:ff:ff:ff
    inet 10.122.132.11/24 brd 10.122.132.255 scope global noprefixroute enp8s0
       valid_lft forever preferred_lft forever
    inet6 fe80::20a0:8206:7964:ea87/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
```


**En nodo 2**:

```bash
root@lq-d25:~# sudo virsh attach-interface --domain Nodo2 --type network --source Control --model virtio --config
La interfaz ha sido asociada exitosamente
```

```bash
[root@nodo2 ~]# nmcli connection add type ethernet con-name Control ifname enp8s0 ipv4.method manual ipv4.addresses 10.122.132.12/24
Conexi칩n 춺Control췉 (44c1efa5-be2c-4565-9baf-655865d579fe) a침adida con 칠xito.
```

```bash
[root@nodo2 ~]# nmcli connection up Control 
Conexi칩n activada con 칠xito (ruta activa D-Bus: /org/freedesktop/NetworkManager/ActiveConnection/9)
```

```bash
[root@nodo2 ~]# ip addr show enp8s0
4: enp8s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:6b:ad:d9 brd ff:ff:ff:ff:ff:ff
    inet 10.122.132.12/24 brd 10.122.132.255 scope global noprefixroute enp8s0
       valid_lft forever preferred_lft forever
    inet6 fe80::e87a:1d40:fc79:1f7f/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
```

3. En Nodo1 y Nodo2 reconfigurar las interfaces de red para que cumplan las especificaciones. Esto es que la primera interfaz est칠 conectada a la red Almacenamiento, la segunda interfaz a la red Cluster y, por 칰ltimo, la tercera interface a la red Control. Recuerde que la configuraci칩n de las interfaces de las redes Almacenamiento y Control se debe establecer de forma manual.

Nodo 1:

```bash
[root@nodo1 ~]# ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host noprefixroute 
       valid_lft forever preferred_lft forever
2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:f9:be:78 brd ff:ff:ff:ff:ff:ff
    inet 10.22.122.11/24 brd 10.22.122.255 scope global noprefixroute enp1s0
       valid_lft forever preferred_lft forever
    inet6 fe80::698a:e0e6:e8b0:d494/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:05:fd:ec brd ff:ff:ff:ff:ff:ff
    inet 192.168.140.116/24 brd 192.168.140.255 scope global dynamic noprefixroute enp7s0
       valid_lft 2978sec preferred_lft 2978sec
    inet6 fe80::81:d839:b5cd:6ff5/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
4: enp8s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:af:20:ef brd ff:ff:ff:ff:ff:ff
    inet 10.122.132.11/24 brd 10.122.132.255 scope global noprefixroute enp8s0
       valid_lft forever preferred_lft forever
    inet6 fe80::20a0:8206:7964:ea87/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
```

Nodo 2:

```bash
[root@nodo2 ~]# ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host noprefixroute 
       valid_lft forever preferred_lft forever
2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:a3:82:83 brd ff:ff:ff:ff:ff:ff
    inet 10.22.122.12/24 brd 10.22.122.255 scope global noprefixroute enp1s0
       valid_lft forever preferred_lft forever
    inet6 fe80::4c9f:ac5d:73af:d24f/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
3: enp7s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:33:fd:52 brd ff:ff:ff:ff:ff:ff
    inet 192.168.140.120/24 brd 192.168.140.255 scope global dynamic noprefixroute enp7s0
       valid_lft 2951sec preferred_lft 2951sec
    inet6 fe80::da77:319b:6a4c:659/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
4: enp8s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:6b:ad:d9 brd ff:ff:ff:ff:ff:ff
    inet 10.122.132.12/24 brd 10.122.132.255 scope global noprefixroute enp8s0
       valid_lft forever preferred_lft forever
    inet6 fe80::e87a:1d40:fc79:1f7f/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
```

4. Hacer un "update" de la instalaci칩n existente en cada uno de los nodos, esto es en las m치quinas Nodo1 y Nodo2. **Importante, no debe hacer la operaci칩n "update" en el sistema anfitri칩n**.

En nodo 1:
```bash
[root@nodo1 ~]# dnf update -y
춰Completado!
[root@nodo1 ~]# 

```

En nodo 2:

```bash
[root@nodo2 ~]# dnf update -y
# ...
춰Completado!
[root@nodo2 ~]# 
```

5. Establecer el nombre de dominio completamente cualificado en cada m치quina (orden hostnamectl). En el caso del nodo Almacenamiento `almacenamiento.vpd.com`, en el caso de Nodo1 `nodo1.vpd.com` y en el caso de Nodo2 `nodo2.vpd.com`.

Nodo 1:

```bash
[root@nodo1 ~]# hostnamectl
     Static hostname: nodo1.vpd.com
           Icon name: computer-vm
             Chassis: vm 游둾
          Machine ID: 6d2630bf3d2046a589c37eaa7313994b
             Boot ID: a7eacab9640649c08caed9e34c076a9e
        Product UUID: 2b88a0ed-78cc-4ac8-ac5b-a665173f8f11
      Virtualization: kvm
    Operating System: Fedora Linux 41 (Server Edition)    
         CPE OS Name: cpe:/o:fedoraproject:fedora:41
      OS Support End: Mon 2025-12-15
OS Support Remaining: 7month 1w 6d
              Kernel: Linux 6.12.11-200.fc41.x86_64
        Architecture: x86-64
     Hardware Vendor: QEMU
      Hardware Model: Standard PC _Q35 + ICH9, 2009_
    Firmware Version: 1.16.3-1.fc39
       Firmware Date: Tue 2014-04-01
        Firmware Age: 11y 1month 1d
```

Nodo 2:

```bash
[root@nodo2 ~]# hostnamectl
     Static hostname: nodo2.vpd.com
           Icon name: computer-vm
             Chassis: vm 游둾
          Machine ID: 6d2630bf3d2046a589c37eaa7313994b
             Boot ID: 31ef90efc7f743f9a9abd9bb8a65c798
        Product UUID: bebe4eb9-cab1-4101-aedc-32920714da29
      Virtualization: kvm
    Operating System: Fedora Linux 41 (Server Edition)    
         CPE OS Name: cpe:/o:fedoraproject:fedora:41
      OS Support End: Mon 2025-12-15
OS Support Remaining: 7month 1w 6d
              Kernel: Linux 6.12.11-200.fc41.x86_64
        Architecture: x86-64
     Hardware Vendor: QEMU
      Hardware Model: Standard PC _Q35 + ICH9, 2009_
    Firmware Version: 1.16.3-1.fc39
       Firmware Date: Tue 2014-04-01
        Firmware Age: 11y 1month 1d
```

Almacenamiento:

```bash
[root@almacenamiento ~]# hostnamectl
     Static hostname: almacenamiento.vpd.com
           Icon name: computer-vm
             Chassis: vm 游둾
          Machine ID: 6d2630bf3d2046a589c37eaa7313994b
             Boot ID: 72ada47f3a314ca29a3fb47756055687
        Product UUID: 9ac8572e-eda5-44f9-8282-5acbbb68449e
      Virtualization: kvm
    Operating System: Fedora Linux 41 (Server Edition)    
         CPE OS Name: cpe:/o:fedoraproject:fedora:41
      OS Support End: Mon 2025-12-15
OS Support Remaining: 7month 1w 6d
              Kernel: Linux 6.12.11-200.fc41.x86_64
        Architecture: x86-64
     Hardware Vendor: QEMU
      Hardware Model: Standard PC _Q35 + ICH9, 2009_
    Firmware Version: 1.16.3-1.fc39
       Firmware Date: Tue 2014-04-01
        Firmware Age: 11y 1month 1d
```

6. A침adir los nombres e IPs de todas las m치quinas que intervienen en la pr치ctica en el fichero `/etc/hosts`:

En nodo 1:

```bash
[root@nodo2 ~]# cat /etc/hosts
# Loopback entries; do not change.
# For historical reasons, localhost precedes localhost.localdomain:
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.22.122.10	almacenamiento.vpd.com
10.22.132.11	nodo1.vpd.com
10.22.132.12	nodo2.vpd.com
```

En nodo 2:

```bash
[root@nodo1 ~]# cat /etc/hosts
# Loopback entries; do not change.
# For historical reasons, localhost precedes localhost.localdomain:
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.22.122.10	almacenamiento.vpd.com
10.22.132.11	nodo1.vpd.com
10.22.132.12	nodo2.vpd.com
```

En Almacenamiento:

```bash
[root@almacenamiento ~]# cat /etc/hosts
# Loopback entries; do not change.
# For historical reasons, localhost precedes localhost.localdomain:
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.22.122.10	almacenamiento.vpd.com
10.22.122.11	nodo1.vpd.com
10.22.122.12	nodo2.vpd.com
```

**IMPORTANTE**: Al concluir esta fase deber치 comprobar que todas las m치quinas tienen conectividad al exterior y entre ellas utilizando las distintas interfaces de red. Para ello es fundamental que desde cada m치quina realice pruebas de conexi칩n con el resto de las m치quinas para cada interfaz, empleando para ello tanto las direcciones IP asignadas a cada interfaz como los nombres de dominio completamente cualificados de las m치quinas. Compruebe tambi칠n la conectividad con la puerta de enlace del host anfitri칩n para cada red (10.22.122.1 y 10.22.132.1). Estas pruebas de conexi칩n las puede realizar empleando, por ejemplo, la orden ping. Al concluir esta fase se recomienda hacer copias de seguridad de los tres nodos (estado_1).

> **Nota**: Antes de continuar con la siguiente fase debe validar el trabajo realizado con los profesores de la asignatura.

### Comandos detallados por pasos

```bash
# Paso 1: Crear la red privada Control
sudo virsh net-define control.xml  # o usar virt-manager para crear una red aislada 10.22.132.0/24
sudo virsh net-start Control
sudo virsh net-autostart Control

# Paso 2: A침adir una nueva interfaz de red (para Control) a Nodo1 y Nodo2
sudo virsh attach-interface --domain Nodo1 --type network --source Control --model virtio --config --live
sudo virsh attach-interface --domain Nodo2 --type network --source Control --model virtio --config --live

# Paso 3: Reconfigurar interfaces de red en Nodo1 y Nodo2
# Nodo1
sudo virsh start Nodo1
sudo virsh console Nodo1
hostnamectl set-hostname nodo1.vpd.com
nmcli con add type ethernet con-name enp1s0 ifname enp1s0 ipv4.method manual ipv4.addresses 10.22.122.11/24
nmcli con add type ethernet con-name enp7s0 ifname enp7s0 ipv4.method auto
nmcli con add type ethernet con-name enp8s0 ifname enp8s0 ipv4.method manual ipv4.addresses 10.22.132.11/24
nmcli con up enp1s0
nmcli con up enp7s0
nmcli con up enp8s0

# Nodo2
sudo virsh start Nodo2
sudo virsh console Nodo2
hostnamectl set-hostname nodo2.vpd.com
nmcli con add type ethernet con-name enp1s0 ifname enp1s0 ipv4.method manual ipv4.addresses 10.22.122.12/24
nmcli con add type ethernet con-name enp7s0 ifname enp7s0 ipv4.method auto
nmcli con add type ethernet con-name enp8s0 ifname enp8s0 ipv4.method manual ipv4.addresses 10.22.132.12/24
nmcli con up enp1s0
nmcli con up enp7s0
nmcli con up enp8s0

# Paso 4: Actualizar los nodos
sudo dnf update -y  # Ejecutar solo dentro de Nodo1 y Nodo2

# Paso 5: Establecer hostname (ya hecho en paso 3 con hostnamectl)

# Paso 6: Editar /etc/hosts en todos los nodos
# Nodo1 y Nodo2:
echo -e "127.0.0.1 localhost\n::1 localhost\n10.22.122.10 almacenamiento.vpd.com\n10.22.132.11 nodo1.vpd.com\n10.22.132.12 nodo2.vpd.com" > /etc/hosts

# Nodo almacenamiento:
echo -e "127.0.0.1 localhost\n::1 localhost\n10.22.122.10 almacenamiento.vpd.com\n10.22.122.11 nodo1.vpd.com\n10.22.122.12 nodo2.vpd.com" > /etc/hosts

# Paso final: Validar conectividad
ping -c 3 10.22.122.10  # desde nodo1/nodo2 a almacenamiento
ping -c 3 10.22.132.11  # desde nodo2 a nodo1 y viceversa
ping -c 3 8.8.8.8       # verificar acceso a Internet
```

En este punto nos hemos dado cuenta de que inicialmente pusimos mal las IPs en los nodos 1 y 2 para Control:

Lo corregimos:

En Nodo1

```bash
nmcli con modify Control ipv4.addresses "10.22.132.11/24"
nmcli con up Control
```

En Nodo2

```
nmcli con modify Control ipv4.addresses "10.22.132.12/24"
nmcli con up Control
```

Y vuelve a verificar:

```
ip addr show enp8s0
```

```
[root@nodo1 ~]# ip addr show enp8s0 
4: enp8s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:af:20:ef brd ff:ff:ff:ff:ff:ff
    inet 10.22.132.11/24 brd 10.22.132.255 scope global noprefixroute enp8s0
       valid_lft forever preferred_lft forever
    inet6 fe80::20a0:8206:7964:ea87/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
```

```
[root@nodo2 ~]# ip addr show enp8s0
4: enp8s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:6b:ad:d9 brd ff:ff:ff:ff:ff:ff
    inet 10.22.132.12/24 brd 10.22.132.255 scope global noprefixroute enp8s0
       valid_lft forever preferred_lft forever
    inet6 fe80::e87a:1d40:fc79:1f7f/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
```

Algunas validaciones:

```bash
[root@almacenamiento ~]# ping nodo1.vpd.com
PING nodo1.vpd.com (10.22.122.11) 56(84) bytes of data.
64 bytes from nodo1.vpd.com (10.22.122.11): icmp_seq=1 ttl=64 time=0.321 ms
64 bytes from nodo1.vpd.com (10.22.122.11): icmp_seq=2 ttl=64 time=0.521 ms

--- nodo1.vpd.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 0.321/0.421/0.521/0.100 ms
[root@almacenamiento ~]# ping nodo2.vpd.com
PING nodo2.vpd.com (10.22.122.12) 56(84) bytes of data.
64 bytes from nodo2.vpd.com (10.22.122.12): icmp_seq=1 ttl=64 time=0.385 ms
64 bytes from nodo2.vpd.com (10.22.122.12): icmp_seq=2 ttl=64 time=0.418 ms
64 bytes from nodo2.vpd.com (10.22.122.12): icmp_seq=3 ttl=64 time=0.602 ms

--- nodo2.vpd.com ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2004ms
rtt min/avg/max/mdev = 0.385/0.468/0.602/0.095 ms
```

```
[root@nodo1 ~]# ip addr show enp8s0 
4: enp8s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:af:20:ef brd ff:ff:ff:ff:ff:ff
    inet 10.22.132.11/24 brd 10.22.132.255 scope global noprefixroute enp8s0
       valid_lft forever preferred_lft forever
    inet6 fe80::20a0:8206:7964:ea87/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
[root@nodo1 ~]# ping almacenamiento.vpd.com
PING almacenamiento.vpd.com (10.22.122.10) 56(84) bytes of data.
64 bytes from almacenamiento.vpd.com (10.22.122.10): icmp_seq=1 ttl=64 time=0.319 ms
64 bytes from almacenamiento.vpd.com (10.22.122.10): icmp_seq=2 ttl=64 time=0.676 ms

--- almacenamiento.vpd.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 0.319/0.497/0.676/0.178 ms
```

**Copia de seguridad**

Detenemos las MVs y:

```bash
root@lq-d25:~# mkdir -p /backups/libvirt/$(date +%Y%m%d)
root@lq-d25:~# cp /var/lib/libvirt/images/Nodo1.qcow2 /backups/libvirt/$(date +%Y%m%d)/Nodo1_$(date +%H%M).qcow2
root@lq-d25:~# cp /var/lib/libvirt/images/Nodo2.qcow2 /backups/libvirt/$(date +%Y%m%d)/Nodo2_$(date +%H%M).qcow2
root@lq-d25:~# cp /var/lib/libvirt/images/Almacenamiento.qcow2 /backups/libvirt/$(date +%Y%m%d)/Almacenamiento_$(date +%H%M).qcow2
root@lq-d25:~# ls /backups/
libvirt
root@lq-d25:~# ls /backups/libvirt/
20250508
root@lq-d25:~# ls /backups/libvirt/20250508/
Almacenamiento_1847.qcow2  Nodo1_1846.qcow2  Nodo2_1847.qcow2
```

### Fase 2. Instalaci칩n del servidor Apache

En esta etapa realizaremos la instalaci칩n y configuraci칩n de un servidor web Apache en Nodo1 y Nodo2 utilizando el espacio de almacenamiento compartido iSCSI que nos proporciona el nodo de almacenamiento.

Se deber치 instalar y configurar el servidor Apache en Nodo1 y Nodo2 de manera que el directorio de contenidos del servicio (`/var/www`) se encuentre en el volumen l칩gico apacheLV ubicado en el espacio compartido de almacenamiento desplegado en la pr치ctica sobre iSCSI.

#### Tarea 2.1. Instalaci칩n y configuraci칩n del servidor Apache

### Comandos detallados para Tarea 2.1

```bash
# Nodo1: Instalar y probar Apache
sudo dnf install httpd -y
sudo systemctl disable httpd
sudo systemctl start httpd
curl http://<IP_Cluster_Nodo1>   # verificar p치gina de test
sudo systemctl stop httpd

# Nodo2: Instalar y probar Apache
sudo dnf install httpd -y
sudo systemctl disable httpd
sudo systemctl start httpd
curl http://<IP_Cluster_Nodo2>   # verificar p치gina de test
sudo systemctl stop httpd
```

Plan de trabajo para esta fase:

1. En Nodo1 instalar y configurar el servidor Apache. **No configurar el servicio para que se inicie autom치ticamente con el arranque del sistema**.

2. Arrancando manualmente el servicio, verificar que funciona correctamente accediendo desde el anfitri칩n mediante un navegador o utilizando la orden curl. Si todo est치 correcto, entonces se deber치 mostrar la p치gina de test del servicio Apache. Una vez verificado, parar el servicio Apache. Para ello, podr칤a utilizar las direcciones IP que tienen configuradas las interfaces de red del nodo. Se recomienda probar con la IP de la interfaz conectada a la red NAT Cluster.

3. En Nodo2 instalar y configurar el servidor Apache. **No configurar el servicio para que se inicie autom치ticamente con el arranque del sistema**.

4. Arrancando manualmente el servicio, verificar que funciona correctamente accediendo desde el anfitri칩n mediante un navegador o utilizando la orden curl. Si todo est치 correcto, entonces se deber치 mostrar la p치gina de test del servicio Apache. Una vez verificado, parar el servicio Apache. Para ello, podr칤a utilizar las direcciones IP que tienen configuradas las interfaces de red del nodo. Se recomienda probar con la IP de la interfaz conectada a la red NAT Cluster.


#### Tarea 2.2. Configuraci칩n del almacenamiento compartido para Apache

### Comandos detallados para Fase 2

```bash
# Paso 5: Montar el volumen l칩gico apacheLV
lvscan  # verificar que el volumen apacheLV est치 visible
mkdir -p /var/www
mount /dev/mapper/vgX-apacheLV /var/www  # Reemplaza vgX con el nombre correcto del grupo de vol칰menes

# Paso 6: Preparar estructura de directorios
chcon --user=system_u /var/www
mkdir -p /var/www/html
mkdir -p /var/www/cgi-bin

# Paso 7: Crear archivo index.html
echo "<html><body>Enhorabuena: configuraci칩n correcta</body></html>" > /var/www/html/index.html

# Paso 8: Restaurar contextos SELinux
restorecon -Rv /var/www

# Paso 9: Verificar Apache
sudo systemctl start httpd
curl http://localhost         # o curl http://<ip_nodo>
sudo systemctl stop httpd

# Paso 10: Desmontar el volumen
umount /var/www

# Paso 11: Repetir en el segundo nodo
# Montar volumen, iniciar servicio y verificar
mount /dev/mapper/vgX-apacheLV /var/www
sudo systemctl start httpd
curl http://localhost         # verificar contenido
```

### Paso 12: Copia de seguridad (estado_2)

```bash
# Estado_2: respaldar discos de VM
virsh shutdown Nodo1
virsh domstate Nodo1   # debe decir "shut off"
virsh shutdown Nodo2
virsh domstate Nodo2
virsh shutdown Almacenamiento
virsh domstate Almacenamiento

mkdir -p /backups/estado_2/$(date +%Y%m%d)
cp /var/lib/libvirt/images/Nodo1.qcow2      /backups/estado_2/$(date +%Y%m%d)/Nodo1_state2.qcow2
cp /var/lib/libvirt/images/Nodo2.qcow2      /backups/estado_2/$(date +%Y%m%d)/Nodo2_state2.qcow2
cp /var/lib/libvirt/images/Almacenamiento.qcow2 /backups/estado_2/$(date +%Y%m%d)/Almacenamiento_state2.qcow2

# Reiniciar VMs tras copia
virsh start Nodo1
virsh start Nodo2
virsh start Almacenamiento
```

5. En uno de los nodos que formar치n parte del cluster (Nodo1 o Nodo2), montar el volumen l칩gico compartido apacheLV en el directorio `/var/www`.

6. En el mismo nodo, una vez montado el volumen l칩gico apacheLV en `/var/www`, construir en este directorio la estructura de directorios que el servicio httpd espera encontrar. Para ello, en primer lugar deber치 hacer que el usuario SElinux system_u sea el usuario de contexto SElinux, de forma que los directorios se creen con las etiquetas de usuario SELinux adecuadas. A continuaci칩n, deber치 crear los directorios html y cgi-bin en el directorio `/var/www`.

7. En el mismo nodo, una vez realizado el paso anterior, en el directorio `/var/www/html` cree el archivo index.html y almacene en 칠l el siguiente contenido html:

```html
<html>
  <body>
    Enhorabuena: configuraci칩n correcta
  </body>
</html>
```

8. Una vez realizado el paso anterior, en el mismo nodo, establecer al directorio `/var/www` y sus descendientes los atributos de contexto SElinux correctos para que cuando el servicio httpd intente acceder a los archivos de contenidos no se produzca un error de seguridad.

9. Una vez realizado el paso anterior, en el mismo nodo, verificar que el servidor web Apache funciona correctamente. Para ello debe arrancar manualmente el servicio y acceder desde el anfitri칩n mediante un navegador o usando la orden curl. Si se muestra el contenido del archivo `/var/www/html/index.html`, entonces es que la configuraci칩n realizada es correcta. **Si no se muestra el contenido del fichero mencionado, entonces es que la configuraci칩n realizada no es correcta y debe repasarla**.

10. Una vez superada la prueba realizada en el paso anterior, pare el servicio httpd y desmonte el volumen compartido apacheLV.

11. En el otro nodo, verificar que el servidor web Apache funciona correctamente. Para ello debe, en primer lugar, montar el volumen compartido apacheLV en el directorio `/var/www` y, en segundo lugar, arrancar manualmente el servicio y acceder desde el anfitri칩n mediante un navegador o usando la orden curl. Si se muestra el contenido del archivo `/var/www/html/index.html`, entonces es que la configuraci칩n realizada es correcta. Si no se muestra el contenido del fichero mencionado, entonces es que la configuraci칩n realizada no es correcta y debe repasarla.

## 4. Pruebas y validaci칩n

### Pruebas de conectividad

- Verificar conectividad entre todos los nodos por todas las interfaces (`ping` entre IPs y FQDNs).
- Verificar conectividad a Internet desde Nodo1 y Nodo2: `ping -c 3 8.8.8.8`.
- Verificar rutas correctas: `ip route`.

### Verificaci칩n de Apache

- En Nodo1:
  - Montar `/var/www` con el volumen apacheLV.
  - Iniciar manualmente `httpd`: `sudo systemctl start httpd`.
  - Verificar con `curl http://<IP_Cluster_Nodo1>` que devuelve el mensaje de "Enhorabuena".
  - Parar el servicio: `sudo systemctl stop httpd`.

- En Nodo2:
  - Montar `/var/www` con el volumen apacheLV.
  - Iniciar manualmente `httpd`: `sudo systemctl start httpd`.
  - Verificar con `curl http://<IP_Cluster_Nodo2>` que devuelve el mismo contenido.
  - Parar el servicio: `sudo systemctl stop httpd`.

### Verificaci칩n de almacenamiento compartido

- Confirmar que el contenido creado en `/var/www` en un nodo aparece correctamente en el otro tras montar el volumen.

### Recomendaci칩n

- Realizar capturas de pantalla de cada verificaci칩n y adjuntarlas como evidencia.

## 5. Conclusiones

En esta secci칩n debe incluir:

- Resumen del trabajo realizado
- Dificultades encontradas y soluciones aplicadas
- Conocimientos adquiridos
- Posibles mejoras o ampliaciones futuras

## 6. Bibliograf칤a

1. Red Hat. (2023). "High Availability Add-On Overview". [Enlace](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/high_availability_add-on_overview/index)

2. Red Hat. (2023). "High Availability Add-On Administration". [Enlace](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/high_availability_add-on_administration/index)

3. Red Hat. (2023). "Configuring and managing high availability clusters". [Enlace](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_high_availability_clusters/index)
